\chapter{Teoria di base del prodotto scalare}

\begin{note}
	Nel corso del documento, per $V$, qualora non specificato, si intenderà uno spazio vettoriale di dimensione
	finita $n$.
\end{note}

\section{Prime definizioni}

\subsection{Prodotto scalare e vettori ortogonali rispetto a \texorpdfstring{$\varphi$}{φ}}

\begin{definition} [prodotto scalare]
	Un \textbf{prodotto scalare} su $V$ è una forma bilineare simmetrica $\varphi$ con argomenti in $V$.
\end{definition}

\begin{example}
	Sia $\varphi : M(n, \KK) \times M(n, \KK) \to \KK$ tale che $\varphi(A, B) = \tr(AB)$. \\
	
	\li $\varphi(A + A', B) = \tr((A + A')B) = \tr(AB + A'B) = \tr(AB) + \tr(A'B) = \varphi(A, B) + \varphi(A', B)$ (linearità
	nel primo argomento), \\
	\li $\varphi(\alpha A, B) = \tr(\alpha A B) = \alpha \tr(AB) = \alpha \varphi(A, B)$ (omogeneità nel primo argomento), \\
	\li $\varphi(A, B) = \tr(AB) = \tr(BA) = \varphi(B, A)$ (simmetria), \\
	\li poiché $\varphi$ è simmetrica, $\varphi$ è lineare e omogenea anche nel secondo argomento, e quindi è una
	forma bilineare simmetrica, ossia un prodotto scalare su $M(n, \KK)$.
\end{example}

\begin{definition} [vettori ortogonali]
	Due vettori $\v$, $\w \in V$ si dicono \textbf{ortogonali} rispetto al prodotto scalare $\varphi$, ossia $\v \perp \w$, se $\varphi(\v, \w) = 0$.
\end{definition}

\begin{definition}[somma diretta ortogonale]
	Siano $U$ e $W \subseteq V$ due sottospazi di $V$ in somma diretta. Allora si dice che $U$ e $W$ sono in \textbf{somma diretta ortogonale} rispetto al prodotto scalare $\varphi$ di $V$, ossia che $U \oplus W = U \oplus^\perp W$, se $\varphi(\vec u, \vec w) = 0$ $\forall \vec u \in U$, $\vec w \in W$.
\end{definition}

\begin{definition}
	Si definisce prodotto scalare \textit{canonico} di $\KK^n$ la forma bilineare simmetrica $\varphi = \innprod{\cdot, \cdot}$ con
	argomenti in $\KK^n$ tale che:
	
	\[ \varphi(\v, \w) = \innprod{\v, \w} = \vec v ^\top \vec w, \quad \forall \v, \w \in V. \]
\end{definition}

\begin{remark}
	Si può facilmente osservare che il prodotto scalare canonico di $\KK^n$ è effettivamente un prodotto
	scalare. \\
	
	\li $\varphi(\vv 1 + \vv 2, \w) = (\vv 1 + \vv 2)^\top \w = (\vv 1 ^\top + \vv 2 ^\top) \w = \vv 1 ^\top \w + \vv 2 ^\top \w = \varphi(\vv 1, \w) + \varphi(\vv 2, \w)$ (linearità nel
	primo argomento), \\
	\li $\varphi(\alpha \v, \w) = (\alpha \v)^\top \w = \alpha \v^\top \w = \alpha \varphi(\v, \w)$ (omogeneità nel primo argomento), \\
	\li $\varphi(\v, \w) = \v ^\top \w = (\v ^\top \w)^\top = \w^\top \v = \varphi(\w, \v)$ (simmetria), \\
	\li poiché $\varphi$ è simmetrica, $\varphi$ è lineare e omogenea anche nel secondo argomento, e quindi è una
	forma bilineare simmetrica, ossia un prodotto scalare su $\KK^n$.
\end{remark}

\begin{example}
	Altri esempi di prodotto scalare sono i seguenti: \\
	
	\li $\varphi(A, B) = \tr(A^\top B)$ per $M(n, \KK)$, \\
	\li $\varphi(p(x), q(x)) = p(a) q(a)$ per $\KK[x]$, con $a \in \KK$, \\
	\li $\varphi(p(x), q(x)) = \sum_{i=1}^n p(x_i) q(x_i)$ per $\KK[x]$, con $x_1$, ..., $x_n$ distinti, \\
	\li $\varphi(p(x), q(x)) = \int_a^b p(x)q(x) dx$ per lo spazio delle funzioni integrabili su $\RR$, con $a$, $b$ in $\RR$, \\
	\li $\varphi(\vec{x}, \vec{y}) = \vec{x}^\top A \, \vec{y}$ per $\KK^n$, con $A \in M(n, \KK)$ simmetrica, detto anche \textbf{prodotto scalare indotto dalla matrice $A$}, ed indicato con $\varphi_A$.
\end{example}

\subsection{Prodotto definito o semidefinito}

\begin{definition}
	Sia\footnote{In realtà, la definizione è facilmente estendibile a qualsiasi campo, purché esso
		sia ordinato.} $\KK = \RR$. Allora un prodotto scalare $\varphi$ si dice \textbf{definito positivo} ($\varphi > 0$) se $\v \in V$, $\vec{v} \neq \vec{0} \implies
	\varphi(\vec{v}, \vec{v}) > 0$. Analogamente $\varphi$ è \textbf{definito negativo} ($\varphi < 0$) se $\vec{v} \neq \vec 0 \implies \varphi(\v, \v) < 0$. In generale si dice che $\varphi$ è \textbf{definito} se è definito positivo o
	definito negativo. \\
	
	Infine, $\varphi$ è \textbf{semidefinito positivo} ($\varphi \geq 0$) se $\varphi(\v, \v) \geq 0$ $\forall \v \in V$ (o
	\textbf{semidefinito negativo}, e quindi $\varphi \leq 0$, se invece $\varphi(\v, \v) \leq 0$ $\forall \v \in V$). Analogamente ai
	prodotti definiti, si dice che $\varphi$ è \textbf{semidefinito} se è semidefinito positivo o semidefinito
	negativo.
\end{definition}

\begin{example}
	Il prodotto scalare canonico di $\RR^n$ è definito positivo: infatti $\varphi((x_1, ..., x_n), (x_1, ..., x_n)) =
	\sum_{i=1}^n x_i^2  > 0$, se $(x_1, ..., x_n) \neq \vec 0$. \\
	
	Al contrario, il prodotto scalare $\varphi : \RR^2 \to \RR$ tale che $\varphi((x_1, x_2), (y_1, y_2)) = x_1 y_1 - x_2 y_2$ non è definito positivo: $\varphi((x, y), (x, y)) = 0$, $\forall$ $(x, y) \mid x^2 = y^2$, ossia se
	$y = x$ o $y = -x$.
\end{example}

\section{Il radicale di un prodotto scalare}

\subsection{La forma quadratica $q$ associata a \texorpdfstring{$\varphi$}{φ} e vettori (an)isotropi}

\begin{definition}
	Ad un dato prodotto scalare $\varphi$ di $V$ si associa una mappa
	$q : V \to \KK$, detta \textbf{forma quadratica}, tale che $q(\vec{v}) = \varphi(\vec{v}, \vec{v})$.
\end{definition}

\begin{remark}
	Si osserva che $q$ non è lineare in generale: infatti $q(\vec{v} + \vec{w}) \neq q(\vec{v}) + q(\vec{w})$ in
	$\RR^n$.
\end{remark}

\begin{definition}[vettore (an)isotropo]
	Un vettore $\vec{v} \in V$ si dice \textbf{isotropo} rispetto al prodotto scalare $\varphi$ se $q(\vec{v}) =
	\varphi(\vec{v}, \vec{v}) = 0$. Al contrario, $\v$ si dice \textbf{anisotropo} se non è isotropo, ossia
	se $q(\v) \neq 0$.
\end{definition}

\begin{definition}[cono isotropo]
	Si definisce \textbf{cono isotropo} di $V$ rispetto al prodotto scalare $\varphi$ il seguente insieme:
	
	\[ \CI(\varphi) = \{ \v \in V \mid \varphi(\v, \v) = 0 \}, \]
	
	\vskip 0.05in
	
	ossia l'insieme dei vettori isotropi di $V$.
\end{definition}

\begin{example}
	Rispetto al prodotto scalare $\varphi : \RR^3 \to \RR$ tale che $\varphi((x_1, x_2, x_3), (y_1, y_2, y_3)) =
	x_1 y_1 + x_2 y_2 - x_3 y_3$, i vettori isotropi sono i vettori della forma $(x, y, z)$ tali che $x^2 + y^2 = z^2$, e quindi $\CI(\varphi)$ è l'insieme dei
	vettori stanti sul cono di equazione $x^2 + y^2 = z^2$.
\end{example}

\subsection{Matrice associata a \texorpdfstring{$\varphi$}{φ} e relazione di congruenza}

\begin{remark}
	Come già osservato in generale per le applicazioni multilineari, il prodotto scalare è univocamente determinato
	dai valori che assume nelle coppie $\vv{i}, \vv{j}$ estraibili da una base $\basis$. Infatti, se
	$\basis = (\vv1, ..., \vv{k})$, $\vec{v} = \sum_{i=1}^k \alpha_i \vv{i}$ e $\vec{w} = \sum_{i=1}^k \beta_i \vv{i}$,
	allora:
	
	\[ \varphi(\vec{v}, \vec{w}) = \sum_{i=1}^k \sum_{j=1}^k \alpha_i \beta_j \, \varphi(\vv{i}, \vv{j}). \]
\end{remark}

\begin{definition}
	Sia $\varphi$ un prodotto scalare di $V$ e sia $\basis = (\vv1, ..., \vv{n})$ una base ordinata di $V$. Allora si definisce la \textbf{matrice associata}
	a $\varphi$ come la matrice:
	
	\[ M_\basis(\varphi) = (\varphi(\vv{i}, \vv{j}))_{i,\,j = 1\text{---}n} \in M(n, \KK). \] 
\end{definition}

\begin{remark}\nl
	\li $M_\basis(\varphi)$ è simmetrica, infatti $\varphi(\vv{i}, \vv{j}) = \varphi(\vv{j}, \vv{i})$,
	dal momento che il prodotto scalare è simmetrico, \\
	\li $\varphi(\vec{v}, \vec{w}) = [\vec{v}]_\basis^\top M_\basis(\varphi) [\vec{w}]_\basis$.
\end{remark}

\begin{theorem} (di cambiamento di base per matrici di prodotti scalari) Siano $\basis$, $\basis'$ due
	basi ordinate di $V$. Allora, se $\varphi$ è un prodotto scalare di $V$ e $P = M^{\basis'}_{\basis}(\Id_V)$, vale la seguente identità:
	
	\[ \underbrace{M_{\basis'}(\varphi)}_{A'} = P^\top \underbrace{M_{\basis}}_{A} P. \]
\end{theorem}

\begin{proof} Siano $\basis = (\vv{1}, ..., \vv{n})$ e $\basis' = (\vec{w}_1, ..., \vec{w}_n)$. Allora
	$A'_{ij} = \varphi(\vec{w}_i, \vec{w}_j) = [\vec{w}_i]_{\basis}^\top A [\vec{w}_j]_{\basis} =
	(P^i)^\top A P^j = P_i^\top (AP)^j = (P^\top AP)_{ij}$, da cui la tesi.
\end{proof}

\begin{definition}
	Si definisce \textbf{congruenza} la relazione di equivalenza $\cong$ (denotata anche come $\equiv$) definita nel seguente
	modo su $A, B \in M(n, \KK)$:
	
	\[ A \cong B \defiff \exists P \in GL(n, \KK) \mid A = P^\top A P. \]
\end{definition}

\begin{remark}
	Si può facilmente osservare che la congruenza è in effetti una relazione di equivalenza. \\
	
	\li $A = I^\top A I \implies A \cong A$ (riflessione), \\
	\li $A \cong B \implies A = P^\top B P \implies B = (P^\top)\inv A P\inv = (P\inv)^\top A P\inv \implies B \cong A$ (simmetria), \\
	\li $A \cong B$, $B \cong C$ $\implies A = P^\top B P$, $B = Q^\top C Q$, quindi $A = P^\top Q^\top C Q P =
	(QP)^\top C (QP) \implies A \cong C$ (transitività). 
\end{remark}

\begin{remark}
	Si osservano alcune proprietà della congruenza. \\
	
	\li Per il teorema di cambiamento di base del prodotto scalare, due matrici associate a uno stesso
	prodotto scalare sono sempre congruenti (esattamente come due matrici associate a uno stesso
	endomorfismo sono sempre simili). \\
	\li Se $A$ e $B$ sono congruenti, $A = P^\top B P \implies \rg(A) = \rg(P^\top B P) = \rg(BP) = \rg(B)$,
	dal momento che $P$ e $P^\top$ sono invertibili; quindi il rango è un invariante per congruenza. Allora
	si può ben definire il rango $\rg(\varphi)$ di un prodotto scalare come il rango della matrice
	associata di $\varphi$ in una qualsiasi base di $V$. \\
	\li Se $A$ e $B$ sono congruenti, $A = P^\top B P \implies \det(A) = \det(P^\top B P) = \det(P^\top) \det(B) \det(P)=
	\det(P)^2 \det(B)$. Quindi, per $\KK = \RR$, il segno del determinante è un altro invariante per congruenza.
\end{remark}

\subsection{Studio del radicale \texorpdfstring{$V^\perp$}{V⟂} attraverso \texorpdfstring{$M_\basis(\varphi)$}{M\_B(φ)}}

\begin{definition}
	Si definisce il \textbf{radicale} di un prodotto scalare $\varphi$ come lo spazio:
	
	\[ V^\perp = \Rad(\varphi) = \{ \vec{v} \in V \mid \varphi(\vec{v}, \vec{w}) = 0 \, \forall \vec{w} \in V \} \]
	
	\vskip 0.05in
\end{definition}

\begin{remark}
	Il radicale del prodotto scalare canonico su $\RR^n$ ha dimensione nulla, dal momento che $\forall \vec{v} \in \RR^n \setminus \{\vec{0}\}$, $q(\vec{v}) = \varphi(\vec{v}, \vec{v}) > 0 \implies \v \notin V^\perp$. In
	generale ogni prodotto scalare definito positivo (o negativo) è non degenere, dal momento che ogni vettore
	non nullo non è isotropo, e dunque non può appartenere a $V^\perp$.
\end{remark}

\begin{definition}
	Un prodotto scalare si dice \textbf{degenere} se il radicale dello spazio su tale prodotto scalare ha
	dimensione non nulla.
\end{definition}

\begin{remark}
	Sia $\alpha_\varphi : V \to \dual{V}$ la mappa\footnote{In letteratura questa mappa, se invertibile, è nota come \textit{isomorfismo musicale}, ed è in realtà indicata come $\flat$.} tale che
	$\alpha_\varphi(\vec{v}) = p$, dove $p(\vec{w}) = \varphi(\vec{v}, \vec{w})$ $\forall \v$, $\w \in V$. \\
	
	Si osserva che $\alpha_\varphi$ è un'applicazione lineare. Infatti, $\forall \v$, $\w$, $\U \in V$,
	$\alpha_\varphi(\v + \w)(\U) = \varphi(\v + \w, \U) = \varphi(\v, \U) + \varphi(\w, \U) =
	\alpha_\varphi(\v)(\U) + \alpha_\varphi(\w)(\U) \implies \alpha_\varphi(\v + \w) = \alpha_\varphi(\v) + \alpha_\varphi(\w)$. Inoltre $\forall \v$, $\w \in V$, $\lambda \in \KK$, $\alpha_\varphi(\lambda \v)(\w) =
	\varphi(\lambda \v, \w) = \lambda \varphi(\v, \w) = \lambda \alpha_\varphi(\v)(\w) \implies
	\alpha_\varphi(\lambda \v) = \lambda \alpha_\varphi(\v)$.
	
	Si osserva inoltre che $\Ker \alpha_\varphi$ raccoglie tutti
	i vettori $\v \in V$ tali che $\varphi(\v, \w) = 0$ $\forall \w \in W$, ossia esattamente i vettori di $V^\perp$, per cui si conclude che $V^\perp = \Ker \alpha_\varphi$ (per cui $V^\perp$ è effettivamente uno
	spazio vettoriale). Se $V$ ha dimensione finita, $\dim V = \dim \dual{V}$,
	e si può allora concludere che $\dim V^\perp > 0 \iff \Ker \alpha_\varphi \neq \{\vec{0}\} \iff \alpha_\varphi$ non è
	invertibile (infatti lo spazio di partenza e di arrivo di $\alpha_\varphi$ hanno la stessa dimensione). In
	particolare, $\alpha_\varphi$ non è invertibile se e solo se $\det(\alpha_\varphi) = 0$. \\
	
	Sia $\basis = (\vv{1}, ..., \vv{n})$ una base ordinata di $V$. Si consideri allora la base ordinata del
	duale costruita su $\basis$, ossia $\dual{\basis} = (\vecdual{v_1}, ..., \vecdual{v_n})$. Allora
	$M_{\basisdual}^\basis(\alpha_\varphi)^i = [\alpha_\varphi(\vv{i})]_{\basisdual} = \Matrix{\varphi(\vec{v_i}, \vec{v_1}) \\ \vdots \\ \varphi(\vec{v_i}, \vec{v_n})} \underbrace{=}_{\varphi \text{ è simmetrica}}
	\Matrix{\varphi(\vec{v_1}, \vec{v_i}) \\ \vdots \\ \varphi(\vec{v_n}, \vec{v_i})} = M_\basis(\varphi)^i$. Quindi
	$M_{\basisdual}^\basis(\alpha_\varphi) = M_\basis(\varphi)$. \\
	
	Si conclude allora che $\varphi$ è degenere se e solo se $\det (M_\basis(\varphi)) = 0$ e che
	$V^\perp \cong \Ker M_\basis(\varphi)$ mediante l'isomorfismo del passaggio alle coordinate.
\end{remark}

\subsection{Condizioni per la (semi)definitezza di un prodotto scalare}

\begin{proposition} Sia $\KK = \RR$. Allora
	$\varphi$ è definito $\iff$ $\CI(\varphi) = \zerovecset$. \label{prop:definitezza_varphi}
\end{proposition}

\begin{proof}
	Si dimostrano le due implicazioni separatamente. \\
	
	\rightproof Se $\varphi$ è definito, allora $\varphi(\v, \v)$ è sicuramente diverso da zero
	se $\v \neq \vec 0$. Pertanto $\CI(\varphi) = \zerovecset$. \\
	
	\leftproof Sia $\varphi$ non definito. Se non esistono $\v \neq \vec 0$, $\w \neq \vec 0 \in V$ tali che
	$q(\v) > 0$ e che $q(\w) < 0$, allora $\varphi$ è necessariamente semidefinito. In tal caso,
	poiché $\varphi$ non è definito, deve anche esistere $\U \in V$, $\U \neq \vec 0 \mid q(\U) = 0 \implies \CI(\varphi) \neq \zerovecset$. \\
	
	Se invece tali $\v$, $\w$ esistono, questi sono anche linearmente indipendenti. Se infatti
	non lo fossero, uno sarebbe il multiplo dell'altro, e quindi le loro due forme quadratiche
	sarebbero concordi di segno, \Lightning. Si consideri allora la combinazione lineare
	$\v + \lambda \w$ al variare di $\lambda \in \RR$, imponendo che essa sia isotropa:
	
	\[ q(\v + \lambda \w) = 0 \iff \lambda^2 q(\w)+ 2 \lambda q(\v, \w) + q(\v) = 0. \]
	
	\vskip 0.05in
	
	Dal momento che $\frac{\Delta}{4} = \overbrace{q(\v, \w)^2}^{\geq 0} - \overbrace{q(\w)q(\v)}^{> 0}$ è
	sicuramente maggiore di zero, tale equazione ammette due soluzioni reali $\lambda_1$, $\lambda_2$.
	In particolare $\lambda_1$ è tale che $\v + \lambda_1 \w \neq \vec 0$, dal momento che $\v$ e $\w$
	sono linearmente indipendenti. Allora $\v + \lambda_1 \w$ è un vettore isotropo non nullo
	di $V$ $\implies \CI(\varphi) \neq \zerovecset$. \\
	
	Si conclude allora, tramite la contronominale, che se $\CI(\varphi) = \zerovecset$, $\varphi$
	è necessariamente definito. 
\end{proof}

\begin{proposition} Sia $\KK = \RR$. Allora
	$\varphi$ è semidefinito $\iff$ $\CI(\varphi) = V^\perp$. \label{prop:semidefinitezza_varphi}
\end{proposition}

\begin{proof}
	Si dimostrano le due implicazioni separatamente. \\
	
	\rightproof Sia $\varphi$ semidefinito. Chiaramente $V^\perp \subseteq \CI(\varphi)$. Si assuma per assurdo
	che $V^\perp \subsetneq \CI(\varphi)$. Sia allora $\v$ tale che $\v \in \CI(\varphi)$ e che $\v \notin V^\perp$.
	Poiché $\v \notin V^\perp$, esiste un vettore $\w \in V$ tale che $\varphi(\v, \w) \neq 0$. Si osserva
	che $\v$ e $\w$ sono linearmente indipendenti tra loro. Se infatti non lo fossero, esisterebbe $\mu \in \RR$
	tale che $\w = \mu \v \implies \varphi(\v, \w) = \mu \, \varphi(\v, \v) = 0$, \Lightning. \\
	
	Si consideri allora la combinazione lineare $\v + \lambda \w$. Si consideri $\varphi$ semidefinito positivo.
	In tal caso si può imporre che la valutazione di $q$ in $\v + \lambda \w$ sia strettamente negativa:
	
	\[ q(\v + \lambda \w) < 0 \iff \overbrace{q(\v)}^{=0} + \lambda^2 q(\w) + 2 \lambda \, \varphi(\v, \w) < 0. \]
	
	\vskip 0.05in
	
	In particolare, dal momento che $\frac{\Delta}{4} = \varphi(\v, \w)^2 > 0$, tale disequazione ammette
	una soluzione $\lambda_1 \neq 0$. Inoltre $\v + \lambda_1 \w \neq \vec 0$, dal momento che $\v$ e
	$\w$ sono linearmente indipendenti. Allora si è trovato un vettore non nullo per cui la valutazione in esso
	di $q$ è negativa, contraddicendo l'ipotesi di semidefinitezza positiva di $\varphi$, \Lightning. Analogamente
	si dimostra la tesi per $\varphi$ semidefinito negativo. \\
	
	\leftproof Sia $\varphi$ non semidefinito. Allora devono esistere $\v$, $\w \in V$ tali che
	$q(\v) > 0$ e che $q(\w) < 0$. In particolare, $\v$ e $\w$ sono linearmente indipendenti
	tra loro, dal momento che se non lo fossero, uno sarebbe multiplo dell'altro, e le
	valutazioni in essi di $q$ sarebbero concordi di segno, \Lightning. Si consideri allora
	la combinazione lineare $\v + \lambda \w$, imponendo che $q$ si annulli in essa:
	
	\[ q(\v + \lambda \w) = 0 \iff \lambda^2 q(\w)+ 2 \lambda q(\v, \w) + q(\v) = 0. \]
	
	\vskip 0.05in
	
	In particolare, dal momento che $\frac{\Delta}{4} = \varphi(\v, \w)^2 > 0$, tale disequazione ammette
	una soluzione $\lambda_1 \neq 0$. Allora, per tale $\lambda_1$, $\v + \lambda_1 \w \in \CI(\varphi)$.
	Tuttavia $\varphi(\v + \lambda_1 \w, \v - \lambda_1 \w) = q(\v) - \underbrace{\lambda_1^2 q(\w)}_{<0} > 0 \implies
	\v + \lambda_1 \w \notin V^\perp \implies \CI(\varphi) \supsetneq V^\perp$. \\
	
	Si conclude allora, tramite la contronominale, che se $\CI(\varphi) = V^\perp$, $\varphi$
	è necessariamente semidefinito. 
\end{proof}

\section{Formula delle dimensioni e di polarizzazione rispetto a $\varphi$}

\begin{definition}[sottospazio ortogonale a $W$]
	Sia $W \subseteq V$ un sottospazio di $V$. Si identifica allora come \textbf{sottospazio ortogonale a $W$}
	il sottospazio $W^\perp = \{ \v \in V \mid \varphi(\v, \w) \, \forall \w \in W \}$.
\end{definition}

\begin{proposition}[formula delle dimensioni del prodotto scalare]
	Sia $W \subseteq V$ un sottospazio di $V$. Allora vale la seguente identità:
	
	\[ \dim W + \dim W^\perp = \dim V + \dim (W \cap V^\perp). \]
\end{proposition}

\begin{proof}
	Si consideri l'applicazione lineare $a_\varphi$ introdotta precedentemente. Si osserva che $W^\perp = \Ker (i^\top \circ a_\varphi)$, dove $i : W \to V$ è tale che $i(\vec w) = \vec w$. Allora,
	per la formula delle dimensioni, vale la seguente identità: 
	
	\begin{equation}
		\label{eq:dim_formula_dimensioni_1}
		\dim V = \dim W^\perp + \rg (i^\top \circ a_\varphi). 
	\end{equation}
	
	\vskip 0.05in
	
	Sia allora $f = i^\top \circ a_\varphi$.
	Si consideri ora l'applicazione $g = a_\varphi \circ i : W \to \dual V$. Sia ora $\basis_W$ una base di $W$ e
	$\basis_V$ una base di $V$. Allora le matrici associate di $f$ e di $g$ sono le seguenti:
	
	\begin{enumerate}[(i)]
		\item $M_{\dual \basis_W}^{\basis_V}(f) = M_{\dual \basis_W}^{\basis_V}(i^\top \circ a_\varphi) =
		\underbrace{M_{\dual \basis_W}^{\dual \basis_V}(i^\top)}_A \underbrace{M_{\dual \basis_V}^{\basis_V}(a_\varphi)}_B = AB$,
		\item $M_{\dual \basis_V}^{\basis_W}(g) = M_{\dual \basis_V}^{\basis_W}(a_\varphi \circ i) =
		\underbrace{M_{\dual \basis_V}^{\basis_V}(a_\varphi)}_B \underbrace{M_{\basis_V}^{\basis_W}(i)}_{A^\top} = BA^\top \overbrace{=}^{B^\top = B} (AB)^\top$.
	\end{enumerate}
	
	Poiché $\rg(A) = \rg(A^\top)$, si deduce che $\rg(f) = \rg(g) \implies \rg(i^\top \circ a_\varphi) = \rg(a_\varphi \circ i) = \rg(\restr{a_\varphi}{W}) = \dim W - \dim \Ker \restr{a_\varphi}{W}$, ossia che:
	
	\begin{equation}
		\label{eq:dim_formula_dimensioni_2}
		\rg(i^\top \circ a_\varphi) = \dim W - \dim (W \cap \underbrace{\Ker a_\varphi}_{V^\perp}) = \dim W - \dim (W \cap V^\perp).
	\end{equation}
	
	Si conclude allora, sostituendo l'equazione \eqref{eq:dim_formula_dimensioni_2} nell'equazione \eqref{eq:dim_formula_dimensioni_1}, che $\dim V = \dim W^\top + \dim W - \dim (W \cap V^\perp)$, ossia la tesi.
\end{proof}

\begin{proof}[Dimostrazione alternativa]
	Si consideri nuovamente l'applicazione lineare $\alpha_\varphi$ introdotta
	precedentemente. Si osserva innanzitutto che\footnote{$\alpha_\varphi\inv$ in questo caso non indica un'eventuale applicazione inversa di $\alpha_\varphi$, ma indica l'insieme delle eventuali controimmagini degli elementi su cui è applicata.} $W^\perp = \alpha_\varphi\inv(\Ann(W))$. Allora vale la seguente identità:
	
	\begin{equation}
		\label{eq:formula_dimensioni_dimostrazione_alternativa_1}
		\alpha_\varphi(W^\perp) = \Ann(W) \cap \Im \alpha_\varphi.
	\end{equation}

	\vskip 0.05in
	
	Si mostra che $\Im \alpha_\varphi = \Ann(V^\perp)$. Chiaramente $\Im \alpha_\varphi \subseteq \Ann(V^\perp)$: siano infatti $\v \in V$ e $\w \in V^\perp$, allora $\alpha_\varphi(\v)(\w) = \varphi(\v, \w) = 0$. Inoltre $\dim \Im \alpha_\varphi = \rg \alpha_\varphi = n - \dim \Ker \alpha_\varphi = \dim V - \dim V^\perp = \dim \Ann(V^\perp)$,
	da cui segue l'uguaglianza dei due sottospazi. Allora l'equazione \eqref{eq:formula_dimensioni_dimostrazione_alternativa_1} si può riscrivere\footnote{Si è utilizzata l'identità $\Ann(U) \cap \Ann(W) = \Ann(U + W)$, dove $U$ e $W$ sono due sottospazi di $V$, nonché che $\Ker \alpha_\varphi = V^\perp$.} come:
	
	\[ \alpha_\varphi(W^\perp) = \Ann(W) \cap \Ann(V^\perp) = \Ann(W + V^\perp) \]
	
	da cui segue che:
	
	\[ \dim W^\perp - \dim (V^\perp \cap W^\perp) = \dim V - \dim (W + V^\perp), \]
	
	e quindi, applicando la formula di Grassmann, che\footnote{Ricordiamo che $V^\perp \subseteq W^\perp$ per ogni sottospazio $W$ di $V$, e quindi che $\dim (V^\perp \cap W^\perp) = \dim V^\perp$.}:
	
	\[ \dim W^\perp - \dim V^\perp = \dim V - \dim W - \dim V^\perp + \dim (W \cap V^\perp), \]
	
	ossia la tesi.
\end{proof}

\begin{remark} Si identifica $\w^\perp$ come il sottospazio di tutti i vettori di $V$ ortogonali a $\w$.
	In particolare, se $W = \Span(\vec w)$ è il sottospazio generato da $\vec w \neq \vec 0$, $\vec w \in V$, allora $W^\perp = \w^\perp$. Inoltre valgono le seguenti equivalenze: $\vec w \notin W^\perp \iff$ $\Rad (\restr{\varphi}{W}) = W \cap W^\perp = \zerovecset$ $\iff \vec w \text{ non è isotropo } \iff$ $V = W \oplus^\perp W^\perp$. \\
	
	In generale, se $W$ è un sottospazio qualsiasi di $V$ tale che $W \cap W^\perp = \zerovecset$, vale
	che $V = W \oplus^\perp W^\perp$.
\end{remark}

\begin{proposition}[formula di polarizzazione] \label{prop:formula_polarizzazione}
	Se $\Char \KK \neq 2$, un prodotto scalare è univocamente determinato dalla sua forma quadratica $q$.
	In particolare vale la seguente identità:
	
	\[ \varphi(\v, \w) = \frac{q(\v + \w) - q(\v) - q(\w)}{2}. \]
	
	\vskip 0.05in
\end{proposition}

\section{Il teorema di Lagrange e basi ortogonali}

\begin{definition}
	Si definisce \textbf{base ortogonale} di $V$ una base $\vv 1$, ..., $\vv n$ tale per cui $\varphi(\vv i, \vv j) = 0
	\impliedby i \neq j$, ossia una base per cui la matrice associata del prodotto scalare è diagonale. 
\end{definition}

\begin{theorem}[di Lagrange]
	Ogni spazio vettoriale $V$ su $\KK$ tale per cui $\Char \KK \neq 2$ ammette una base ortogonale.
\end{theorem}

\begin{proof}
	Si dimostra il teorema per induzione su $n := \dim V$. Per $n \leq 1$, la tesi è triviale (se esiste una base, tale base è
	già ortogonale). Sia
	allora il teorema vero per $i \leq n$. Se $V$ ammette un vettore non isotropo $\vec w$, sia $W = \Span(\vec w)$ e si consideri la decomposizione $V = W \oplus W^\perp$. Poiché $W^\perp$ ha dimensione $n-1$, per ipotesi induttiva
	ammette una base ortogonale. Inoltre, tale base è anche ortogonale a $W$, e quindi l'aggiunta di $\vec w$ a
	questa base ne fa una base ortogonale di $V$. Se invece $V$ non ammette vettori non isotropi, ogni forma quadratica
	è nulla, e quindi il prodotto scalare è nullo per la \textit{\nameref{prop:formula_polarizzazione}}. Allora in questo caso
	ogni base è una base ortogonale, completando il passo induttivo, e dunque la dimostrazione.
\end{proof}

\subsection{L'algoritmo di ortogonalizzazione di Gram-Schmidt}

\begin{definition}[coefficiente di Fourier]
	Siano $\v \in V$ e $\w \in V \setminus \CI(\varphi)$. Allora si definisce il \textbf{coefficiente di Fourier}
	di $\v$ rispetto a $\w$ come il rapporto $C(\w, \v) = \frac{\varphi(\v, \w)}{\varphi(\w, \w)}$.
\end{definition}

\begin{algorithm}[algoritmo di ortogonalizzazione di Gram-Schmidt]
	Se $\CI(\varphi) = \zerovecset$ (e quindi nel caso di $\KK = \RR$, dalla
	\textit{Proposizione \ref{prop:definitezza_varphi}}, se $\varphi$ è definito) ed è
	data una base $\basis = \{ \vv 1, \ldots, \vv n \}$ per $V$, è possibile
	applicare l'\textbf{algoritmo di ortogonalizzazione di Gram-Schmidt} per ottenere
	da $\basis$ una nuova base $\basis' = \{ \vv 1', \ldots, \vv n' \}$ con le seguenti proprietà:
	
	\begin{enumerate}[(i)]
		\item $\basis'$ è una base ortogonale,
		\item $\basis'$ mantiene la stessa bandiera di $\basis$ (ossia $\Span(\vv 1, \ldots, \vv i) = \Span(\vv 1', \ldots, \vv i')$ per ogni $1 \leq i \leq n$).
	\end{enumerate}
	
	L'algoritmo si applica nel seguente modo: si prenda in considerazione $\vv 1$ e si sottragga ad ogni altro vettore
	della base il vettore $C(\vv 1, \vv i) \, \vv 1 = \frac{\varphi(\vv 1, \vv i)}{\varphi(\vv 1, \vv 1)} \vv 1$,
	rendendo ortogonale ogni altro vettore della base con $\vv 1$. Si sta quindi applicando la mappa
	$\vv i \mapsto \vv i - \frac{\varphi(\vv 1, \vv i)}{\varphi(\vv 1, \vv 1)} \vv i = \vv i ^{(1)}$.
	Si verifica infatti che $\vv 1$ e $\vv i ^{(1)}$ sono ortogonali per $2 \leq i \leq n$:
	
	\[ \varphi(\vv 1, \vv i^{(1)}) = \varphi(\vv 1, \vv i) - \varphi\left(\vv 1, \frac{\varphi(\vv 1, \vv i)}{\varphi(\vv 1, \vv 1)} \vv i\right) = \varphi(\vv 1, \vv i) - \varphi(\vv 1, \vv i) = 0. \]
	
	Poiché $\vv 1$ non è isotropo, si deduce che vale la decomposizione $V = \Span(\vv 1) \oplus \Span(\vv 1)^\perp$.
	In particolare $\dim \Span(\vv 1)^\perp = n-1$: essendo allora i vettori $\vv 2 ^{(1)}, \ldots, \vv n ^{(1)}$
	linearmente indipendenti e appartenenti a $\Span(\vv 1)^\perp$, ne sono una base. Si conclude quindi
	che vale la seguente decomposizione:
	
	\[ V = \Span(\vv 1) \oplus^\perp \Span(\vv 2 ^{(1)}, \ldots, \vv n ^{(1)}). \]
	
	\vskip 0.05in
	
	Si riapplica dunque l'algoritmo di Gram-Schmidt prendendo come spazio vettoriale lo spazio generato dai
	vettori a cui si è applicato precedentemente l'algoritmo, ossia $V' = \Span(\vv 2 ^{(1)}, \ldots, \vv n ^{(1)})$,
	fino a che non si ottiene $V' = \zerovecset$.
\end{algorithm}

\begin{example}
	Si consideri $V = (\RR^3, \innprod{\cdot, \cdot})$, ossia $\RR^3$ dotato del prodotto scalare standard.
	Si applica l'algoritmo di ortogonalizzazione di Gram-Schmidt sulla seguente base:
	
	\[ \basis = \Biggl\{ \underbrace{\Vector{1 \\ 0 \\ 0}}_{\vv 1 \, = \, \e1}, \underbrace{\Vector{1 \\ 1 \\ 0}}_{\vv 2}, \underbrace{\Vector{1 \\ 1 \\ 1}}_{\vv 3} \Biggl\}. \]
	
	\vskip 0.05in
	
	Alla prima iterazione dell'algoritmo si ottengono i seguenti vettori:
	
	\begin{itemize}
		\item $\vv 2 ^{(1)} = \vv 2 -  \frac{\varphi(\vv 1, \vv 2)}{\varphi(\vv 1, \vv 1)} \vv 1 = \vv 2 - \vv 1 = \Vector{0 \\ 1 \\ 0} = \e 2$,
		\item $\vv 3 ^{(1)} = \vv 3 - \frac{\varphi(\vv 1, \vv 3)}{\varphi(\vv 1, \vv 1)} \vv 1 = \vv 3 - \vv 1 = \Vector{0 \\ 1 \\ 1}$.
	\end{itemize}
	
	Si considera ora $V' = \Span(\vv 2 ^{(1)}, \vv 3 ^{(1)})$. Alla seconda iterazione dell'algoritmo si
	ottiene allora il seguente vettore:
	
	\begin{itemize}
		\item $\vv 3 ^{(2)} = \vv 3 ^{(1)} - \frac{\varphi(\vv 2 ^{(1)},  \vv 3 ^{(1)})}{\varphi(\vv 2 ^{(1)}, \vv 2 ^{(1)})} \vv 2 ^{(1)} = \vv 3 ^{(1)} - \vv 2 ^{(1)} = \Vector{0 \\ 0 \\ 1} = \e 3$.
	\end{itemize}
	
	Quindi la base ottenuta è $\basis' = \{\e1, \e2, \e3\}$, ossia la base canonica di $\RR^3$.
\end{example}

\section{Il teorema di Sylvester}

\subsection{Caso complesso}

\begin{note}
	D'ora in poi, nel corso del documento, si assumerà $\Char \KK \neq 2$.
\end{note}

\begin{theorem}[di Sylvester, caso complesso]
	Sia $\KK$ un campo i cui elementi sono tutti quadrati di un
	altro elemento del campo (e.g.~$\CC$). Allora esiste una base
	ortogonale $\basis$ tale per cui:
	
	\[ M_\basis(\varphi) = \Matrix{I_r & \rvline & 0 \\ \hline 0 & \rvline & 0\,}. \]
\end{theorem}

\begin{proof}
	Per il teorema di Lagrange, esiste una base ortogonale $\basis'$ di $V$.
	Si riordini allora la base $\basis'$ in modo tale che la forma quadratica valutata nei primi elementi sia sempre diversa da zero. Allora, poiché ogni
	elemento di $\KK$ è per ipotesi quadrato di un altro elemento
	di $\KK$, si sostituisca $\basis'$ con una base $\basis$ tale per
	cui, se $q(\vv i) = 0$, $\vv i \mapsto \vv i$, e altrimenti
	$\vv i \mapsto \frac{\vv i}{\sqrt{q(\vv i)}}$. Allora $\basis$
	è una base tale per cui la matrice associata del prodotto scalare
	in tale base è proprio come desiderata nella tesi, dove $r$ è
	il numero di elementi tali per cui la forma quadratica valutata
	in essi sia diversa da zero.
\end{proof}

\begin{remark}\nl
	\li Si può immediatamente concludere che il rango è un invariante
	completo per la congruenza in un campo $\KK$ in cui tutti gli elementi
	sono quadrati, ossia che $A \cong B \iff \rg(A) = \rg(B)$, se $A$ e
	$B$ sono matrici simmetriche con elementi in $\KK$. \\
	
	Ogni matrice simmetrica rappresenta infatti un prodotto scalare, ed è
	pertanto congruente ad una matrice della forma desiderata
	nell'enunciato del teorema di Sylvester complesso. Poiché il rango
	è un invariante della congruenza, si ricava che $r$ nella forma
	della matrice di Sylvester, rappresentando il rango, è anche
	il rango di ogni sua matrice congruente. \\
	
	In particolare, se due
	matrici simmetriche hanno lo stesso rango, allora sono congruenti
	alla stessa matrice di Sylvester, e quindi, essendo la congruenza
	una relazione di equivalenza, sono congruenti a loro volta tra di loro. \\
	
	\li Due matrici simmetriche in $\KK$ con stesso rango, allora, non solo
	sono SD-equivalenti, ma sono anche congruenti. \\
	
	\li Ogni base ortogonale deve quindi avere lo stesso numero
	di vettori isotropi, dal momento che tale numero rappresenta
	la dimensione del radicale $V^\perp$.
\end{remark}

\subsection{Caso reale e segnatura di \texorpdfstring{$\varphi$}{φ}}

\begin{definition} [segnatura di un prodotto scalare]
	Data una base ortogonale $\basis$ di $V$ rispetto al prodotto
	scalare $\varphi$,
	si definiscono i seguenti indici:
	\begin{align*}
		\iota_+(\varphi) &= \max\{ \dim W \mid W \subseteq V \E \restr{\varphi}{W} > 0 \}, &\text{(}\textbf{indice di positività}\text{)} \\
		\iota_-(\varphi) &= \max\{ \dim W \mid W \subseteq V \E \restr{\varphi}{W} < 0 \}, &\text{(}\textbf{indice di negatività}\text{)}\\
		\iota_0(\varphi) &= \dim V^\perp. &\text{(}\textbf{indice di nullità}\text{)}
	\end{align*}
	
	Quando il prodotto scalare $\varphi$ è noto dal contesto, si
	semplifica la notazione
	scrivendo solo $\iota_+$, $\iota_-$ e $\iota_0$. In particolare,
	la terna $\sigma(\varphi) = \sigma = (i_+, i_-, i_0)$ è detta \textbf{segnatura} del
	prodotto $\varphi$.
\end{definition}

\begin{theorem}[di Sylvester, caso reale] Sia $\KK$ un campo ordinato
	i cui elementi positivi sono tutti quadrati (e.g.~$\RR$). Allora
	esiste una base ortogonale $\basis$ tale per cui:
	
	\[ M_\basis(\varphi) = \Matrix{I_{\iota_+} & \rvline & 0 & \rvline & 0 \\ \hline 0 & \rvline & -I_{\iota_-} & \rvline & 0 \\ \hline 0 & \rvline & 0 & \rvline & 0\cdot I_{\iota_0} }. \]
	
	\vskip 0.05in
	
	Inoltre, per ogni base ortogonale, esistono esattamente
	$\iota_+$ vettori della base con forma quadratica positiva,
	$\iota_-$ con forma negativa e $\iota_0$ con
	forma nulla.
\end{theorem}

\begin{proof}
	Per il teorema di Lagrange, esiste una base ortogonale $\basis'$ di $V$.
	Si riordini la base in modo tale che la forma quadratica valutata nei primi elementi sia strettamente positiva, che nei secondi elementi sia strettamente negativa e che negli ultimi sia nulla. Si sostituisca
	$\basis'$ con una base $\basis$ tale per cui, se $q(\vv i) > 0$,
	allora $\vv i \mapsto \frac{\vv i}{\sqrt{q(\vv i)}}$; se
	$q(\vv i) < 0$, allora $\vv i \mapsto \frac{\vv i}{\sqrt{-q(\vv i)}}$;
	altrimenti $\vv i \mapsto \vv i$. Si è allora trovata una base
	la cui matrice associata del prodotto scalare è come desiderata nella
	tesi. \\
	
	Sia ora $\basis$ una qualsiasi base ortogonale di $V$.
	Siano inoltre $a$ il numero di vettori della base con forma quadratica
	positiva, $b$ il numero di vettori con forma negativa e $c$ quello
	dei vettori con forma nulla. Si consideri $W_+ = \Span(\vv 1, ..., \vv a)$, $W_- = \Span(\vv{a+1}, ..., \vv b)$, $W_0 = \Span(\vv{b+1}, ..., \vv c)$. \\
	
	Sia $M = M_\basis(\varphi)$. Si osserva che $c = n - \rg(M) = \dim \Ker(M) = \dim V^\perp = \iota_0$. Inoltre $\forall \v \in W_+$, dacché
	$\basis$ è ortogonale,
	$q(\v) = q(\sum_{i=1}^a \alpha_i \vv i) = \sum_{i=1}^a \alpha_i^2 q(\vv i) > 0$, e quindi $\restr{\varphi}{W_+} > 0$, da cui $\iota_+ \geq a$.
	Analogamente $\iota_- \geq b$. \\
	
	Si mostra ora che è impossibile che $\iota_+ > a$. Se così infatti
	fosse, sia $W$ tale che $\dim W = \iota_+$ e che $\restr{\varphi}{W} > 0$. $\iota_+ + b + c$ sarebbe maggiore di $a + b + c = n := \dim V$. Quindi, per la formula di Grassman, $\dim(W + W_- + W_0) = \dim W +
	\dim(W_- + W_0) - \dim (W \cap (W_- + W_0)) \implies \dim (W \cap (W_- + W_0)) =  \dim W +
	\dim(W_- + W_0) - \dim(W + W_- + W_0) > 0$, ossia esisterebbe
	$\v \neq \{\vec 0\} \mid \v \in W \cap (W_- + W_0)$. Tuttavia
	questo è assurdo, dacché dovrebbe valere sia $q(\v) > 0$ che
	$q(\v) < 0$, \Lightning. Quindi $\iota_+ = a$, e analogamente
	$\iota_- = b$.
\end{proof}

\begin{definition}
	Si dice \textbf{base di Sylvester} una base di $V$ tale per cui la
	matrice associata di $\varphi$ sia esattamente nella forma
	vista nell'enunciato del teorema di Sylvester. Analogamente
	si definisce tale matrice come \textbf{matrice di Sylvester}.
\end{definition}

\begin{remark} \nl
	\li Come conseguenza del teorema di Sylvester reale, si osserva che la segnatura di una matrice simmetrica reale
	è invariante per cambiamento di base, se la base è ortogonale. \\
	
	\li La segnatura è un invariante completo per la congruenza nel caso reale. Se infatti due matrici hanno la stessa segnatura, queste sono
	entrambe congruenti alla stessa matrice di Sylvester, e quindi, essendo
	la congruenza una relazione di equivalenza, sono congruenti
	tra loro. Analogamente vale il viceversa, dal momento che ogni
	base ortogonale di due matrici congruenti deve contenere gli
	stessi numeri $\iota_+$, $\iota_-$ e $\iota_0$ di vettori
	di base con forma quadratica positiva, negativa e nulla. \\
	
	\li Vale che $\varphi$ è definito positivo $\iff$ $\sigma = (n, 0, 0)$. Infatti, per il teorema
	di Sylvester reale, $i_+ = n$ $\iff$ la dimensione del massimo sottospazio di $V$ su cui $\varphi$
	è definito positivo è $n$ $\iff$ $\varphi$ è definito positivo. Analogamente $\varphi$ è definito
	negativo $\iff$ $\sigma = (0, n, 0)$. \\
	
	\li Nello stesso spirito dei prodotti definiti, $\varphi$ è semidefinito positivo $\iff$ $\iota_- = 0$.
	Infatti valgono le seguenti equivalenze: $\varphi$ è semidefinito positivo $\iff$ non esiste un vettore
	$\v \in V$, $\v \neq \vec 0$ tale che $q(\v) < 0$ $\iff$ $\iota_- = 0$. Analogamente $\varphi$ è semidefinito
	negativo $\iff$ $\iota_+ = 0$. \\
	
	\li Se $\ww 1$, ..., $\ww k$ sono tutti i vettori di una base
	ortogonale $\basis$ con forma quadratica nulla, si osserva che $W = \Span(\ww 1, ..., \ww k)$ altro non è che $V^\perp$ stesso. \\
	
	Infatti, come
	visto anche nella dimostrazione del teorema di Sylvester reale, vale
	che	$\dim W = \dim \Ker (M_\basis(\varphi)) = \dim V^\perp$.
	Sia allora la base $\basis = \{\ww 1, \ldots, \ww k, \vv{k+1}, \ldots, \vv n\}$ un'estensione di $\{\ww 1, \ldots, \ww k\}$. Se $\w \in W$ e $\v \in V$, $\varphi(\w, \v) = \varphi(\sum_{i=1}^k
	\alpha_i \ww i, \sum_{i=1}^k \beta_i \ww i + \sum_{i=k+1}^n \beta_i \vv i)
	= \sum_{i=1}^k \alpha_i \beta_i q(\ww i) = 0$ (dove $\alpha_i$ e $\beta_i \in \KK$ rappresentano la $i$-esima coordinata di $\w$ e $\v$ nella base $\basis$), e quindi
	$W \subseteq V^\perp$. Si conclude allora, tramite l'uguaglianza
	dimensionale, che $W = V^\perp$. \\
	
	\li Poiché $\dim \Ker(\varphi) = \iota_0$, vale in particolare che $\rg(\varphi) = n - \iota_0 = \iota_+ + \iota_-$ (infatti vale che $n = \iota_+ + \iota_- + \iota_0$, dal momento che $n$ rappresenta il numero di elementi di una base ortogonale). \\
	
	\li Se $V = U \oplusperp W$, allora $\iota_+(\varphi) = \iota_+(\restr{\varphi}{U}) + \iota_+(\restr{\varphi}{W})$.
	Analogamente vale la stessa cosa per gli altri indici. Infatti,
	prese due basi ortogonali $\basis_U$, $\basis_W$ di $U$ e $W$,
	la loro unione $\basis$ è una base ortogonale di $V$. Pertanto
	il numero di vettori della base $\basis$ con forma quadratica positiva
	è esattamente $\iota_+(\restr{\varphi}{U}) + \iota_+(\restr{\varphi}{W})$. \\
	
	\li In generale, se $W$ è un sottospazio di $V$, vale che $\iota_+(\varphi) \geq \iota_+(\restr{\varphi}{W})$.
	Infatti, se $U$ è un sottospazio di $W$ di dimensione $\iota_+(\restr{\varphi}{W})$ tale che
	$\restr{(\restr{\varphi}{W})}{U} > 0$, allora $U$ è in particolare un sottospazio di $V$ tale che $\restr{\varphi}{U} > 0$. Pertanto, per definizione, essendo $\iota_+(\varphi)$ la dimensione del massimo sottospazio su cui $\varphi$, ristretto ad esso, è definito positivo, deve valere che $\iota_+(\varphi) \geq \iota_+(\restr{\varphi}{W})$. Analogamente, $\iota_-(\varphi) \geq \iota_-(\restr{\varphi}{W})$.
\end{remark}

\subsubsection{Classificazione delle segnature per $n = 1$, $2$, $3$}

Sia $\basis$ una base di Sylvester per $\varphi$. Sia $A = M_\basis(\varphi)$. Si indica con $x$, $y$ e $z$
le tre coordinate di $\v \in V$ secondo la base $\basis$. \\

\mbox{($n = 1$)} Vi sono solo tre possibili matrici per $A$:

\begin{itemize}
	\item $A = (0)$, con $\sigma = (0, 0, 1)$, $\rg(\varphi) = 0$ e $\CI(\varphi) = V$,
	\item $A = (1)$, con $\sigma = (1, 0, 0)$, $\rg(\varphi) = 1$ e $\CI(\varphi) = \zerovecset$,
	\item $A = (-1)$, con $\sigma = (0, 1, 0)$, $\rg(\varphi) = 1$ e $\CI(\varphi) = \zerovecset$.
\end{itemize}

\vskip 0.1in

\mbox{($n = 2$)} Vi sono sei possibili matrici per $A$:

\begin{itemize}
	\item $A = 0$, con $\sigma = (0, 0, 2)$, $\rg(\varphi) = 0$ e $\CI(\varphi) = V$,
	\item $A = \Matrix{1 & 0 \\ 0 & 0}$, con $\sigma = (1, 0, 1)$, $\rg(\varphi) = 1$ e $\CI(\varphi) = \{x = 0 \mid \v \in V\} = V^\perp$,
	\item $A = \Matrix{-1 & 0 \\ 0 & 0}$, con $\sigma = (0, 1, 1)$, $\rg(\varphi) = 1$ e $\CI(\varphi) = \{x = 0 \mid \v \in V\} = V^\perp$,
	\item $A = \Matrix{1 & 0 \\ 0 & -1}$, con $\sigma = (1, 1, 0)$, $\rg(\varphi) = 2$ e $\CI(\varphi) = \{x^2 = y^2 \mid \v \in V\}$,
	\item $A = I_2$, con $\sigma = (2, 0, 0)$, $\rg(\varphi) = 2$ e $\CI(\varphi) = \zerovecset$,
	\item $A = -I_2$, con $\sigma = (0, 2, 0)$, $\rg(\varphi) = 2$ e $\CI(\varphi) = \zerovecset$.
\end{itemize}

Si osserva in particolare che $\det(A) = -1 \iff \sigma = (1, 1, 0)$. Pertanto se $M$ è una matrice associata
al prodotto scalare $\varphi$ in una base $\basis'$, $\det(M) < 0 \iff \sigma = (1, 1, 0)$. \\

\mbox{($n = 3$)} Se $A$ contiene almeno uno zero nella diagonale, si può studiare $A$ riconducendosi al caso $n = 2$,
considerando la matrice $A^{1,2}_{1,2}$, e incrementando di uno l'indice di nullità di $\varphi$ (eventualmente
considerando anche come varia il cono isotropo). Altrimenti $A$
può essere rappresentato dalle seguenti quattro matrici:

\begin{itemize}
	\item $A = I_3$, con $\sigma = (3, 0, 0)$, $\rg(\varphi) = 3$ e $\CI(\varphi) = \zerovecset$,
	\item $A = -I_3$, con $\sigma = (0, 3, 0)$, $\rg(\varphi) = 3$ e $\CI(\varphi) = \zerovecset$,
	\item $A = \Matrix{1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1}$, con $\sigma = (2, 1, 0)$, $\rg(\varphi) = 3$ e $\CI(\varphi) = \{ x^2 + y^2 = z^2 \mid \v \in V \}$,
	\item $A = \Matrix{1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & -1}$, con $\sigma = (1, 2, 0)$, $\rg(\varphi) = 3$ e $\CI(\varphi) = \{ y^2 + z^2 = x^2 \mid \v \in V \}$.
\end{itemize}

Si osserva infine che, se $V = \RR^3$ e $\basis$ ne è la base canonica, i coni isotropi delle ultime due matrici rappresentano proprio due coni nello spazio tridimensionale.

\subsubsection{Metodo di Jacobi per il calcolo della segnatura}

\begin{proposition} \label{prop:pre_metodo_jacobi} Sia $\KK$ un campo ordinato
	i cui elementi positivi sono tutti quadrati (e.g.~$\RR$). 
	Sia $W$ un sottospazio di $V$ di dimensione $k$. Sia $W'$ un sottospazio di $V$ di dimensione
	$k+1$. Sia $\sigma(\restr{\varphi}{W}) = (p, q, 0)$, con $p$, $q \in \NN$ e siano $\basis$ e $\basis'$
	due basi di $W$ e $W'$. Siano $B = M_\basis(\restr{\varphi}{W'})$ e $B' = M_{\basis'}(\restr{\varphi}{W}))$. \\
	
	Sia $d := \displaystyle\frac{\det(B')}{\det(B)}$. Allora vale che:
	
	\[ \sigma(\restr{\varphi}{W'}) = \system{(p+1, q, 0) & \se d > 0, \\ (p, q+1, 0) & \se d < 0, \\ (p, q, 1) & \altrimenti.} \]
	
	\vskip 0.05in
\end{proposition}

\begin{proof}
	Dalle precedenti osservazioni, vale che $\iota_+(\restr{\varphi}{W'}) \geq \iota_+(\restr{\varphi}{W})$
	e che $\iota_-(\restr{\varphi}{W'}) \geq \iota_-(\restr{\varphi}{W})$. Inoltre $\restr{\varphi}{W}$
	è non degenere dal momento che $\iota_0(\restr{\varphi}{W}) = 0$, e pertanto
	$p + q = \rg(\restr{\varphi}{W}) = k$. \\
	
	Siano ora $\basis_\perp$ e $\basis_\perp'$ due basi di Sylvester di $W$ e $W'$. Siano $A = M_{\basis_\perp}(\restr{\varphi}{W})$
	e $A' = M_{\basis_\perp'}(\restr{\varphi}{W})$. Allora $\det(A) = 1^p (-1)^q$, mentre $\det(A') = 1^p (-1)^q \, d'$,
	dove $d' \in \{-1, 0, 1\}$. Allora $\det(A') = \det(A) d' \implies d' = \frac{\det(A')}{\det(A)}$, dal
	momento che $\det(A) \neq 0$, essendo $\restr{\varphi}{W}$ non degenere. \\
	
	In particolare, $\sigma(\restr{\varphi}{W'}) = (p, q, 1)$ se e solo se $\det(A') = 0 \implies d' = 0$. Dal
	momento che $\det(A') = 0 \iff \det(B') = 0$, $d' = 0 \iff d = 0$. Pertanto si conclude che
	$\sigma(\restr{\varphi}{W'}) = (p, q, 1) \iff d = 0$. \\
	
	Al contrario, $\sigma(\restr{\varphi}{W'}) = (p+1, q, 0)$ se e solo se $d' = 1$, ossia se e solo se $\det(A')$
	e $\det(A)$ sono concordi di segno. Dal momento che il segno è un invariante del cambiamento di base per la
	matrice associata a $\varphi$, $d' = 1$ se e solo se $\det(B)$ e $\det(B')$ sono concordi di segno, ossia
	se e solo se $d > 0$. Pertanto $\sigma(\restr{\varphi}{W'}) = (p+1, q, 0) \iff d > 0$. Analogamente si
	verifica che $\sigma(\restr{\varphi}{W'}) = (p, q+1, 0) \iff d < 0$, da cui la tesi.
\end{proof}

\begin{algorithm}[metodo di Jacobi] \label{alg:metodo_jacobi}
	Sia $\basis$ una base di $V$ e sia $A = M_\basis(\varphi)$. Se il determinante di ogni minore di testa\footnote{In realtà il metodo si estende ad ogni successione di minori coerente con un'estensione di base (i.e.~i minori principali di $A$).}
	di $A$ (ossia dei minori della forma $A^{1, \ldots, i}_{1, \ldots, i}$, con $1 \leq i \leq n-1$) è diverso
	da zero, è possibile applicare il \textbf{metodo di Jacobi} per il calcolo della segnatura di $\varphi$. \\
	
	Sia $d_i = \det\left(A^{1,\ldots,i}_{1,\ldots,i}\right)$ $\forall 1 \leq i \leq n$ e si ponga $d_0 := 1$. Allora, per la \textit{Proposizione \ref{prop:pre_metodo_jacobi}}, $\iota_+$ corrisponde al numero di permanenze del segno
	tra elementi consecutivi (escludendo $0$) di $(d_i)$, mentre $\iota_-$ corrisponde al numero di variazioni
	del segno (anche stavolta escludendo $0$). Infine $\iota_0$ può valere solo $0$ o $1$, dove $\iota_0 = 1 \iff \det(A) = 0$.
\end{algorithm}

\begin{example}
	Sia $A = \Matrix{1 & 1 & 0 \\ 1 & 2 & -1 \\ 0 & -1 & 4} \in M(3, \RR)$. \\
	
	\vskip 0.1in
	
	Si calcola la segnatura di $\varphi_A$ mediante
	il metodo di Jacobi. Poiché $A$ è la matrice associata di $\varphi_A$ nella base canonica di $\RR^3$,
	si può applicare il metodo di Jacobi direttamente su $A$. \\
	
	Si calcola allora la successione dei $d_i$:
	
	\begin{enumerate}
		\item $d_1 = \det(1) = 1$,
		\item $d_2 = \det\Matrix{1 & 1 \\ 1 & 2} = 2 - 1 = 1$,
		\item $d_3 = \det(A) = (8 - 1) - 4 = 3$.
	\end{enumerate}
	
	Dal momento che vi sono tre permanenze di segno, si conclude che $\sigma(\varphi_A) = (3, 0, 0)$, ossia
	che $\varphi_A$ è definito positivo.
\end{example}

\subsubsection{Criterio di Sylvester per la definitezza di un prodotto scalare}

\begin{proposition}[criterio di Sylvester per i prodotti definiti] Sia $\KK= \RR$.
	Sia $\basis$ una base di $V$, e sia $A = M_\basis(\varphi)$. Sia $d_i = \det\left(A^{1,\ldots,i}_{1,\ldots,i}\right)$.
	Allora $\varphi$ è definito positivo se e solo se $d_i > 0$ $\forall 1 \leq i \leq n$. Analogamente
	$\varphi$ è definito negativo se e solo se $(-1)^i \, d_i > 0$ $\forall 1 \leq i \leq n$. 
\end{proposition}

\begin{proof}
	Si osserva che $\varphi$ è definito positivo se e solo se $\iota_+ = n$. Pertanto, per il
	\textit{\nameref{alg:metodo_jacobi}}, $\varphi$ è definito positivo se e solo se vi sono
	solo permanenze di segno tra elementi consecutivi nella successione $(d_i)$, e quindi
	se e solo se $d_i > 0$ $\forall 1 \leq i \leq n$. Analogamente $\varphi$ è definito
	negativo se e solo se $\iota_- = n$, e quindi se e solo se vi sono solo variazioni
	di segno $\iff d_i > 0$ se $i$ è pari e $d_i < 0$ se $i$ è dispari $\iff (-1)^i \, d_i > 0$, $\forall 1 \leq i \leq n$.
\end{proof}

\section{Sottospazi isotropi e indice di Witt}

\begin{definition}[sottospazio isotropo]
	Sia $W$ un sottospazio di $V$. Allora si dice che $W$ è un \textbf{sottospazio isotropo} di $V$
	se $\restr{\varphi}{W} = 0$.
\end{definition}

\begin{remark}\nl
	\li $V^\perp$ è un sottospazio isotropo di $V$. \\
	\li $\vec{v}$ è un vettore isotropo $\iff$ $W = \Span(\vec v)$ è un sottospazio isotropo di $V$. \\
	\li $W \subseteq V$ è isotropo $\iff$ $W \subseteq W^\perp$.
\end{remark}

\begin{proposition} \label{prop:disuguaglianza_sottospazio_isotropo}
	Se $W$ è un sottospazio isotropo di $V$, allora
	$\dim W \leq \floor{\frac{\dim V + \dim V^\perp}{2}}$.
\end{proposition}

\begin{proof}
	Poiché $W$ è un sottospazio isotropo di $V$, vale che $W \subseteq W^\perp$. Allora vale che:
	
	\begin{equation} \label{eq:disuguaglianza_sottospazio_isotropo_1}
		\dim W \leq \dim W^\perp.
	\end{equation}
	
	\vskip 0.05in

	Inoltre, per la formula delle dimensioni del prodotto scalare, vale anche che:
	
	\begin{equation} \label{eq:disuguaglianza_sottospazio_isotropo_2}
		\dim W^\perp = \dim V + \dim (W \cap V^\perp) - \dim W.
	\end{equation}
	
	\vskip 0.05in
	 
	 Sostituendo allora l'equazione \eqref{eq:disuguaglianza_sottospazio_isotropo_2} nella disuguaglianza
	 \eqref{eq:disuguaglianza_sottospazio_isotropo_1}, si ottiene che $\dim W \leq \frac{\dim V + \dim (W \cap V^\perp)}{2}$. Dal momento che $W \cap V^\perp \subseteq V^\perp$,
	 $\dim (W \cap V^\perp) \leq \dim V^\perp$, e quindi $\dim W \leq \frac{\dim V + \dim V^\perp}{2}$. Poiché $\dim W$ è un numero naturale, vale come conseguenza la tesi.
\end{proof}

\begin{definition}[indice di Witt]
	Si definisce l'\textbf{indice di Witt} $W(\varphi)$ di $(V, \varphi)$
	come la massima dimensione di un sottospazio isotropo di $V$. 
\end{definition}

\begin{remark}\nl
	\li Se $W$ è isotropo e $\varphi$ è non degenere, il risultato della \textit{Proposizione \ref{prop:disuguaglianza_sottospazio_isotropo}} si riduce alla disuguaglianza
	$\dim W \leq \floor{\frac{1}{2} \dim V}$, \\
	\li Se $\varphi > 0$ o $\varphi < 0$, $W(\varphi) = 0$. Infatti ogni sottospazio non nullo $W$ di $V$
	non ammette vettori isotropi, da cui si deduce che $\restr{\varphi}{W} \neq 0$. \\
	\li Ancora per la \textit{Proposizione \ref{prop:disuguaglianza_sottospazio_isotropo}}, vale che $W(\varphi) \leq \floor{\frac{\dim V + \dim V^\perp}{2}}$.
\end{remark}

\begin{proposition}
	Sia $\KK = \CC$. Allora
	$W(\varphi) = \floor{\frac{\dim V + \dim V^\perp}{2}}$.
\end{proposition}

\begin{proof}
	Sia $\basis$ una base di Sylvester per $V$. In particolare, detto $k := \dim V^\perp$; sia $\basis = \{ \vv 1, \ldots, \vv{n-k}, \uu 1, \ldots, \uu k \}$ ordinata in modo tale che $\vv i$ non
	sia isotropo per $1 \leq i \leq n-k$ e che $\uu i$ sia invece isotropo per
	$1 \leq i \leq k$. Si costruisca allora l'insieme $\basis' = \{\vv 1 ' := \vv 1 + i \vv 2, \, \vv 2 ' := \vv 3 + i \vv 4, \ldots, \uu 1, \ldots, \uu k\}$ ottenuto prendendo in ordine quante più coppie distinte possibili di
	vettori $\vec i$ e aggiungendo al vettore con indice minore della coppia il vettore
	con indice maggiore moltiplicato per $i$. In questo modo si è costruita un insieme linearmente indipendente
	contenente $\floor{\frac{n-k}{2}} + k = \floor{\frac{\dim V - \dim V^\perp}{2}} + \dim V^\perp = \floor{\frac{\dim V + \dim V^\perp}{2}}$. \\
	
	Sia allora $W = \Span(\basis')$. I vettori della forma $\uu i$ con $1 \leq i \leq k$ sono
	chiaramente già ortogonali con gli altri vettori della base $\basis'$ di $V$. Si consideri allora
	il prodotto $\varphi(\vv i', \vv j')$. Se $i \neq j$, il prodotto ha argomenti tra di
	loro già ortogonali per costruzione di $\basis$; se invece $i = j$, detto $\vv i' = \vv s + i \vv {s+1}$ con $s \in \NN$, $\varphi(\vv i', \vv i') = \varphi(\vv s, \vv s) - \varphi(\vv {s+1}, \vv {s+1}) = 1 - 1 = 0$. Allora $M_{\basis'}(\restr{\varphi}{W}) = 0 \implies \restr{\varphi}{W} = 0$. Pertanto $W$ è un sottospazio isotropo di dimensione
	$\floor{\frac{\dim V + \dim V^\perp}{2}}$. Poiché per la \textit{Proposizione \ref{prop:disuguaglianza_sottospazio_isotropo}} tale dimensione maggiora tutte le
	dimensioni dei sottospazi isotropi, si conclude che $W(\varphi) = \floor{\frac{\dim V + \dim V^\perp}{2}}$, da cui la tesi.
\end{proof}

\begin{proposition}
	Sia $\KK = \RR$. Allora
	$W(\varphi) = \iota_0 + \min\{\iota_+(\varphi), \iota_-(\varphi)\}$.
\end{proposition}

\begin{proof}
	Senza perdità di generalità si assuma $\iota_-(\varphi) \leq \iota_+(\varphi)$ (il caso $\iota_-(\varphi) > \iota_+(\varphi)$ è analogo). Sia $W$ un sottospazio con $\dim W > \iota_0(\varphi) + \iota_-(\varphi)$. Sia $W^+$
	un sottospazio con $\dim W^+ = \iota_+(\varphi)$ e $\restr{\varphi}{W^+} > 0$. Si
	osserva pertanto che $\dim W + \dim W^+ > \iota_+(\varphi) + \iota_-(\varphi) + \iota_0(\varphi) = n$: allora, per la formula
	di Grassmann, $n - \dim(W \cap W^+) < \dim (W + W^+) \leq n \implies \dim (W \cap W^+) > 0$. Quindi $\exists \w \in W$, $\w \neq \vec 0$ tale che $\varphi(\w, \w) > 0$, da cui
	si ricava che $W$ non è isotropo. Pertanto $W(\varphi) \leq \iota_0(\varphi) + \iota_-(\varphi)$. \\
	
	Siano $a := \iota_+(\varphi)$, $b := \iota_-(\varphi)$ e $c := \iota_0(\varphi)$.
	Sia $\basis = \{ \vv 1, \ldots, \vv a, \ww 1, \ldots, \ww b, \uu 1, \ldots, \uu c \}$ una base di Sylvester per $\varphi$. Siano $\vv 1$, ..., $\vv a$ tali che $\varphi(\vv i, \vv i) = 1$
	con $1 \leq i \leq a$. Analogamente siano $\ww 1$, ..., $\ww b$ tali che $\varphi(\ww i, \ww i) = -1$ con
	$1 \leq i \leq b$ e siano $\uu 1$, ..., $\uu c$ tali che $\varphi(\uu i, \uu i) = 0$ con
	$1 \leq i \leq c$. Posto $\basis' = \{ \vv 1 ' := \vv 1 + \ww 1, \ldots, \vv b ' := \vv b + \ww b, \uu 1, \ldots, \uu c \}$, si definisca $W = \Span(\basis')$. \\
	
	Si osserva che $\basis'$ è linearmente indipendente, e dunque che $\dim W = \iota_-(\varphi) + \iota_0(\varphi)$. Chiaramente i vettori $\uu i$ sono ancora ortogonali con gli elementi di $\basis'$ e sono tali per cui $\varphi(\uu i, \uu i) = 0$ $\forall 1 \leq i \leq c$. Inoltre
	$\varphi(\vv i ', \vv j ') = \varphi(\vv i + \ww i, \vv j + \ww j)$. Se $i \neq j$, allora
	$\varphi(\vv i ', \vv j ') = 0$, dal momento che i vettori di $\basis$ sono a due a due ortogonali
	tra loro. Se invece $i = j$, allora $\varphi(\vv i ', \vv j ') = \varphi(\vv i, \vv i) + \varphi(\ww i, \ww i) = 1-1=0$. Quindi $M_{\basis'}(\restr{\varphi}{W}) = 0$, da cui si conclude che $\restr{\varphi}{W} = 0$.
	Pertanto $W(\varphi) \geq \iota_0(\varphi) + \iota_-(\varphi)$, e quindi si conclude che $W(\varphi) = \iota_0(\varphi) + \iota_-(\varphi)$, da cui la tesi.
\end{proof}

\section{Isometrie tra spazi vettoriali}

\begin{definition} (isometria)
	Dati due spazi vettoriali $(V, \varphi)$ e
	$(V', \varphi')$ dotati di prodotto scalare sullo stesso campo $\KK$, si dice che
	$V$ e $V'$ sono \textbf{isometrici} se esiste un isomorfismo
	$f$, detto \textit{isometria}, che preserva tali che prodotti, ossia tale che:
	
	\[ \varphi(\vec v, \vec w) = \varphi'(f(\vec v), f(\vec w)). \]
\end{definition}

\vskip 0.01in

\begin{proposition} Sia $f : V \to V'$ un isomorfismo. Allora $f$ è un'isometria $\iff$ $\forall$ base $\basis = \{ \vv 1, \ldots, \vv n \}$ di $V$, $\basis' = \{ f(\vv 1), \ldots, f(\vv n) \}$ è una base di $V'$ e $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$ $\iff$ $\exists$ base $\basis = \{ \vv 1, \ldots, \vv n \}$ di $V$ tale che $\basis' = \{ f(\vv 1), \ldots, f(\vv n) \}$ è una base di $V'$ e $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$.
	\label{prop:isometrie_base_sufficiente}
\end{proposition}

\begin{proof} Se $f$ è un'isometria, detta $\basis$ una base di $V$, $\basis' = f(\basis)$ è una base di $V'$
	dal momento che $f$ è prima di tutto un isomorfismo. Inoltre, dacché $f$ è un'isometria, vale sicuramente che
	$\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. \\
	
	Sia ora assunto per ipotesi che $\forall$ base $\basis = \{ \vv 1, \ldots, \vv n \}$ di $V$, $\basis' = \{ f(\vv 1), \ldots, f(\vv n) \}$ è una base di $V'$ e $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. Allora, analogamente a prima, detta $\basis = \{ \vv 1, \ldots, \vv n \}$ una base di $V$, $\basis' = f(\basis)$ è una base di $V'$, e in quanto tale,
	per ipotesi, è tale che $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. \\
	
	Sia infine assunto per ipotesi che $\exists$ base $\basis = \{ \vv 1, \ldots, \vv n \}$ di $V$ tale che $\basis' = \{ f(\vv 1), \ldots, f(\vv n) \}$ è una base di $V'$ e $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. Siano $\v$, $\w \in V$. Allora $\exists a_1$, ..., $a_n$, $b_1$, ..., $b_n \in \KK$
	tali che $\v = a_1 \vv 1 + \ldots + a_n \vv n$ e $\w = b_1 \vv 1 + \ldots + b_n \vv n$. Si ricava pertanto
	che:
	
	\[ \varphi'(f(\v), f(\w)) = \sum_{i=1}^n \sum_{j=1}^n a_i b_j \, \varphi'(f(\vv i), f(\vv j)) =
	\sum_{i=1}^n \sum_{j=1}^n a_i b_j \, \varphi(\vv i, \vv j) = \varphi(\v, \w), \]
	
	da cui la tesi.
\end{proof}

\begin{proposition} Sono equivalenti le seguenti affermazioni:
		\label{prop:isometrie_equivalenza_base}
	\begin{enumerate}[(i)]
		\item $V$ e $V'$ sono isometrici;
		\item $\forall$ base $\basis$ di $V$, base $\basis'$ di $V'$,
		$M_\basis(\varphi)$ e $M_{\basis'}(\varphi')$ sono congruenti;
		\item $\exists$ base $\basis$ di $V$, base $\basis'$ di $V'$ tale che
		$M_\basis(\varphi)$ e $M_{\basis'}(\varphi')$ sono congruenti.
	\end{enumerate}
\end{proposition}

\begin{proof} Se $V$ e $V'$ sono isometrici, sia $f : V \to V'$ un'isometria. Sia $\basisC = \{ \vv 1, \ldots, \vv n \}$ una base di $V$. Allora, poiché $f$ è anche un isomorfismo, $\basisC' = f(\basisC)$ è una base di $V$ tale che
	$\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. Pertanto $M_\basisC(\varphi) = M_{\basisC'}(\varphi')$. Si conclude allora che, cambiando base in $V$ (o in $V'$), la matrice associata
	al prodotto scalare varia per congruenza dalla formula di cambiamento di base per il prodotto scalare, da cui si ricava che per ogni scelta di $\basis$ base di $V$ e di $\basis'$ base di $V'$, $M_\basis(\varphi) \cong M_{\basis'}(\varphi')$. Inoltre, se tale risultato è vero per ogni $\basis$ base di $V$ e di $\basis'$ base di $V'$, dal momento che sicuramente esistono due basi $\basis$, $\basis'$ di $V$ e $V'$, vale anche (ii) $\implies$ (iii). \\
	
	Si dimostra ora (iii) $\implies$ (i). Per ipotesi $M_\basis(\varphi) \cong M_{\basis'}(\varphi')$, quindi
	$\exists P \in \GL(n, \KK) \mid M_{\basis'}(\varphi') = P^\top M_\basis(\varphi) P$. Allora $\exists$ $\basis''$
	base di $V'$ tale che $P = M_{\basis''}^{\basis'}(\Idv)$, da cui $P\inv = M_{\basis'}^{\basis''}(\varphi)$. Per la formula di cambiamento di base del prodotto
	scalare, $M_{\basis''}(\varphi') = (P\inv)^\top M_{\basis'} P\inv = M_\basis(\varphi)$. Detta
	$\basis'' = \{ \ww 1, \ldots, \ww n \}$, si costruisce allora l'isomorfismo $f : V \to V'$ tale
	che $f(\vv i) = \ww i$ $\forall 1 \leq i \leq n$.. Dal momento che per costruzione $M_\basis(\varphi) = M_{\basis''}(\varphi')$,
	$\varphi(\vv i, \vv j) = \varphi'(\ww i, \ww j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$.
	Si conclude dunque, dalla \textit{Proposizione \ref{prop:isometrie_base_sufficiente}}, che $\varphi(\v, \w) = \varphi'(f(\v), f(\w))$ $\forall \v, \w \in V$, e dunque
	che $f$ è un'isometria, come desiderato dalla tesi. 
\end{proof}

\begin{proposition} $(V, \varphi)$ e $(V', \varphi')$ spazi vettoriali
	su $\RR$ sono
	isometrici $\iff$ $\varphi$ e $\varphi'$ hanno la stessa segnatura.
\end{proposition}

\begin{proof}Si dimostrano le due implicazioni separatamente.\nl\nl
	\rightproof Per la \textit{Proposizione \ref{prop:isometrie_equivalenza_base}}, esistono due basi $\basis$ e $\basis'$, una di $V$ e una di $V'$,
	tali che $M_\basis(\varphi) \cong M_{\basis'}(\varphi')$. Allora, poiché queste due matrici sono congruenti, esse devono condividere anche la stessa
	segnatura, che è invariante completo per congruenza, e dunque le segnature di $\varphi$ e di $\varphi'$ coincidono. \\
	
	\leftproof Se $\varphi$ e $\varphi'$ hanno la stessa segnatura, allora, detta $\basis$
	una base di $V$ e $\basis'$ una base di $V'$, $M_\basis(\phi) \cong M_{\basis'}(\phi')$.
	Allora, per la \textit{Proposizione \ref{prop:isometrie_equivalenza_base}}, $V$ e $V'$ sono isometrici.
\end{proof}

%TODO: presentare prima i funzionali rappresentabili e poi il teorema di Riesz
\section{Teorema di rappresentazione di Riesz}

\begin{theorem} (di rappresentazione di Riesz per il prodotto scalare) 
	Sia $V$ uno spazio vettoriale e sia $\varphi$ un suo prodotto scalare
	non degenere. Allora per ogni $f \in V^*$ esiste un unico $\v \in V$ tale che
	$f(\w) = \varphi(\v, \w)$ $\forall \w \in V$.
\end{theorem}

\begin{proof}
	Si consideri l'applicazione $a_\varphi$. Poiché $\varphi$ non è degenere, $\Ker a_\varphi = V^\perp = \zerovecset$, da cui si deduce che $a_\varphi$ è un isomorfismo. Quindi $\forall f \in V^*$ esiste
	un unico $\v \in V$ tale per cui $a_\varphi(\v) = f$, e dunque tale per cui $\varphi(\v, \w) = a_\varphi(\v)(\w) = f(\w)$ $\forall \w \in V$.
\end{proof}

\begin{proof}[Dimostrazione costruttiva]
	Sia $\basis = \{ \vv 1, \ldots, \vv n \}$ una base ortogonale di $V$ per $\varphi$. Allora $\basis^*$ è una base di $V^*$. In
	particolare $f = f(\vv 1) \vec{v_1^*} + \ldots + f(\vv n) \vec{v_n^*}$. Sia $\v = \frac{f(\vv 1)}{\varphi(\vv 1, \vv 1)} \vv 1 + \ldots + \frac{f(\vv n)}{\varphi(\vv n, \vv n)}$. Detto $\w = a_1 \vv 1 + \ldots + a_n \vv n$,
	si deduce che $\varphi(\v, \w) = a_1 f(\vv 1) + \ldots + a_n f(\vv n) = f(\w)$. Se esistesse $\v' \in V$ con
	la stessa proprietà di $\v$, $\varphi(\v, \w) = \varphi(\v', \w) \implies \varphi(\v - \v', \w)$ $\forall \w \in V$. Si deduce dunque che $\v - \v' \in V^\perp$, contenente solo $\vec 0$ dacché $\varphi$ è non degenere;
	e quindi si conclude che $\v = \v'$, ossia che esiste solo un vettore con la stessa proprietà di $\v$.
\end{proof}
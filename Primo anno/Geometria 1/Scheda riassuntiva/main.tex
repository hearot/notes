\documentclass[10pt,landscape]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts}
\usepackage{multicol,multirow}
\usepackage{marvosym}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\usepackage{personal_commands}

\setlength{\extrarowheight}{0pt}

\ifthenelse{\lengthtest { \paperwidth = 11in}}
{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
	{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
}
%\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
	{-1ex plus -.5ex minus -.2ex}%
	{0.5ex plus .2ex}%x
	{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
	{-1explus -.5ex minus -.2ex}%
	{0.5ex plus .2ex}%
	{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
	{-1ex plus -.5ex minus -.2ex}%
	{1ex plus .2ex}%
	{\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}
% -----------------------------------------------------------------------

\title{Scheda riassuntiva di Geometria 1}

\begin{document}
	
	\parskip=0.7ex
	
	\raggedright
	\footnotesize
	
	\begin{center}
		\Large{\textbf{Scheda riassuntiva di Geometria 1}} \\
	\end{center}
	\begin{multicols}{3}
		\setlength{\premulticols}{1pt}
		\setlength{\postmulticols}{1pt}
		\setlength{\multicolsep}{1pt}
		\setlength{\columnsep}{2pt}
		
		\subsection{Alcuni accenni alla geometria di \texorpdfstring{$\RR^3$}{R\^{}3}}
		
		Si definisce prodotto scalare la forma
		bilineare simmetrica unicamente determinata da $\innprod{\vec{e_i}, \vec{e_j}} = \delta_{ij}$. Vale la seguente identità: $\innprod{(x, y, z), (x', y', z')} = xx' + yy' + zz'$.
		
		Inoltre $\innprod{\vec{a}, \vec{b}} = \card{\vec{a}} \card{\vec{b}} \cos(\theta)$, dove $\theta$ è l'angolo compreso tra i due vettori.
		Due vettori $\vec{a}$, $\vec{b}$ si dicono ortogonali
		se e solo se $\innprod{\vec{a}, \vec{b}} = 0$.
		
		Si definisce prodotto vettoriale la forma bilineare alternante
		da $\RR^3 \times \RR^3$
		in $\RR^3$ tale che $\vec{e_1} \times \vec{e_2} = \vec{e_3}$,
		$\vec{e_2} \times \vec{e_3} = \vec{e_1}$,
		$\vec{e_3} \times \vec{e_1} = \vec{e_2}$ e
		$\vec{e_i} \times \vec{e_i} = \vec{0}$. Dati due
		vettori $(x, y, z)$ e $(x', y', z')$, si può determinarne
		il prodotto vettoriale informalmente come:
		
		\[ \begin{vmatrix}
			\vec{e_1} & \vec{e_2} & \vec{e_3} \\
			x & y & z \\
			x' & y' & z'
		\end{vmatrix} . \]
		
		Vale l'identità $\card{\vec{a} \times \vec{b}} = \card{\vec{a}} \card{\vec{b}} \sin(\theta)$, dove $\theta$ è l'angolo con cui, ruotando di
		$\theta$ in senso antiorario $\vec{a}$, si ricade su $\vec{b}$.
		Due vettori $\vec{a}$, $\vec{b}$ si dicono paralleli se $\exists
		k \mid \vec{a} = k \vec{b}$, o equivalentemente se
		$\vec{a} \times \vec{b} = \vec{0}$. Altrettanto si può dire
		se $\innprod{\vec{a}, \vec{b}} = \card{\vec{a}} \card{\vec{b}}$
		(i.e.~$\cos(\theta) = 1 \implies \theta = 0$).
		
		Una retta in $\RR^3$ è un sottospazio affine della
		forma $\vec{v} + \Span(\vec{r})$. Analogamente
		un piano è della forma $\vec{v} + \Span(\vec{x}, \vec{y})$.
		
		Nella forma cartesiana, un piano è della forma $ax+by+cz=d$,
		dove $(a,b,c)$ è detta normale del piano. Una retta è
		l'intersezione di due piani, e dunque è un sistema lineare
		di due equazioni di un piano. Due piani sono perpendicolari
		fra loro se e solo se le loro normali sono ortogonali. Due
		piani sono paralleli se e solo se le loro normali sono parallele.
		Il vettore $\vec{r}$ che genera lo $\Span$ di una retta che è
		intersezione di due piani può essere computato come
		prodotto vettoriale delle normali dei due piani.
		
		Valgono le seguenti identità:
		
		\begin{itemize}
			\item $\vec{a} \times (\vec{b} \times \vec{c}) =
			\innprod{\vec{a}, \vec{c}}\,\vec{b} - \innprod{\vec{a}, \vec{b}}\,\vec{c}$ (\textit{identità di Lagrange}),
			\item $\vec{a} \times (\vec{b} \times \vec{c}) + \vec{b} \times (\vec{c} \times \vec{a}) + \vec{c} \times (\vec{a} \times \vec{b}) =
			\vec{0}$ (\textit{identità di Jacobi}).
		\end{itemize}
		
		Dati tre punti $\vec{a}$, $\vec{b}$, $\vec{c}$, il volume
		del parallelepipedo individuato da questi punti è:
		
		\[\card{\det\begin{pmatrix}\vec{a} \\ \vec{b} \\ \vec{c}\end{pmatrix}} =
		\card{\innprod{\vec{a}, \vec{b} \times \vec{c}}}.\]
		
		Tre punti sono complanari se e solo se il volume di tale parallelpipedo è nullo
		(infatti questo è equivalente a dire che almeno uno dei tre punti
		si scrive come combinazione lineare degli altri due).
		
		
		\subsection{Proprietà generali di uno spazio vettoriale}
		
		Uno spazio vettoriale $V$ su un campo $\KK$ soddisfa i seguenti
		assiomi:
		
		\begin{itemize}
			\item $(V, +)$ è un gruppo abeliano,
			\item il prodotto esterno da $\KK \times V$ in $V$ è
			associativo rispetto agli scalari (i.e. $a(b\vec{v}) = (ab)\vec{v}$),
			\item $1_{\KK} \cdot \vec{v} = \vec{v}$,
			\item il prodotto esterno è distributivo da ambo i
			lati (i.e. $(a+b)\vec{v} = a\vec{v} + b\vec{v}$ e
			$a(\vec{v} + \vec{w}) = a\vec{v} + a\vec{w}$.
		\end{itemize}
		
		Un insieme di vettori $I$ si dice linearmente indipendente se
		una qualsiasi combinazione lineare di un suo sottinsieme
		finito è nulla se e solo se i coefficienti dei vettori
		sono tutti nulli. Si dice linearmente dipendente in caso
		contrario.
		
		Un insieme di vettori $G$ si dice generatore di $V$ se ogni vettore
		di $V$ si può scrivere come combinazione lineare di un numero
		finito di elementi di $G$, ossia se $V = \Span(G)$.
		
		Una base è un insieme contemporaneamente linearmente indipendente
		e generatore di $V$. Equivalentemente una base è un insieme generatore
		minimale rispetto all'inclusione e un insieme linearmente indipendente
		massimale, sempre rispetto all'inclusione. Ogni spazio vettoriale,
		anche quelli non finitamente generati,
		ammettono una base. La dimensione della base è unica ed è il
		numero di elementi dell'insieme che è base.
		
		Dato un insieme linearmente indipendente $I$ in uno spazio di dimensione
		finita, tale insieme, data una base $\basis$, può essere esteso
		a una base $T$ che contiene $I$ e che è completato da
		elementi di $\basis$.
		
		Analogamente, dato un insieme generatore finito $G$, da esso
		si può estrarre sempre una base dello spazio.
		
		Uno spazio vettoriale fondato su un campo infinito
		con un insieme di vettori infinito non
		è mai unione finita di sottospazi propri. Un insieme linearmente
		indipendente di $V$ con esattamente $\dim V$ elementi è una
		base di $V$. Analogamente, un insieme generatore di $V$ con esattamente
		$\dim V$ elementi è una base di $V$. In generale, quando il campo su
		cui fonda lo spazio vettoriale è ambiguo, si scrive $\dim_\KK V$ o $[V : \KK]$
		per indicarne la dimensione relativa al campo $\KK$ (per esempio un $\CC$-spazio è
		compatibilmente anche un $\RR$-spazio).
		
		Sia $\basis = \{\vec{v_1}, \ldots, \vec{v_n}\}$ una base ordinata dello spazio vettoriale $V$.
		
		\begin{itemize}
			\item $\zerovecset$ e $V$ sono detti sottospazi banali,
			\item lo $\Span$ di $n$ vettori è il più piccolo sottospazio
			di $V$ contenenti tali vettori,
			\item $\Span(\basis) = V$,
			\item $\Span(\emptyset) = \zerovecset$,
			\item $U \cap W$ è sempre un sottospazio se $U$ e $W$ sono due sottospazi di $V$,
			\item $U \cup W$ è un sottospazio se e solo se $U \subseteq W$ o $U \supseteq W$ (e quindi $U \cup W = U$ o $U \cup W = W$),
			\item dato $X$ generatore di $V$, $X \setminus \{\vec{x_0}\}$
			genera $V \iff \vec{x_0} \in \Span(X \setminus \{\vec{x_0}\})$,
			\item $X \subseteq Y$ è un sottospazio di $Y \iff \Span(X) = X$,
			\item $\Span(X) \subseteq Y \iff X \subseteq Y$, se $Y$ è uno spazio,
			\item $\Span(\Span(A)) = \Span(A)$,
			\item se $I$ è un insieme linearmente indipendente di $V$,
			allora $\card{I} \leq \dim V$,
			\item se $G$ è un insieme generatore di $V$, allora
			$\card{G} \geq \dim V$,
			\item $[\vec{v}]_\basis$ è la rappresentazione
			di $\vec{v}$ nella base ordinata $\basis$, ed è
			un vettore di $\KK^n$ che alla coordinata $i$-esima
			associa il coefficiente di $\vec{v_i}$ nella combinazione
			lineare di $\vec{v}$ nella base $\basis$,
			\item la rappresentazione nella base $\basis$ è sempre
			unica ed esiste sempre (è quindi un isomorfismo tra $V$ e
			$\KK^n$),
			\item si definisce base canonica di $\KK^n$ la base
			$e = \{\vec{e_1}, \ldots, \vec{e_n}\}$, dove
			$\vec{e_i}$ è un vettore con tutte le coordinate nulle,
			eccetto per la $i$-esima, che è pari ad $1$ (pertanto
			$\dim \KK^n = n$),
			\item una base naturale di $M(m, n, \KK)$ è data
			da $\basis = \{E_{11}, E_{12}, \ldots, E_{1n}, \ldots, E_{mn}\}$,
			dove $E_{ij}$ è una matrice con tutti gli elementi nulli, eccetto
			quello nel posto $(i, j)$, che è pari ad $1$ (dunque
			$\dim M(m, n, \KK) = mn$),
			\item le matrici $A$ di taglia $n$ tali che $A^\top = A$ formano il
			sottospazio $\Sym(n, \KK)$ di $M(n, \KK)$, detto sottospazio delle matrici
			simmetriche, la cui base naturale è data da
			$\basis' = \{E_{ij} + E_{ji} \in \basis \mid i < j\} \cup
			\{E_{ij} \in \basis \mid i = j\}$, dove $\basis$ è la
			base naturale di $M(m, n, \KK)$ (dunque $\dim \Sym(n, \KK) = \frac{n(n+1)}{2}$),
			\item le matrici $A$ di taglia $n$ tali che $A^\top = -A$ formano il
			sottospazio $\Lambda(n, \KK)$ di $M(n, \KK)$, detto sottospazio delle matrici
			antisimmetriche, la cui base naturale, se $\Char \KK \neq 2$, è data da
			$\basis' = \{E_{ij} - E_{ji} \in \basis \mid i < j\}$, dove $\basis$ è la
			base naturale di $M(m, n, \KK)$ (dunque $\dim \Lambda(n, \KK) = \frac{n(n-1)}{2}$),
			\item se invece $\Char \KK = 2$, le matrici antisimmetriche sono esattamente
			le matrici simmetriche,
			\item poiché $\Sym(n, \KK) \cap \Lambda(n, \KK) = \zerovecset$ e
			$\dim \Sym(n, \KK) + \dim \Lambda(n, \KK) = \dim M(n, \KK)$,
			vale che $M(n, \KK) = \Sym(n, \KK) \oplus \Lambda(n, \KK)$,
			\item una base naturale di $\KK[x]$ è data da $\basis = \{x^n \mid
			n \in \NN \}$, mentre una di $\KK_t[x]$ è data da $\basis \cap
			\KK_t[x] = \{x^n \mid n \in \NN \land n \leq t\}$ (quindi
			$\dim \KK[x] = \infty$ e $\dim \KK_t[x] = t+1$),
			\item una base naturale di $\KK$ è $1_\KK = \{1_\KK\}$ (quindi
			$\dim \KK = 1$),
			\item un sottospazio di dimensione $1$ si definisce \textit{retta},
			uno di dimensione $2$ \textit{piano}, uno di dimensione $3$
			\textit{spazio}, e, infine, uno di dimensione $n-1$ un iperpiano,
			\item un iperpiano $\Pi$ è sempre rappresentabile da un'equazione cartesiana
			nelle coordinate della rappresentazione della base (infatti ogni
			iperpiano è il kernel di un funzionale $f \in \dual{V}$, e $M^\basis_{1_\KK}(f) \, [\vec{v}]_\basis = 0$ è l'equazione cartesiana; è sufficiente prendere una base di $\Pi$ e completarla
			a base di $V$ con un vettore $\vec{t}$, considerando infine
			$\Ker \dual{\vec{t}}$),
			\item un iperpiano $\Pi$, rappresentato da un'equazione cartesiana $\alpha_1 x_1 + \ldots + \alpha_n x_n = 0$, è in $\KK^n$ esattamente il sottospazio ortogonale a $(\alpha_1, \ldots, \alpha_n)^\top$ tramite il prodotto scalare standard,
			\item in generale, un sistema di equazioni omogenee è l'intersezione di più
			sottospazi ortogonali,
			\item se $\mathbb{F}$ è un'estensione di campo di $\KK$, allora vale $[V : \KK] = [V : \mathbb{F}] [\mathbb{F} : \KK]$ (\textit{teorema delle torri algebriche}).
			
		\end{itemize}
		
		\subsection{Applicazioni lineari, somme dirette, quozienti e
			prodotti diretti}
		
		Un'applicazione da $V$ in $W$ si dice applicazione lineare
		se:
		
		\begin{itemize}
			\item $f(\vec{v} + \vec{w}) = f(\vec{v}) + f(\vec{w})$,
			\item $f(\alpha\vec{v}) = \alpha f(\vec{v})$.
		\end{itemize}
		
		Si definisce $\mathcal{L}(V, W) \subseteq W^V$ come lo spazio delle
		applicazioni lineari da $V$ a $W$. Si definisce
		$\End(V)$ come lo spazio degli endomorfismi di $V$, ossia
		delle applicazioni lineari da $V$ in $V$, dette anche
		operatori. Un'applicazione lineare si dice isomorfismo
		se è bigettiva. La composizione di funzioni è associativa.
		
		Dato un sottospazio $A$ di $V$, si definisce lo spazio
		quoziente $V/A$ come l'insieme quoziente $V/{\sim}$ della relazione
		di equivalenza $\vec{a} \sim \vec{b} \iff a-b \in A$ dotato
		dell'usuale somma e prodotto esterno. Si scrive $[\vec{v}]_A$
		come $\vec{v} + A$ e vale che $A = \vec{0} + A$. In particolare
		$\vec{v} + A = A \iff \vec{v} \in A$.
		
		Siano $f : V \to W$, $h : V \to W$, $g : W \to Z$ tre
		applicazioni lineari.
		$\basis_V$ e $\basis_W$ sono
		due basi rispettivamente di $V$ e $W$. In particolare
		sia $\basis_V = \{\vec{v_1}, \ldots, \vec{v_n}\}$. Si
		ricorda che $\rg(f) = \dim \Im f$. Siano $e$ ed $e'$ le
		basi canoniche rispettivamente di $\KK^n$ e $\KK^m$.
		
		\begin{itemize}
			\item $f(\vec{0}_V) = \vec{0}_W$,
			\item $\Ker f = f^{-1}(\vec{0}_W)$ è un sottospazio di $V$,
			\item $\Im f = f(V)$ è un sottospazio di $W$,
			\item $\Im f = \Span(f(\vec{v_1}), \ldots, f(\vec{v_n}))$,
			\item $f$ è iniettiva $\iff \Ker f = \zerovecset$,
			\item $V/\Ker f \cong \Im f$ (\textit{primo teorema d'isomorfismo}),
			\item $\dim \Ker f + \dim \Im f = \dim V$ (\textit{teorema del rango}, o formula delle dimensioni,
			valido se la dimensione di $V$ è finita),
			\item $g \circ f$ è un'applicazione lineare da $V$ in $Z$,
			\item la composizione di funzioni è associativa e distributiva
			da ambo i lati,
			\item $g \circ (\alpha f) = \alpha (g \circ f) = (\alpha g) \circ f$,
			se $\alpha \in \KK$,
			\item $\Ker f \subseteq \Ker (g \circ f)$,
			\item $\Im (g \circ f) \subseteq \Im g$,
			\item $\dim \Im (g \circ f) = \dim \Im \restr{g}{\Im f} =
			\dim \Im f - \dim \Ker \restr{g}{\Im f} = \dim \Im f -
			\dim (\Ker g \cap \Im f)$ (è sufficiente applicare la formula             delle dimensioni sulla composizione),
			\item $\dim \Im (g \circ f) \leq \min\{\dim \Im g, \dim \Im f\}$,
			\item $\dim \Ker (g \circ f) \leq \dim \Ker g + \dim \Ker f$ (è
			sufficiente applicare la formula delle dimensioni su
			$\restr{(g \circ f)}{\Ker (g \circ f)}$),
			\item $f$ iniettiva $\implies \dim V \leq \dim W$,
			\item $f$ surgettiva $\implies \dim V \geq \dim W$,
			\item $f$ isomorfismo $\implies \dim V = \dim W$,
			\item $g \circ f$ iniettiva $\implies f$ iniettiva,
			\item $g \circ f$ surgettiva $\implies g$ surgettiva,
			\item $f$ surgettiva $\implies \rg(g \circ f) = \rg(g)$,
			\item $g$ iniettiva $\implies \rg(g \circ f) = \rg(f)$,
			\item $M^{\basis_V}_{\basis_W}(f) = \begin{pmatrix} \; [f(\vec{v_1})]_{\basis_W} \, \mid \, \cdots \, \mid \, [f(\vec{v_n})]_{\basis_W} \; \end{pmatrix}$ è la matrice
			associata a $f$ sulle basi $\basis_V$, $\basis_W$,
			\item $M^V_W(f + h) = M^V_W(f) + M^V_W(h)$,
			\item $M^V_Z(g \circ f) = M^W_Z(g) M^V_W(f)$,
			\item data $A \in M(m, n, \KK)$, sia $f_A : \KK^n \to \KK^m$ tale
			che $f_A(\vec{x}) = A \vec{x}$, allora $M^{e}_{e'}(f_A) = A$,
			\item $f$ è completamente determinata dai suoi valori in una
			qualsiasi base di $V$ ($M^{\basis_V}_{\basis_W}$ è un isomorfismo
			tra $\mathcal{L}(V, W)$ e $M(\dim W, \dim V, \mathbb{K})$),
			\item $\dim \mathcal{L}(V, W) = \dim V \cdot \dim W$ (dall'isomorfismo
			di sopra),
			\item $[\,]^{-1}_{\basis_W} \circ M^{\basis_V}_{\basis_W}(f) \circ
			{[\,]_{\basis_V}} = f$,
			\item $[f(\vec{v})]_{\basis_W} = M^{\basis_V}_{\basis_W}(f) \cdot
			[\vec{v}]_{\basis_V}$,
			\item $\Im(f) = [\,]^{-1}_{\basis_W}\left(\Im M^{\basis_V}_{\basis_W}(f)\right)$
			\item $\rg(f) = \rg\left(M^{\basis_V}_{\basis_W}(f)\right)$,
			\item $\Ker(f) = [\,]^{-1}_{\basis_V}\left(\Ker M^{\basis_V}_{\basis_W}(f)\right)$,
			\item $\dim \Ker(f) = \dim \Ker M^{\basis_V}_{\basis_W}(f)$.
		\end{itemize}
		
		Siano $\basis_V'$, $\basis_W'$ altre due basi rispettivamente
		di $V$ e $W$. Allora vale il \textit{teorema del cambiamento
			di base}:
		
		\[ M^{\basis_V'}_{\basis_W'}(f) = M^{\basis_W}_{\basis_W'}(id_W) \,
		M^{\basis_V}_{\basis_W}(f) \, M^{\basis_V'}_{\basis_V}(id_V).\]
		
		Siano $A$ e $B$ due sottospazi di $V$. $\basis_A$ e $\basis_B$ sono
		due basi rispettivamente di $A$ e $B$.
		
		\begin{itemize}
			\item $A+B = \{\vec{a}+\vec{b} \in V \mid \vec{a} \in A, \vec{b} \in
			B\}$ è un sottospazio,
			\item $\dim (A+B) = \dim A + \dim B - \dim (A \cap B)$
			(\textit{formula di Grassmann}),
			\item $A$ e $B$ sono in somma diretta $\iff A \cap B = \zerovecset \iff$ ogni elemento di $A+B$ si scrive in modo unico come somma di
			$\vec{a} \in A$ e $\vec{b} \in B \iff \dim (A+B) = \dim A + \dim B$
			(in tal caso si scrive $A+B = A\oplus B$),
			\item $\dim V/A = \dim V - \dim A$ (è sufficiente applicare il
			teorema del rango alla proiezione al quoziente),
			\item $\dim V \times W = \dim V + \dim W$ ($\basis_V \times \{\vec{0}_W\} \cup \{\vec{0}_V\} \times \basis_W$ è una base
			di $V \times W$).
		\end{itemize}
		
		Si definisce \textit{immersione} da $V$ in $V \times W$
		l'applicazione lineare $i_V$ tale che $i_V(\vec{v}) = (\vec{v}, \vec{0})$.
		Si definisce \textit{proiezione} da $V \times W$ in $V$
		l'applicazione lineare $p_V$ tale che $p_V(\vec{v}, \vec{w}) = \vec{v}$.
		Analogamente si può fare con gli altri spazi del prodotto cartesiano.
		
		Si dice che $B$ è un supplementare di $A$ se $V = A \oplus B$ (ossia $\iff
		\dim A + \dim B = \dim V \, \land \, A \cap B = \zerovecset$). Il supplementare
		non è per forza unico. Per trovare un supplementare di $A$ è sufficiente
		completare $\basis_A$ ad una base $\basis$ di $V$ e considerare
		$B := \Span(\basis \setminus \basis_A)$.
		
		\subsubsection{Somma diretta di più sottospazi}
		
		Si dice che i sottospazi $W_1$, ..., $W_k$ di $V$ sono in somma
		diretta, e si scrive $W_1 + \ldots + W_k = W_1 \oplus \ldots \oplus W_k$,
		se la rappresentazione di un vettore della somma di questi sottospazi
		è unica, ossia se esistono unici $\ww 1 \in W_1$, ..., $\ww k \in W_k$ tali
		per cui $\w \in W_1 + \ldots + W_k$ si scrive come $\w = \ww 1 + \ldots + \ww k$. \\
		
		In generale, sono equivalenti i seguenti fatti:
		
		\begin{enumerate}[(i)]
			\itemsep0em
			\item $W_1$, ..., $W_k$ sono in somma diretta,
			\item Se esistono $\ww 1 \in W_1$, ..., $\ww k \in W_k$ tali per cui
			$\ww 1 + \ldots + \ww k = \vec 0$, allora $\ww 1 = \cdots = \ww k = \vec 0$ (è sufficiente considerare due scritture alternative e poi farne la differenza per dimostrare un'implicazione),
			\item Se $\basis_{W_1}$, ..., $\basis_{W_k}$ sono basi di $W_1$, ..., $W_k$,
			allora $\bigcup_{i=1}^k \basis_{W_i}$ è base di $W_1 + \ldots + W_k$ (è sufficiente considerare l'indipendenza lineare per dimostrare un'implicazione),
			\item $\dim (W_1 + \ldots + W_k) = \dim W_1 + \ldots + \dim W_k$ (si dimostra facilmente che è equivalente a (iii), e quindi che lo è alle altre proposizioni),
			\item $W_i \cap (W_1 + \ldots + W_{i-1}) = \zerovecset$ $\forall 2 \leq i \leq k$ (è sufficiente spezzare la somma in $(W_1 + \ldots + W_{i-1}) + W_i$ e ricondursi al caso di due sottospazi, mostrando in particolare, per induzione, l'equivalenza con (iv), da cui seguono le altre equivalenze),
			\item $W_i \cap (W_1 + \ldots + W_{i-1} + \widehat{W_i} + W_{i+1} + W_k) = \zerovecset$ $\forall 1 \leq i \leq k$, ossia $W_i$, intersecato con la somma
			dei restanti sottospazi, è di dimensione nulla (è facile ricondursi alla proposizione (v) per induzione).
		\end{enumerate}
		
		\subsection{Proprietà generali delle matrici}
		
		Si dice che una matrice $A \in M(n, \KK)$ è singolare se $\det(A) = 0$,
		o equivalentemente se non è invertibile. Compatibilmente, si
		dice che una matrice $A \in M(n, \KK)$ è non singolare se $\det(A) \neq
		0$, ossia se $A$ è invertibile.
		
		Si definisce la matrice trasposta di
		$A \in M(m, n, \KK)$, detta $A^\top$, in modo
		tale che $A_{ij} = A^\top_{ji}$.
		
		\begin{itemize}
			\item $(AB)^\top = B^\top A^\top$,
			\item $(A+B)^\top = A^\top + B^\top$,
			\item $(\lambda A)^\top = \lambda A^\top$,
			\item $(A^\top)^\top = A$,
			\item se $A$ è invertibile, $(A^\top)^{-1} = (A^{-1})^\top$,
			\item $ \begin{pmatrix}
				A
				& \rvline & B \\
				\hline
				C & \rvline &
				D
			\end{pmatrix}\begin{pmatrix}
				E
				& \rvline & F \\
				\hline
				G & \rvline &
				H
			\end{pmatrix}=\begin{pmatrix}
				AE+BG
				& \rvline & AF+BH  \\
				\hline
				CE+DG & \rvline &
				CF+DH
			\end{pmatrix}$.
		\end{itemize}
		
		Siano $A \in M(m, n, \KK)$ e $B \in M(n, m, \KK)$.
		
		Si definisce $\GL(n, \KK)$ come il gruppo delle matrici
		di taglia $n$ invertibili sulla moltiplicazione matriciale. Si definisce
		triangolare superiore una matrice i cui elementi al di sotto
		della diagonale sono nulli, mentre si definisce triangolare
		inferiore una matrice i cui elementi nulli sono quelli al di sopra
		della diagonale.
		
		Si definiscono
		\[ Z(M(n, \KK)) = \left\{ A \in M(n, \KK) \mid AB=BA \, \forall B \in M(n, \KK) \right\}, \]
		ossia l'insieme delle matrici che commutano con tutte le altre matrici, e
		\[ Z_{\GL}(M(n, \KK)) = \left\{ A \in M(n, \KK) \mid AB=BA \, \forall B \in \GL(n, \KK) \right\}, \]
		ovvero l'insieme delle matrici che commutano con tutte le matrici
		di $\GL(n, \KK)$.
		
		Si definisce $\tr \in M(m, \KK)^*$ come il funzionale che associa
		ad ogni matrice la somma degli elementi sulla sua diagonale. 
		
		\begin{itemize}
			\item $\tr(A^\top) = \tr(A)$,
			\item $\tr(AB) = \tr(BA)$,
			\item $Z(M(n, \KK)) = \Span(I_n)$,
			\item $Z_{\GL}(M(n, \KK)) = \Span(I_n)$.
		\end{itemize}
		
		Sia $A \in M(n, \KK)$. Sia $C_A \in \End(M(n, \KK))$ definito in modo
		tale che $C_A(B) = AB - BA$. Allora $\Ker C_A = M(n, \KK)
		\iff A \in \Span(I_n)$. Siano $I$ un insieme di $n^2$ indici
		distinti, allora l'insieme
		\[ T = \left\{ A^i \mid i \in I \right\} \]
		
		è linearmente dipendente (è sufficiente notare che
		se così non fosse, se $A \notin \Span(I_n)$,
		tale $T$ sarebbe base di $M(n, \KK)$, ma
		così $\Ker C_A = M(n, \KK) \implies A \in \Span(I_n)$,
		\Lightning{}, e che se $A \in \Span(I_n)$, $T$
		è chiaramente linearmente dipendente).
		
		In generale esiste sempre un polinomio $p(X) \in \KK[x]$
		di grado $n$ tale per cui $p(A) = 0$, dove un tale polinomio
		è per esempio il polinomio caratteristico di $p$, ossia $p(\lambda)=
		\det(\lambda I_n - A)$ (\textit{teorema di 
			Hamilton-Cayley}).
		
		Si elencano adesso i tipi principali di matrici:
		\begin{itemize}
			\item $A$ è simmetrica $\defiff A^\top = A$,
			\item $A$ è antisimmetrica $\defiff A^\top = -A$,
			\item $A$ è hermitiana $\defiff A^* := \left(\overline{A\,}\right)^\top = A$,
			\item $A$ è ortogonale ($A \in O(n)$ o $O_n$) $\defiff AA^\top = A^\top A = I_n$,
			\item $A$ è unitaria ($A \in U(n)$ o $U_n$) $\defiff AA^* = A^*A = I_n$.
		\end{itemize}
		
		Se $A \in M(m, n, \RR)$, allora $\Ker A^\top A = \Ker A$. Infatti, se $\vec x \in \Ker A^\top A$, allora $A^\top A \vec x = \vec 0 \implies \vec x ^\top A^\top A \vec x = \vec 0 \implies q(A \vec x) = \vec 0 \implies A \vec x = \vec 0 \implies \vec x \in \Ker A$, dove $q$ è la forma quadratica derivante dal prodotto scalare standard
		di $\RR^n$. Da questo risultato si deduce anche che $\rg(A^\top A) = \rg(A)$. \\
		\vskip 0.05in
		
		Se $A \in M(m, n, \CC)$, allora $\Ker A^* A = \Ker A$ e $\rg (A^* A) = \rg(A)$ (si segue la stessa linea
		di dimostrazione di sopra).
		
		\subsection{Rango di una matrice}
		
		Si definisce rango di una matrice $A$ il numero di colonne linearmente
		indipendenti di $A$. Siano $A$, $B \in M(m, n, \KK)$.
		
		\begin{itemize}
			\item $\rg(A) = \rg(A^\top)$ (i.e.~il rango è lo stesso se calcolato
			sulle righe invece che sulle colonne),
			\item $\rg(A) \leq \min\{m, n\}$ (come conseguenza dell'affermazione
			precedente),
			\item $\rg(A+B) \leq \rg(A) + \rg(B) \impliedby \Im (A+B) \subseteq
			\Im(A) + \Im(B)$,
			\item $\rg(A+B) = \rg(A) + \rg(B) \implies \Im(A+B) = \Im(A) \oplus \Im(B)$ (è sufficiente applicare la formula di Grassmann),
			\item $\rg(A)$ è il minimo numero di matrici di rango uno che
			sommate restituiscono $A$ (è sufficiente usare la proposizione
			precedente per dimostrare che devono essere almeno $\rg(A)$),
			\item $\rg(A)=1 \implies \exists B \in M(m, 1, \KK)$, $C \in M(1, n, \KK) \mid A=BC$ (infatti $A$ può scriversi come $\begin{pmatrix}\alpha_1 A^i & \cdots & \alpha_n A^i \end{pmatrix}$ per un certo $i \leq n$ tale che $A^i \neq \vec{0}$).
		\end{itemize}
		
		Siano $A \in M(m, n, \KK)$, $B \in M(n, k, \KK)$ e $C \in M(k, t, \KK)$.
		
		\begin{itemize}
			\item $\rg(AB) \geq \rg(A) + \rg(B) - n$ (\textit{disuguaglianza
				di Sylvester} -- è sufficiente
			usare la formula delle dimensioni ristretta alla composizione
			$f_A \circ f_B$),
			\item $\rg(ABC) \geq \rg(AB) + \rg(BC) - \rg(B)$ (\textit{disuguaglianza di Frobenius}, di cui la proposizione
			precedente è un caso particolare con $B = I_n$ e $k=n$),
			\item $\rg(AB) = \rg(B) \impliedby \Ker A = \zerovecset$ (è
			sufficiente usare la formula delle dimensioni ristretta
			alla composizione $f_A \circ f_B$),
			\item $\rg(AB) = \rg(A) \impliedby f_B$ surgettiva (come sopra).
		\end{itemize}
		
		Sia $A \in M(n, \KK)$.
		
		\begin{itemize}
			\item se $A$ è antisimmetrica e il campo su cui si fonda
			lo spazio vettoriale non ha caratteristica $2$, allora
			$\rg(A)$ è pari,
			\item $\rg(A) = n \iff \dim \Ker A = 0 \iff \det(A) \neq 0 \iff A$ è invertibile,
		\end{itemize}
		
		\subsection{Sistemi lineari, algoritmo di eliminazione di Gauss ed 
			SD-equivalenza}
		
		Un sistema lineare di $m$ equazioni in $n$ variabili può essere
		rappresentato nella forma $A\vec{x} = B$, dove $A \in M(m, n, \KK)$,
		$\vec{x} \in \KK^n$ e $B \in \KK^m$. Un sistema lineare si
		dice omogeneo se $B = \vec{0}$. In tal caso l'insieme delle soluzioni del
		sistema coincide con $\Ker A = \Ker f_A$, dove $f_A : \KK^n \to \KK^m$ è 
		l'applicazione lineare indotta dalla matrice $A$. Le soluzioni
		di un sistema lineare sono raccolte nel sottospazio affine
		$\vec{s} + \Ker A$, dove $\vec{s}$ è una qualsiasi soluzione
		del sistema completo.
		
		\begin{itemize}
			\item $A\vec{x} = B$ ammette soluzione se e solo se
			$B \in \Span(A^1, \ldots, A^n) \iff \Span(A^1, \ldots, A^n, B) =
			\Span(A^1, \ldots, A^n) \iff \dim \Span(A^1, \ldots, A^n, B) =
			\dim \Span(A^1, \ldots, A^n) \iff
			\dim \Im (A \mid B) = \dim \Im A \iff \rg (A \mid B) = \rg (A)$
			(\textit{teorema di Rouché-Capelli}),
			\item Data un'applicazione lineare $f : \KK^n \to \KK^m$ determinata
			dalla matrice $M$, il sistema di equazioni cartesiane che rappresenta
			$\Im f$ si ottiene imponendo la validità del teorema di Rouché-Capelli
			sul vettore $\vec x = (x_1, \ldots, x_m)$, ossia imponendo $\rg(M) = \rg(M \mid \vec x)$,
			\item $A\vec{x} = B$, se la ammette, ha un'unica soluzione
			se e solo se $\Ker A = \zerovecset \iff \rg A = n$.
		\end{itemize}
		
		Si definiscono tre operazioni sulle righe di una matrice $A$:
		
		\begin{enumerate}
			\item l'operazione di scambio di riga,
			\item l'operazione di moltiplicazione di una riga
			per uno scalare non nullo,
			\item la somma di un multiplo non nullo di una riga
			ad un'altra riga distinta.
		\end{enumerate}
		
  		A queste operazioni è associato il prodotto a sinistra per delle particolari matrici. 
		In particolare, l'operazione di scambio della riga $i$-esima con quella $j$-esima corrisponde alla moltiplicazione a sinistra per la matrice $S_{i,j}$, detta matrice
		elementare di permutazione, dove:
		\[ S_{i,j}=I_n-E_{i,i}-E_{j,j}+E_{i,j}+E_{j,i}. \]

		In particolare, la stessa matrice $S_{i,j}$ si ottiene scambiando la riga $i$-esima e la $j$-esima. Per esempio, scambiare due righe in una matrice $2 \times 2$ corrisponde a
		moltiplicare a sinistra per $S_{1,2}$, dove:
		
		\[S_{1,2}=\begin{pmatrix}
			0 & 1 \\
			1 & 0 \\
		\end{pmatrix}\]
		
		All'operazione di moltiplicazione della riga $i$-esima per uno scalare $\lambda \neq 0$ corrisponde invece la matrice $M_{i, \lambda}$, detta elementare di dilatazione, dove:
		
		\[ M_{i,\lambda} = \Matrix{I_{i-1} & \rvline & 0 & \rvline & 0 \\ \hline 0 & \rvline & \lambda & \rvline & 0 \\ \hline 0 & \rvline & 0 & \rvline & I_{n-i} }. \]
		
		All'operazione di somma della riga $j$-esima moltiplicata per $\lambda \neq 0$ alla riga $i$-esima corrisponde invece la matrice $M_{i,j,\lambda}$, detta elementare di trasvezione (o di tosatura, dall'inglese \textit{shear matrix}), dove:
		\[M_{i,j,\lambda}=I_n+\lambda E_{i,j}.\]
		Se $\lambda \neq 0$, tutte queste matrici sono invertibili ed in particolare valgono le seguente relazioni:
		\begin{itemize}
			\item $S_{i,j}^{-1}=S_{i,j}$, da cui si osserva che l'inversa di una matrice elementare di permutazione è ancora una matrice dello stesso tipo),
			\item $M_{i,\lambda}^{-1}=M_{i,\frac{1}{\lambda}}$, come sopra,
			\item $M_{i,j,\lambda}^{-1}=M_{i,j,-\lambda}$, come sopra,
			\item $M_{i,\lambda} = M_{i, i, \lambda-1}$,
			\item le matrici elementari generano il gruppo delle matrici invertibili $\GL(n, \KK)$, ossia ogni matrice
			invertibile si scrive come prodotto di matrici elementari (è sufficiente
			applicare l'algoritmo di eliminazione di Gauss per righe su $A \in \GL(n, \KK)$ e
			osservare che ne deve risultare obbligatoriamente una matrice diagonale che,
			normalizzata sugli elementi, restituisce esattamente $I_n$; allora poiché applicare l'algoritmo
			equivale a moltiplicare a sinistra per delle matrici elementari $\mathcal{E}_1$, ..., $\mathcal{E}_k$, si verifica che $\mathcal{E}_1 \cdots \mathcal{E}_k A = I_n \implies A = \mathcal{E}_k \inv \cdots \mathcal{E}_1 \inv$, dove si
			conclude la dimostrazione osservando che l'inversa di una matrice elementare
			è ancora una matrice elementare).
		\end{itemize}
		
		Queste operazioni non variano né $\Ker A$ né $\rg (A)$. Permettendo di variare $\Ker A$ si possono effettuare le stesse medesime operazioni
		sulle colonne (lasciando però
		invariato $\Im A$, e quindi $\rg (A)$): tali operazioni corrispondono a moltiplicare a destra per una matrice invertibile, analogamente a come accade per le righe. \\
		
		Le matrici per cui si moltiplica a destra per operare sulle colonne sono esattamente le stesse matrici impiegate
		per le operazioni di riga, sebbene trasposte (e quindi sono ancora matrici elementari). In particolare le matrici elementari di permutazione (per scambiare le righe) e di dilatazione (per moltiplicare una riga per uno scalare non nullo) coincidono. Pertanto, se $A$ è una matrice simmetrica (i.e.~se $A \in \Sym(n, \KK)$), operare mediante le stesse
		operazioni sulle righe e sulle colonne permette di individuare matrici congruenti
		ad $A$.
		
		L'algoritmo di eliminazione di Gauss
		procede nel seguente modo:
		
		\begin{enumerate}
			\item se $A$ ha una riga, l'algoritmo termina;
			\item altrimenti si prenda la prima riga di $A$ con il primo elemento
			non nullo e la si scambi con la prima riga di $A$ (in caso
			non esista, si proceda all'ultimo passo),
			\item per ogni riga di $A$ con primo elemento non nullo,
			esclusa la prima, si sottragga un multiplo della prima riga in modo
			tale che la riga risultante abbia il primo elemento nullo,
			\item si ripeta l'algoritmo considerando come matrice $A$ la
			matrice risultante dall'algoritmo senza la prima riga e la
			prima colonna (in caso tale matrice non possa esistere,
			l'algoritmo termina).
		\end{enumerate}
		
		Si definiscono \textit{pivot} di una matrice l'insieme dei primi
		elementi non nulli di ogni riga della matrice.
		Il rango della matrice iniziale $A$ è pari al numero di \textit{pivot}
		della matrice risultante dall'algoritmo di eliminazione di Gauss.
		Una matrice che processata dall'algoritmo di eliminazione di Gauss
		restituisce sé stessa è detta matrice a scala.
		
		Agendo solo attraverso
		operazioni per riga, l'algoritmo di eliminazione di Gauss non
		modifica $\Ker A$ (si può tuttavia integrare l'algoritmo con le
		operazioni per colonna, perdendo quest'ultimo beneficio).
		
		Agendo
		su una matrice a scala con operazioni per riga considerando
		la matrice riflessa (ossia dove l'elemento $(1, 1)$ e $(m, n)$ sono
		scambiati), si può ottenere una matrice a scala ridotta,
		ossia un matrice dove tutti i pivot sono $1$ e dove tutti
		gli elementi sulle colonne dei pivot, eccetto i pivot stessi,
		sono nulli. % TODO: controllare e sistemare
		
		Si definisce:
		
		\[I^{m \times n}_r =
		\begin{pmatrix}
			I_r
			& \rvline & \bigzero \\
			\hline
			\bigzero & \rvline &
			\bigzero
		\end{pmatrix} \in M(m, n, \KK). \]
		
		Per ogni applicazione lineare $f : V \to W$, con $\dim V = n$ e
		$\dim W = m$ esistono due basi $\basis_V$, $\basis_W$ rispettivamente
		di $V$ e $W$ tale che $M^{\basis_V}_{\basis_W}(f) = I^{m \times n}_r$,
		dove $r=\rg(f)$ (è sufficiente completare con $I$ a base di $V$ una base
		di $\Ker f$ e poi prendere come base di $W$ il completamento di $f(I)$
		su una base di $W$).
		
		Si definisce SD-equivalenza la relazione d'equivalenza su
		$M(m, n, \KK)$ indotta dalla relazione $A \sim_{SD} B \iff \exists P \in
		\GL(m, \KK)$, $Q \in \GL(n, \KK) \mid A=PBQ$. L'invariante completo
		della SD-equivalenza è il rango: $\rg(A) = \rg(B) \iff A \sim_{SD} B$
		(infatti $\rg(A) = r \iff A \sim_{SD} I^{m \times n}_r$ -- è sufficiente
		applicare il cambio di base e sfruttare il fatto che esistono
		sicuramente due basi per cui $f_A$ ha $I^{m \times n}_r$ come
		matrice associata).
		
		Poiché $I^{m \times n}_r$ ha sempre rango $r$, l'insieme
		quoziente della SD-equivalenza su $M(m, n, \KK)$ è il seguente:
		
		\[ M(m, n, \KK)/{\sim_{SD}} = \left\{[\vec{0}], \left[I^{m \times n}_1\right], \ldots, \left[I^{m \times n}_{\min\{m, n\}}\right] \right\}, \]
		
		contenente esattamente $\min\{m, n\}$ elementi. L'unico elemento
		di $[\vec{0}]$ è $\vec{0}$ stesso.
		
		\subsubsection{La regola di Cramer}
		
		Qualora $m=n$ e $A$ fosse invertibile (i.e. $\det(A) \neq 0$),
		per calcolare il valore di $\vec{x}$ si può applicare
		la regola di Cramer.
		
		Si definisce:
		\[ A_i^* = \begin{pmatrix}
			A^1 & \cdots & A^i \to B & \cdots & A^n
		\end{pmatrix}, \]
		
		dove si sostituisce alla $i$-esima colonna di $A$ il vettore $B$. Allora
		vale la seguente relazione:
		
		\[ \vec{x} = \frac{1}{\det(A)} \begin{pmatrix}
			\det(A_1^*) \\ \vdots \\ \det(A_n^*)
		\end{pmatrix}. \]
		
		\subsection{L'inverso (generalizzato e non) di una matrice}
		
		Si definisce matrice dei cofattori di una matrice $A \in M(n, \KK)$ la
		seguente matrice:
		
		\[ \Cof A = \begin{pmatrix}
			\Cof_{1,1}(A) & \ldots & \Cof_{1,n}(A) \\
			\vdots & \ddots & \vdots \\ 
			\Cof_{n,1}(A) & \ldots & \Cof_{n,n}(A),
		\end{pmatrix}, \]
		
		dove, detta $A_{i,j}$ il minore di $A$ ottenuto eliminando
		la $i$-esima riga e la $j$-esima colonna, si definisce il cofattore (o
		complemento algebrico) nel seguente modo:
		
		\[ \Cof_{i,j}(A) = (-1)^{i+j} \det( A_{i, j}). \]
		
		Si definisce inoltre l'aggiunta classica:
		
		\[ \adj(A) = (\Cof A)^\top. \]
		
		Allora, se $A$ ammette un inverso (i.e. se $\det(A) \neq 0$),
		vale la seguente relazione:
		
		\[ A^{-1} = \frac{1}{\det(A)} \adj(A). \]
		
		\vskip 0.05in
		
		Quindi, per esempio, $A^{-1}$ è a coefficienti
		interi $\iff \det(A) = \pm 1$.
		
		Siano $A$, $B \in M(n, \KK)$.
		
		\begin{itemize}
			\item $\adj(AB) = \adj(B)\adj(A)$,
			\item $\adj(A^\top) = \adj(A)^\top$.
		\end{itemize}
		
		Si definisce inverso generalizzato di una matrice $A \in M(m, n, \KK)$
		una matrice $X \in M(n, m, \KK) \mid AXA=A$. Ogni matrice ammette
		un inverso generalizzato (è sufficiente considerare gli inversi
		generalizzati di $I^{m \times n}_r$ e la SD-equivalenza di $A$
		con $I^{m \times n}_r$, dove $\rg(A)=r$). Se $m=n$ ed $A$ è invertibile, allora
		$A^{-1}$ è l'unico inverso generalizzato di $A$. Gli inversi
		generalizzati di $I^{m \times n}_r$ sono della forma:
		
		\[X =
		\begin{pmatrix}
			I_r
			& \rvline & B \\
			\hline
			C & \rvline &
			D
		\end{pmatrix} \in M(m, n, \KK). \]
		
		\subsection{Endomorfismi e similitudine}
		
		Si definisce la similitudine tra matrici su $M(n, \KK)$ come la relazione
		di equivalenza determinata da $A \sim B \iff \exists P \in \GL(n, \KK)
		\mid A = PBP^{-1}$. $A \sim B \implies \rg(A)=\rg(B)$, $\tr(A)=\tr(B)$,
		$\det(A)=\det(B)$, $P_\lambda(A) = P_\lambda(B)$ (invarianti \textit{non completi} della similitudine).
		Vale inoltre che $A \sim B \iff A$ e $B$ hanno la stessa forma
		canonica di Jordan, a meno di permutazioni dei blocchi di Jordan
		(invariante \textit{completo} della similitudine). La matrice
		identità è l'unica matrice identica a sé stessa.
		
		Sia $p \in \End(V)$. Si dice che un endomorfismo è un automorfismo
		se è un isomorfismo. Gli automorfismi formano un sottospazio vettoriale
		di $\End(V)$ denotato con $\Aut(V)$ o $\GL(V)$. Siano $\basis$, $\basis'$ due qualsiasi
		basi di $V$.
		
		\begin{itemize}
			\item $p$ automorfismo $\iff p$ iniettivo $\iff p$ surgettivo (è
			sufficiente applicare la formula delle dimensioni),
			\item $M^\basis_{\basis'}(id_V) M^{\basis'}_\basis(id_V)
			= I_n$ (dunque entrambe le matrici sono invertibili e sono
			l'una l'inverso dell'altra),
			\item se $p$ è un automorfismo, $M^\basis_{\basis'}(p^{-1}) =
			M^{\basis'}_\basis(p)^{-1}$,
			\item $M^\basis_{\basis}(p) = \underbrace{M^{\basis'}_\basis (id_V)}_{P} \,
			M^{\basis'}_{\basis'}(p) \,
			\underbrace{M^{\basis}_{\basis'} (id_V)}_{P^{-1}}$ (ossia
			$M^\basis_{\basis}(p) \sim M^{\basis'}_{\basis'}(p)$).
		\end{itemize}
		
		$M^\basis_{\basis'}(id_V) M^{\basis'}_\basis(id_V)
		= I_n$. Dunque entrambe le matrici sono invertibili. Inoltre
		$M^\basis_\basis(id_V) = I_n$.
		
		Si definisce un analogo della similitudine anche per gli endomorfismi:
		due endomorfismi $f$, $g \in \End(V)$ si dicono coniugati se e solo se $\exists
		h \in \GL(V) \mid f = h \, g \, h\inv$. Il coniugio induce in particolare
		un'altra relazione di equivalenza. Due endomorfismi $f$ e $g$ sono coniugati
		se e solo se le loro matrici associate nella stessa base $\basis$ sono simili.
		
		\subsubsection{Duale, biduale e annullatore}
		
		Si definisce duale di uno spazio vettoriale $V$ lo
		spazio $\dual{V} = \mathcal{L}(V, \KK)$, i cui elementi
		sono detti funzionali. Analogamente
		il biduale è il duale del duale di $V$: $\bidual{V} = \dual{(\dual{V})} = \mathcal{L}(\dual{V}, \KK)$.
		
		Sia data una base $\basis = \{\vec{v_1}, \ldots, \vec{v_n}\}$ di
		uno spazio vettoriale $V$ di dimensione $n$. Allora $\dim \dual{V}
		= \dim \mathcal{L}(V, \KK) = \dim V \cdot \dim \KK = \dim V$. Si definisce
		il funzionale $\dual{\vec{v_i}}$ come l'applicazione lineare
		univocamente determinata dalla relazione:
		
		\[ \dual{\vec{v_i}}(\vec{v_j}) = \delta_{ij}. \]
		
		\vskip 0.05in
		
		Sia $\basis^* = \{\vec{v_1}^*, \ldots, \vec{v_n}^*\}$. Allora
		$\basis^*$ è una base di $\dual{V}$. Poiché $V$ e $\dual{V}$
		hanno la stesso dimensione, tali spazi sono isomorfi, sebbene
		non canonicamente. Ciononostante, $V$ e $\bidual{V}$ sono
		canonicamente isomorfi tramite l'isomorfismo:
		
		\[ \bidual{\varphi} : V \to \bidual{V}, \; \vec{v} \mapsto \restr{\val}{\dual{V}}, \]
		
		che associa ad ogni vettore $\vec{v}$ la funzione
		di valutazione in una funzionale in $\vec{v}$, ossia:
		
		\[ \restr{\val}{\dual{V}} : \dual{V} \to \KK, \; f \mapsto f(\vec{v}). \]
		
		Sia $U \subseteq V$ un sottospazio di $V$.
		Si definisce il sottospazio di $\mathcal{L}(V, W)$:
		
		\[ \Ann_{\mathcal{L}(V, W)}(U) = \left\{ f \in \mathcal{L}(V, W) \mid f(U) = \zerovecset \right\}. \]
		
		Se $V$ è a dimensione finita, la dimensione di
		$\Ann_{\mathcal{L}(V, W)}(U)$ è pari a $(\dim V - \dim U) \cdot \dim W$ (è sufficiente
		prendere una base di $U$, completarla a base di $V$ e
		notare che $f(U) = \zerovecset \iff$ ogni valutazione
		in $f$ degli elementi della base di $U$ è nullo $\iff$ la matrice
		associata di $f$ ha tutte colonne nulle in corrispondenza degli
		elementi della base di $U$).
		
		Si scrive semplicemente $\Ann(U)$ quando $W=\KK$ (ossia
		quando le funzioni sono funzionali di $V$). In tal
		caso $\dim \Ann(U) = \dim V - \dim U$.
		
		\begin{itemize}
			\item $\bidual{\varphi}(U) \subseteq \Ann(\Ann(U))$,
			\item se $V$ è a dimensione finita, $\bidual{\varphi}(U) = \bidual{U} = \Ann(\Ann(U))$ (è sufficiente
			applicare la formula delle dimensioni $\restr{\bidual{\varphi}}{U}$ e notare l'uguaglianza
			tra le due dimensioni),
			\item se $V$ è a dimensione finita e $W$ è un altro
			sottospazio di $V$,
			$U = W \iff \Ann(U) = \Ann(W)$ (è sufficiente
			considerare $\Ann(\Ann(U)) = \Ann(\Ann(W))$ e
			applicare la proposizione precedente, ricordandosi
			che $\bidual{\varphi}$ è un isomorfismo, ed è
			dunque iniettivo).
		\end{itemize}
		
		Si definisce l'applicazione trasposta $^\top$ da $\mathcal{L}(V, W)$ a
		$\mathcal{L}(\dual{W}, \dual{V})$ in modo tale che $f^\top(g)
		= g \circ f \in \dual{V}$. Siano $f$, $g \in \mathcal{L}(V,W)$ e
		sia $h \in \mathcal{L}(W,Z)$.
		
		\begin{itemize}
			\item $(f+g)^\top = f^\top + g^\top$,
			\item $(\lambda f)^\top = \lambda f^\top$,
			\item se $f$ è invertibile, $(f^{-1})^\top = (f^\top)^{-1}$,
			\item $(h \circ f)^\top = f^\top \circ h^\top$.
		\end{itemize}
		
		Siano $\basis_V$, $\basis_W$ due basi rispettivamente di $V$ e
		di $W$. Allora vale la seguente relazione:
		
		\[ M^{\basis_W^*}_{\basis_V^*}(f^\top) = M^{\basis_V}_{\basis_W}(f)^\top. \]
		
		\subsection{Applicazioni multilineari}
		
		Sia $f : V_1 \times \ldots \times V_n \to W$ un'applicazione, dove
		$V_i$ è uno spazio vettoriale $\forall i \leq n$, così come $W$. Tale
		applicazione si dice $n$-lineare ed appartiene allo spazio
		$\Mult(V_1 \times \ldots \times V_n, W)$, se è lineare in ogni sua coordinata, ossia se:
		
		\begin{itemize}
			\item $f(x_1, \ldots, x_i + y_i, \ldots, x_n) =
			f(x_1, \ldots, x_i, \ldots, x_n) + f(x_1, \ldots, y_i, \ldots, x_n)$,
			\item $f(x_1, \ldots, \alpha x_i, \ldots, x_n) = \alpha f(x_1, \ldots, x_i, \ldots, x_n)$.
		\end{itemize}
		
		Sia $W=\KK$, e siano tutti gli spazi $V_i$ fondati su tale campo: allora
		$\Mult(V_1 \times \ldots \times V_n, \KK)$ si scrive anche come $V_1^* \otimes \ldots \otimes V_n^*$, e tale spazio
		è detto prodotto tensoriale tra $V_1$, ..., $V_n$.
		Sia $V_i$ di dimensione finita $\forall i \leq n$. Siano $\basis_{V_i} = \left\{ \vec{v^{(i)}_1}, \ldots,  \vec{v^{(i)}_{k_i}} \right\}$ base
		di $V_i$, dove $k_i = \dim V_i$.
		Si definisce l'applicazione $n$-lineare $\dual{\vec{v^{(1)}_{j_1}}}
		\otimes \cdots \otimes \dual{\vec{v^{(n)}_{j_n}}}\in \Mult(V_1 \times \ldots
		\times V_n, \KK)$ univocamente determinata dalla seguente relazione:
		\[ \dual{\vec{v^{(1)}_{j_1}}} \otimes \cdots \otimes \dual{\vec{v^{(n)}_{j_n}}}(\vec{w_1}, \ldots, \vec{w_n}) = \dual{\vec{v^{(1)}_{j_1}}}(\vec{w_1}) \cdots \dual{\vec{v^{(n)}_{j_n}}}(\vec{w_n}).   \]
		
		Si definisce l'insieme $\basis_{\otimes}$ nel seguente modo:
		\[ \basis_{\otimes} = \left\{ \dual{\vec{v^{(1)}_{j_1}}} \otimes \cdots \otimes \dual{\vec{v^{(n)}_{j_n}}} \mid 1 \leq j_1 \leq k_1, \, \ldots, \, 1 \leq j_n \leq k_n \right\}. \]
		
		Poiché ogni applicazione $n$-lineare è univocamente determinata
		dai valori che assume ogni combinazione degli elementi delle basi
		degli spazi $V_i$, vi è un isomorfismo tra $\Mult(V_1 \times \ldots
		\times V_n, \KK)$ e $\KK^{\basis_{V_1} \times \cdots \times \basis_{V_n}}$, che ha dimensione $\prod_{i=1}^n k_i = k$. Pertanto
		anche $\dim \Mult(V_1 \times \ldots \times V_n, \KK) = k$.
		
		Poiché $\basis_{\otimes}$ genera $\Mult(V_1 \times \ldots
		\times V_n, \KK)$ e i suoi elementi sono tanti quanto è la
		dimensione dello spazio, tale insieme è una base di $\Mult(V_1 \times 
		\ldots \times V_n, \KK)$.
		
		Se $V_i = V_1 = V$ $\forall i \leq n$, si dice che $\Mult(V^n, \KK)$
		è lo spazio delle forme $n$-lineari di $V$. 
		
		\subsubsection{Applicazioni multilineari simmetriche}
		
		Sia $V$ uno spazio di dimensione $n$. Una 
		forma $k$-lineare $f$ si dice simmetrica 
		ed appartiene allo spazio $\Sym^k(V)$ se:
		
		\[ f(\vec{x_1}, \ldots, \vec{x_k}) = f(\vec{x_{\sigma(1)}}, \ldots, \vec{x_{\sigma(k)}}), \quad \forall \sigma \in S_k. \]
		
		Poiché ogni applicazione $n$-lineare simmetrica è univocamente
		determinata dai valori che assume negli elementi della base
		disposti in modo non decrescente, $\dim \Sym^k(V) = \binom{n+k-1}{k}$.
		
		Sia $\basis = \{\vec{v_1}, \ldots, \vec{v_n}\}$ una base
		di $V$. Dato un insieme di indici non decrescente $I$,
		si definisce il prodotto simmetrico (o \textit{prodotto vee}) 
		$\dual{\vec{v_{i_1}}} \vee \cdots \vee \dual{\vec{v_{i_k}}}$
		tra elementi della base come la forma $k$-lineare simmetrica
		determinata dalla seguente relazione:
		\[ \dual{\vec{v_{i_1}}} \vee \cdots \vee \dual{\vec{v_{i_k}}} = \sum_{\sigma \in S_k} \dual{\vec{v_{i_{\sigma(1)}}}} \otimes \cdots \otimes \dual{\vec{v_{i_{\sigma(k)}}}}. \]
		
		Si definisce l'insieme:
		\[\basis_{\Sym} = \left\{  \dual{\vec{v_{i_1}}} \vee \cdots \vee \dual{\vec{v_{i_k}}} \mid 1 \leq i_1 \leq \cdots \leq i_k \leq n \right\}. \]
		
		L'insieme $\basis_{\Sym}$ è sia generatore che linearmente
		indipendente su $\Sym^k(V)$, ed è dunque base. Allora
		$\dim \Sym^k(V) = \binom{n+k-1}{k}$.
		
		\subsubsection{Applicazioni multilineari alternanti}
		
		Sia $V$ uno spazio di dimensione $n$. Una forma
		$k$-lineare $f$ si dice alternante (o antisimmetrica)
		ed appartiene allo spazio $\Lambda^k(V)$ (talvolta scritto
		come $\operatorname{Alt}^k(V)$) se:
		
		\[ f(x_1, \ldots, x_k) = 0 \impliedby \exists \, i, j \leq k \mid x_i = x_j. \]
		
		\vskip 0.05in
		
		Questo implica che:
		
		\[ f(x_1, \ldots, x_k) = \sgn(\sigma) f(x_{\sigma(1)}, \ldots, x_{\sigma(n)}), \quad \forall \sigma \in S_k \]
		
		Se $k > n$, un argomento della base di $V$ si ripete sempre nel
		computo $f$ negli elementi della base, e quindi ogni alternante è
		pari a $\vec{0}$, ossia $\dim \Lambda^k(V) = 0$.
		
		Sia $\basis = \{\vec{v_1}, \ldots, \vec{v_n}\}$ una base
		di $V$. Dato un insieme di indici crescente $I$,
		si definisce il prodotto esterno (o \textit{prodotto wedge}) 
		$\dual{\vec{v_{i_1}}} \wedge \cdots \wedge \dual{\vec{v_{i_k}}}$
		tra elementi della base come la forma $k$-lineare alternante
		determinata dalla relazione:
		\[ \dual{\vec{v_{i_1}}} \wedge \cdots \wedge \dual{\vec{v_{i_k}}} = \sum_{\sigma \in S_k} \sgn(\sigma) \, \dual{\vec{v_{i_{\sigma(1)}}}} \otimes \cdots \otimes \dual{\vec{v_{i_{\sigma(k)}}}}. \]
		
		Si definisce l'insieme:
		
		\[\basis_{\Lambda} = \left\{  \dual{\vec{v_{i_1}}} \wedge \cdots \wedge \dual{\vec{v_{i_k}}} \mid 1 \leq i_1 < \cdots < i_k \leq n \right\}. \]
		
		L'insieme $\basis_{\Lambda}$ è sia generatore che linearmente
		indipendente su $\Lambda^k(V)$, ed è dunque base. Allora
		$\dim \Lambda^k(V) = \binom{n}{k}$. Riassumendo si può scrivere:
		
		\[\dim \Lambda^k(V) = \begin{cases} 0 & \text{se } k > n\,, \\ \binom{n}{k} & \text{altrimenti}. \end{cases}\]
		
		Quindi è quasi sempre vero che:
		
		\[ \underbrace{\dim \Sym^k(V)}_{= \, \binom{n+k-1}{k}} + \underbrace{\dim \Lambda^k(V)}_{\leq \, \binom{n}{k}} < \underbrace{\dim \Mult(V^k, \KK)}_{=\,n^k}, \]
		
		e dunque che $\Sym^k(V) + \Lambda^k(V) \neq \Mult(V^k, \KK)$.

		\subsection{Determinante di una matrice}
		
		Si definisce il determinante $\det$ di una matrice di taglia
		$n \times n$ come l'unica forma $n$-lineare alternante di $(\KK^n)^n$
		tale che $\det(\vec{e_1}, \ldots, \vec{e_n}) = 1$ (infatti
		$\dim \Lambda^n (V) = \binom{n}{n} = 1$, e quindi ogni forma
		alternante è multipla delle altre, eccetto per lo zero).
		
		Equivalentemente $\det = \dual{\vec{e_1}} \, \wedge \cdots \wedge \, \dual{\vec{e_n}}$.
		
		Siano $A$, $B \in M(n, \KK)$. Si scrive
		$\det(A)$ per indicare $\det(A_1, \ldots, A_n)$. Vale pertanto la
		seguente relazione:
		
		\[ \det(A) = \sum_{\sigma \in S_n} \sgn(\sigma) \, a_{1\sigma(1)} \cdots a_{n\sigma(n)}. \]
		
		\begin{itemize}
			\item $\det(I_n) = 1$,
			\item $\det \begin{pmatrix}
				a & b \\ c & d
			\end{pmatrix} = ad-bc$,
			\item $\det \begin{pmatrix}
				a & b & c \\ d & e & f \\ g & h & i
			\end{pmatrix} = a(ei-fh) - b(di-fg) + c(dh-eg)$,
			\item $\det(A) \neq 0 \iff A$ invertibile (ossia non singolare),
			\item $\det(\lambda A) = \lambda^n A$,
			\item $\det(A) = \det(A^\top)$ (è sufficiente applicare la definizione
			di $\det$ e manipolare algebricamente il risultato per evidenziare
			l'uguaglianza),
			\item se $A$ è antisimmetrica, $n$ è dispari e $\Char \KK \neq 2$,
			$\det(A) = \det(-A^\top) = (-1)^n \det(A^\top) = (-1)^n \det(A) = -\det(A) \implies \det(A) = 0$ (quindi ogni matrice antisimmetrica di taglia
			dispari non è invertibile),
			\item $\det(AB) = \det(A)\det(B)$ (\textit{teorema di Binet} -- è
			sufficiente considerare la forma $\frac{\det(AB)}{\det(B)}$ in
			funzione delle righe di $A$ e determinare che tale forma
			è alternante e che vale $1$ nell'identità, e che, per l'unicità
			del determinante, deve obbligatoriamente essere pari a
			$\det(A)$),
			\item se $A$ è invertibile, $\det(A^{-1}) = \det(A)^{-1}$,
			\item $\det   \begin{pmatrix}
				\lambda_{1} & & \\
				& \ddots & \\
				& & \lambda_{n}
			\end{pmatrix} = \det(\lambda_1 \vec{e_1}, \ldots, \lambda_n \vec{e_n}) = \prod_{i=1}^n \lambda_i$,
			\item se $A$ è triangolare superiore (o inferiore), allora $\det(A)$ è
			il prodotto degli elementi sulla sua diagonale principale,
			\item $\det(A_1, \ldots, A_n) = \sgn(\sigma) \det(A_{\sigma(1)}, \ldots, A_{\sigma(n)})$, $\forall \sigma \in S_n$ (infatti $\det$ è alternante),
			\item \setlength{\extrarowheight}{1.3pt}$\det \begin{pmatrix}
				A
				& \rvline & B \\
				\hline
				C & \rvline &
				D
			\end{pmatrix} = \det(AD-BC)$, se $C$ e $D$ commutano e $D$ è invertibile,
			\item $\det \begin{pmatrix}
				A
				& \rvline & B \\
				\hline
				0 & \rvline &
				C
			\end{pmatrix} = \det(A)\det(C)$\setlength{\extrarowheight}{0pt},
			\item se $A$ è nilpotente (ossia se $\exists k \mid A^k = 0$),
			$\det(A) = 0$,
			\item se $A$ è idempotente (ossia se $A^2 = A$), allora
			$\det(A) = 1$ o $\det(A) = 0$,
			\item se $A$ è ortogonale (ossia se $AA^\top = I_n$), allora
			$\det(A) = \pm 1$,
			\item se $A \in M(n, \CC)$, $\det(\conj{A}) = \conj{\det(A)}$ (segue direttamente dallo sviluppo di Laplace del determinante), 
			\item se $A$ è unitaria (ossia se $AA^* = I_n$), allora $\abs{\det(A)} = 1$,
			\item se $A$ è un'involuzione (ossia se $A^2 = I_n$), allora
			$\det(A) = \pm 1$,
			\item se ogni minore di taglia $k$ di $A$ ha determinante nullo,
			allora tutti i minori di $A$ taglia maggiore o uguale a $k$ hanno
			determinante nullo (è una diretta applicazione dello sviluppo di Laplace).
		\end{itemize}
		
		Le operazioni del terzo tipo dell'algoritmo di eliminazione
		di Gauss (ossia l'aggiunta a una riga di un multiplo di un'altra
		riga -- a patto che le due righe siano distinte) non alterano il
		determinante della matrice iniziale, mentre lo scambio di righe
		ne inverte il segno (corrisponde a una trasposizione di $S_n$).
		L'operazione del secondo tipo (la moltiplicazione di una riga
		per uno scalare) altera il determinante moltiplicandolo per
		tale scalare.
		
		Inoltre, se $D$ è invertibile, vale la decomposizione di Schur:
		\setlength{\extrarowheight}{1.3pt}
		\begin{gather*}
			\begin{pmatrix}
				A
				& \rvline & B \\
				\hline
				C & \rvline &
				D
			\end{pmatrix} = \begin{pmatrix}
				I_k
				& \rvline & BD^{-1} \\
				\hline
				0 & \rvline &
				I_k
			\end{pmatrix}
			\begin{pmatrix}
				A-BD^{-1}C
				& \rvline & 0 \\
				\hline
				0 & \rvline &
				D
			\end{pmatrix} \\
			\begin{pmatrix}
				I_k
				& \rvline & 0 \\
				\hline
				D^{-1}C & \rvline &
				I_k
			\end{pmatrix},
		\end{gather*}
		\setlength{\extrarowheight}{0pt}
		
		dove $k \times k$ è la taglia di $A$. Pertanto vale
		la seguente relazione, sempre se $D$ è invertibile:
		
		\[ \det \begin{pmatrix}
			A
			& \rvline & B \\
			\hline
			C & \rvline &
			D
		\end{pmatrix} = \det(A-BD^{-1}C)\det(D). \]
		
		È possibile computare il determinante di $A$, scelta la riga $i$, mediante lo
		sviluppo di Laplace:
		\[ \det(A) = \sum_{j=1}^n a_{ij} \Cof_{i,j}(A) = \sum_{j=1}^n (-1)^{i+j} a_{ij} \det(A_{i,j}). \]
		
		Si definisce matrice di Vandermonde una matrice $A \in M(n, \KK)$ della
		forma:
		
		\[ A = \begin{pmatrix}
			1 & x_1 & x_1^2 & \dots & x_1^{n-1}\\
			1 & x_2 & x_2^2 & \dots & x_2^{n-1}\\
			\vdots & \vdots & \vdots & \ddots &\vdots \\
			1 & x_n & x_n^2 & \dots & x_n^{n-1}.
		\end{pmatrix} \]
		
		Vale allora che:
		
		\[ \det(A) = \prod_{1 \leq i < j \leq n} (x_j - x_i), \]
		
		verificabile notando che $\det(A)$ è di grado $\frac{n(n-1)}{2}$ e
		che ponendo $x_i = x_j$ per una coppia $(i, j)$, tale matrice
		ha due righe uguali, e quindi determinante nullo $\implies (x_j - x_i) \mid \det(A) \overbrace{\implies}^{\text{UFD}} \det(A) = \prod_{1 \leq i < j \leq n} (x_j - x_i) $.
		
		Pertanto una matrice di Vandermonde è invertibile se e solo se la sua
		seconda colonna contiene tutti scalari distinti nelle coordinate. Tale
		matrice risulta utile nello studio dell'interpolazione di Lagrange
		(ossia nella dimostrazione dell'unicità del polinomio di $n-1$ grado
		tale che $p(\alpha_i) = \beta_i$ per $i$ coppie ($\alpha_i$, $\beta_i$) con
		$\alpha_i$ tutti distinti).
		
		\subsubsection{Rango tramite il determinante degli orlati}
		
		Si dicono \textit{sottomatrici} della matrice $A \in M(m, n, \KK)$ tutte
		le matrici contenute in $A$, ossia le matrici $B$ che sono ottenibili da $A$
		mantenendo solo alcune sue righe e colonne. In generale, si scrive $A^{j_1, \ldots, j_s}_{i_1, \ldots, i_t}$ per indicare la sottomatrice ottenuta
		da $A$ mantenendo le colonne di indice $j_1$, ..., $j_s$ e le righe di
		indice $i_1$, ..., $i_t$. Quando è omesso l'indice delle colonne o l'indice
		delle righe, si sottintende di aver mantenuto o tutte le colonne o tutte le righe
		(e.g.~$A_{1,2}$ è la sottomatrice di $A$ ottenuta mantenendo tutte le colonne e
		le prime due righe). Si dice che $M$ è \textit{minore} di $A$ una sua sottomatrice quadrata. Si chiamano \textit{orlati} di un minore $M$ di taglia $k$ i minori di taglia $k+1$ di $A$ aventi $M$ come minore.
		
		\begin{itemize}
			\item se $B$ è una sottomatrice di $A$, allora $\rg(B) \leq \rg(A)$ (è sufficiente prendere un numero massimo di colonne linearmente indipendenti
			di $B$ e mostrare che le relative colonne in $A$ sono ancora linearmente indipendenti),
			\item $\rg(A) = \max\{\rg(B) \mid B \text{ sottomatrice di }\! A\}$ (è sufficiente utilizzare il precedente risultato; infatti $A$ è una sottomatrice di $A$),
			\item $\rg(A) = \max\{\rg(B) \mid B \text{ minore invertibile di }\! A\} = \max\{n \mid \text{esiste un minore di $A$ di taglia $n$ invertibile} \}$ (è sufficiente utilizzare la prima disuguaglianza e considerare un minore di $A$ composto dalle righe e le colonne linearmente indipendenti di $A$, che sono
			dello stesso numero, dal momento che il rango per righe è uguale al rango per colonne),
			\item $\rg(A)$ è il più piccolo naturale $n$ tale per cui, per ogni minore
			$M$ di $A$ di taglia maggiore di $n$, $\det(M) = 0$ (ossia $M$ è singolare; segue direttamente dal precedente risultato),
			\item $\rg(A)$ è il più piccolo naturale $n$ tale per cui, per ogni minore
			$M$ di $A$ di taglia $n+1$, $\det(M) = 0$ (ossia $M$ è singolare; segue dal precedente risultato a cui si combina lo sviluppo di Laplace del determinante -- se ogni minore di taglia $k$ ha determinante nullo, anche tutti i minori di
			taglia maggiore di $k$ hanno determinante nullo).
			\item esiste un minore $M$ di taglia $k$ di $A$ con $\det(M) \neq 0$ $\implies \rg(A) \geq k$ (deriva direttamente dall'ultimo risultato sul rango),
			\item per ogni minore $M$ di taglia $k$ di $A$ vale che $\det(M) = 0$
			$\implies \rg(A) < k$ (come sopra).
		\end{itemize}
		
		Si può facilitare lo studio del rango tramite il teorema di Kronecker (o degli orlati): $\rg(A)$ è il più piccolo naturale $n$ tale per cui esista un minore
		$M$ di taglia $k$ con $\det(M) \neq 0$ e per cui ogni suo orlato $O$ è tale
		per cui $\det(O) = 0$.
		
		Sia infatti, senza perdità di generalità, $M = A^{1,\ldots, k}_{1,\ldots,k}$ tale minore (altrimenti è sufficiente considerare una permutazione delle righe e
		delle colonne per ricadere in questo caso; tale permutazione è ammessa dall'algoritmo di Gauss). Si mostra che $A^j \in \Span(A^1, \ldots, A^k)$ $\forall j > k$. Si consideri ogni orlato $M_j$ di $M$ ottenuto scegliendo
		la $j$-esima colonna di $A$: per ipotesi $\det(M_j) = 0$, ed il rango è almeno
		$k$. Quindi $\rg(M_j) = k$; poiché le prime $k$ righe sono linearmente indipendenti, l'ultima riga aggiunta deve certamente appartenere al loro
		sottospazio generato. Quindi ogni riga di $A^{1,\ldots, k, j}$ appartiene
		al sottospazio $\Span(A_1, \ldots, A_k)$, da cui si deduce che $\rg(A^{1,\ldots, k, j}) = k$, e quindi che $\rg(A^{1,\ldots,k,j}) = k \implies A^j \in \Span(A^1, \ldots, A^k) \implies \rg(A) = k$.
		
		\subsection{Autovalori, diagonalizzabilità e triangolabilità}
		
		Sia $f \in \End(V)$. Si dice che $\lambda \in \KK$ è un autovalore
		di $f$ se e solo se $\exists \vec{v} \neq \vec{0}$, $\vec{v} \in V$
		tale che $f(\vec{v}) = \lambda \vec{v}$, e in tal caso si dice
		che $\vec{v}$ è un autovettore relativo a $\lambda$. Un autovalore
		è tale se esiste una soluzione non nulla a $(f - \lambda \Idv) \vec{v} = \vec{0}$, ossia se e solo se:
		\[\det(f - \lambda \Idv) = 0. \]
		
		Questa relazione è ben definita dacché il determinante è invariante
		per qualsiasi cambio di base applicato ad una matrice associata
		di $f$. Si definisce allora $p_f(\lambda) = \det(f - \lambda \Idv)$,
		detto polinomio caratteristico di $f$, ancora invariante per
		matrici associate a $f$. Si denota inoltre con
		spettro di $f$ l'insieme $\Sp(f)$ degli autovalori di $f$ e con
		$V_\lambda = \Ker(f - \lambda \Idv)$ lo spazio degli autovettori
		relativo a $\lambda$, detto autospazio di $\lambda$.
		
		Si definisce la molteplicità algebrica $\mu_{a,f}(\lambda)$ di un autovalore
		$\lambda$ come la molteplicità che assume come radice del polinomio
		$p_f(\lambda)$. Si definisce la molteplicità geometrica
		$\mu_{g,f}(\lambda)$ di un autovalore $\lambda$ come la dimensione
		del suo autospazio $V_\lambda$. Quando è noto l'endomorfismo
		che si sta considerando si omette la dicitura $f$ nel pedice delle
		molteplicità.
		
		\begin{itemize}
			\item $p_f(\lambda)$ ha sempre grado $n = \dim V$,
			\item $p_f(\lambda)$ è sempre monico a meno del segno,
			\item il coefficiente di $\lambda^n$ è sempre $(-1)^n$,
			\item il coefficiente di $\lambda^{n-1}$ è $(-1)^{n+1} \tr(f)$,
			\item il termine noto di $p_f(\lambda)$ è $\det(f - 0 \cdot \Idv) = \det(f)$,
			\item il termine noto di $p_f(\lambda)$ è $\det(f - 0 \cdot \Idv) = \det(f)$,
			\item $p_f(\lambda)=\sum_{i=0}^{n}(-\lambda)^i(\sum\det(M_{n-i}))$ dove i $M_j$ sono i minori principali di taglia $j$, % TODO: sistemare e aggiungere spiegazione
			\item poiché $p_f(\lambda)$ appartiene all'anello euclideo $\KK[\lambda]$, che è dunque un UFD, esso ammette al più
			$n$ radici,
			\item $\Sp(f)$ ha al più $n$ elementi, ossia esistono al massimo
			$n$ autovalori (dalla precedente considerazione),
			\item se $\KK = \CC$ e $\charpoly{f} \in \RR[\lambda]$, $\lambda \in
			\Sp(f) \iff \overline{\lambda} \in \Sp(f)$ (infatti $\lambda$ è
			soluzione di $\charpoly{f}$, e quindi anche $\overline{\lambda}$
			deve esserne radice, dacché i coefficienti di $\charpoly{f}$ sono
			in $\RR$),
			\item se $\KK$ è un campo algebricamente chiuso, $p_f(\lambda)$
			ammette sempre almeno un autovalore distinto (o esattamente
			$n$ se contati con molteplicità),
			\item $0 \in \Sp(f) \iff \dim \Ker f > 0 \iff \rg f < 0 \iff \det(f) = 0$,
			\item autovettori relativi ad autovalori distinti sono sempre
			linearmente indipendenti,
			\item dati $\lambda_1$, ..., $\lambda_k$ autovalori di $f$,
			gli spazi $V_{\lambda_1}$, ..., $V_{\lambda_k}$ sono sempre
			in somma diretta,
			\item $\sum_{i=1}^k \mu_a(\lambda_i)$ corrisponde al numero
			di fattori lineari di $p_f(\lambda)$,
			\item $\sum_{i=1}^k \mu_a(\lambda_i) = n \iff$ $p_f(\lambda)$
			è completamente fattorizzabile in $\KK[\lambda]$,
			\item vale sempre la disuguaglianza $n \geq \mu_a(\lambda) \geq
			\mu_g(\lambda) \geq 1$ (è sufficiente considerare una
			base di $V_\lambda$ estesa a base di $V$ e calcolarne il
			polinomio caratteristico sfruttando i blocchi della matrice
			associata, notando che $\mu_g(\lambda)$ deve forzatamente essere
			minore di $\mu_a(\lambda)$),
			\item vale sempre la disuguaglianza $n \geq \sum_{i=1}^k \mu_a(\lambda_i) \geq \sum_{i=1}^k \mu_g(\lambda_i)$,
			\item se $W \subseteq V$ è un sottospazio $f$-invariante,
			allora $\charpolyrestr{f}{W} \mid p_f(\lambda)$\footnote{Quando si lavora
				su degli endomorfismi, la notazione $\restr{f}{W}$ è impiegata per
				considerare $f$ ristretta a $W$ sia sul dominio che sul codominio.} (è sufficiente
			prendere una base di $W$ ed estenderla a base di $V$, considerando
			poi la matrice associata in tale base, che è a blocchi),
			\item se $W \subseteq V$ è un sottospazio $f$-invariante,
			ed estesa una base $\basis_W$ di $W$ ad una $\basis$ di $V$,
			detto $U = \Span(\basis \setminus \basis_W)$ il supplementare di $W$ che si ottiene da tale base $\basis$, vale
			che $\charpoly{f} = \charpolyrestr{f}{W} \cdot \charpoly{\hat{f}}$,
			dove $\hat{f} : V/W \to V/W$ è tale che $\hat{f}(\vec{u} + W) = f(\vec{u}) + W$ (come prima, è sufficiente considerare una matrice
			a blocchi),
			\item se $V = W \oplus U$, dove sia $W$ che $U$ sono $f$-invarianti,
			allora $\charpoly{f} = \charpolyrestr{f}{W} \cdot \charpolyrestr{f}{U}$ (la matrice associata in un'unione di basi
			di $W$ e $U$ è infatti diagonale a blocchi),
			\item se sia $W$ che $U$ sono $f$-invarianti, allora $f$ è diagonalizzabile
			se e solo se sia $\restr{f}{W}$ che $\restr{f}{U}$ lo sono,
			\item se $f$ è nilpotente, $p_f(\lambda) = \lambda^n$ (è sufficiente considerare
			un eventuale altro autovalore diverso da zero e mostrare che se tale
			autovalore esistesse, $f$ non sarebbe nilpotente),
			\item un endomorfismo è nilpotente se e solo se $f^n = 0$ (discende direttamente dal teorema di Hamilton-Cayley e dalla forma di $p_f$),
			\item l'unico endomorfismo diagonalizzabile e nilpotente è quello nullo,
			\item in un campo algebricamente chiuso, un endomorfismo è diagonalizzabile
			se e solo se è semisemplice (i.e.~se ogni sottospazio $f$-invariante ammette un
			supplementare $f$-invariante).
		\end{itemize}
		
		Si dice che $f$ è diagonalizzabile se $V$ ammette una base per cui
		la matrice associata di $f$ è diagonale, o equivalentemente se,
		dati $\lambda_1$, ..., $\lambda_k$ autovalori di $f$, si verifica
		che:
		
		\[ V = V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_k}. \]
		
		Ancora in modo equivalente si può dire che $f$ è diagonalizzabile
		se e solo se:
		
		\[ \begin{cases} \sum_{i=1}^k \mu_a(\lambda_i) = n, \\ \mu_g(\lambda_i) = \mu_a(\lambda_i) \; \forall 1 \leq i \leq k, \end{cases} \]
		
		ossia se il polinomio caratteristico è completamente fattorizzabile
		in $\KK[\lambda]$ (se non lo fosse, la somma diretta
		$V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_k}$ avrebbe
		forzatamente dimensione minore di $V$, ed esisterebbero altri
		autovalori in un qualsiasi campo di spezzamento di $p_f(\lambda)$) e se $\sum_{i=1}^k \mu_g(\lambda_i) = n$. Tale condizione, in un
		campo algebricamente chiuso, si riduce a $\mu_g(\lambda_i) = \mu_a(\lambda_i)$, $\forall 1 \leq i \leq k$.
		
		Considerando la forma canonica di Jordan di $f$, si osserva anche
		che $f$ è diagonalizzabile se e solo se per ogni autovalore la
		massima taglia di un blocco di Jordan è esattamente $1$, ossia
		se il polinomio minimo di $f$ è un prodotto di fattori lineari
		distinti (i.e.~se $\varphi_f(t) = \prod_i (t-\lambda_i)$). Si può fare la stessa considerazione guardando al
		teorema di decomposizione primaria (gli indici di Fitting del
		sottospazio generalizzato sono esattamente le moltiplicità algebriche
		degli autovalori nel polinomio minimo).
		
		Data $f$ diagonalizzabile, la matrice diagonale $J$ a cui $f$ è
		associata è, dati gli autovalori $\lambda_1$, ..., $\lambda_k$,
		una matrice diagonale dove $\lambda_i$ compare sulla diagonale
		esattamente $\mu_g(\lambda_i)$ volte.
		
		Data $A \in M(n, \KK)$, $A$ è diagonalizzabile se e solo se $f_A$,
		l'applicazione indotta dalla matrice $A$, è diagonalizzabile,
		ossia se $A$ è simile ad una matrice diagonale $J$, computabile
		come prima. Si scrive in particolare $p_A(\lambda)$ per indicare
		$p_{f_A}(\lambda)$.
		
		Una matrice $P \in \GL(M(n, \KK))$
		tale che $A = P J P\inv$, è tale che $AP = PJ$: presa la $i$-esima
		colonna, allora, $AP^{(i)} = PJ^{(i)} = P^{(i)}$; ossia è sufficiente
		costruire una matrice $P$ dove l'$i$-esima colonna è un autovettore
		relativo all'autovalore presente in $J_{ii}$ linearmente indipendente
		con gli altri autovettori presenti in $P$ relativi allo stesso
		autovalore (esattamente nello stesso modo in cui si costruisce in
		generale tale $P$ con la forma canonica di Jordan).
		
		Se $A$ e $B$ sono diagonalizzabili, allora $A \sim B \iff p_A(\lambda) =
		p_B(\lambda)$ (infatti due matrici diagonali hanno lo stesso polinomio
		caratteristico se e solo se compaiono gli stessi identici autovalori).
		
		Se $f$ è diagonalizzabile, allora ogni spazio $W$ $f$-invariante di
		$V$ è tale che:
		\[ W = (W \cap V_{\lambda_1}) \oplus \cdots \oplus (W \cap V_{\lambda_k}), \]
		
		dove $\lambda_1$, ..., $\lambda_k$ sono gli autovalori distinti di
		$f$, e dunque $\restr{f}{W}$ è sempre diagonalizzabile, se $f$ lo è.
		In generale, dato un sottospazio $W$ di $V$ che
		è $f$-invariante, si può facilmente costruire un suo
		supplementare $f$-invariante. È infatti sufficiente
		prendere una base di $W$ ed estenderla a base di $V$
		completandola tramite una base di autovettori di $V$.
		
		Se $f$ è diagonalizzabile, anche $f^k$ lo è, per ogni $k \in \NN$. Se
		ogni vettore di $V$ è un autovettore di $f$, allora $f = \lambda \Id$,
		con $\lambda \in \KK$ (è sufficiente considerare l'eventuale esistenza di più
		autospazi e due vettori $\v$ e $\w$ di due autospazi distinti e considerare
		le due scritture possibili di $f(\v + \w)$).
		
		Si dice infine che $f$ è triangolabile (o triangolarizzabile) se $V$
		ammette una base per cui la matrice associata di $f$ è triangolare superiore
		(o inferiore, dal momento che è sufficiente riordinare dal basso la base
		per ottenere una matrice associata triangolare superiore). Vale in particolare
		che $f$ è triangolabile se e soltanto se $p_f(\lambda)$ è completamente
		riducibile in fattori lineari in $\KK$ (dunque, nel caso di $\KK$ algebricamente
		chiuso, $f$ è sempre triangolabile). Infatti, se $f$ è triangolabile, il polinomio
		caratteristico ha come radici esattamente gli elementi sulla diagonale della
		matrice associata di $f$ nella base $\basis$ in cui tale matrice è triangolare
		superiore (e dunque $p_f(\lambda)$ è riducibile in fattori lineari). Se invece $p_f(\lambda)$ è riducibile in fattori lineari, si può applicare il seguente
		algoritmo (su cui si fonda induttivamente la dimostrazione della proposizione):
		
		\begin{enumerate}
			\itemsep 0pt
			\item Si calcolino le basi degli autospazi di $f$,
			\item Si estenda l'unione $\basis_A$ di queste basi a una base $\basis$ di $V$,
			\item Si consideri la matrice associata di $f$ nella base $\basis$, della forma: \setlength{\extrarowheight}{1.3pt}
			\[M_\basis(f) = \begin{pmatrix}
				A
				& \rvline & B \\
				\hline
				0 & \rvline &
				C
			\end{pmatrix}, \]\setlength{\extrarowheight}{0pt}dove $A$ è una matrice diagonale contenente gli autovalori di $\Sp(f)$,
			\item Se $M_\basis(f)$ è triangolare superiore, l'algoritmo termina. Altrimenti si ripeta l'algoritmo su $C$ (ossia sull'endomorfismo $p_W \circ \restr{f}{W} \in \End(W)$, dove $W$ è il sottospazio generato dai vettori aggiunti alla base $\basis_A$ per costruire la base $\basis$).
		\end{enumerate}
	
		Inoltre, se $W$ è un sottospazio $f$-invariante di $V$,
		e $f$ è triangolabile, anche $\restr{f}{W}$ lo è (infatti,
		in tal caso, il polinomio caratteristico di $f$ si riduce
		in fattori lineari).
		
		\subsubsection{Diagonalizzabilità e triangolabilità simultanea}

		Due endomorfismi $f$, $g \in \End(V)$ diagonalizzabili si dicono simultaneamente diagonalizzabili se esiste una base $\basis$ di $V$
		tale per cui sia la matrice associata di $f$ in $\basis$ che quella
		di $g$ sono diagonali. Vale in particolare che $f$ e $g$ sono
		simultaneamente diagonalizzabili se e solo se $f \circ g = g \circ f$.
		Per trovare tale base è sufficiente, dati $\lambda_1$, ...,
		$\lambda_k$ autovalori di $f$, considerare $\restr{g}{V_{\lambda_i}}$
		$\forall 1 \leq i \leq k$ ($V_{\lambda_i}$ è infatti $g$-invariante,
		dacché, per $\vec{v} \in V_{\lambda_i}$, $f(g(\vec{v})) =
		g(f(\vec{v})) = g(\lambda_i \vec{v}) = \lambda_i g(\vec{v}) \implies
		g(\vec{v}) \in V_{\lambda_i}$), che, essendo una restrizione di
		un endomorfismo diagonalizzabile su un sottospazio invariante, è diagonalizzabile: presa allora
		una base di autovettori di $\restr{g}{V_{\lambda_i}}$, questi sono
		anche base di autovettori di $V_{\lambda_i}$; unendo tutti questi
		autovettori in un'unica base $\basis$ di $V$, si otterrà dunque
		che una base in cui le matrici associate di $f$ e $g$ sono diagonali.
		
		Analogamente due endomorfismi $f$, $g \in \End(V)$ triangolabili si dicono
		simultaneamente triangolabili se esiste una base $\basis$
		in cui $M_\basis(f)$ e $M_\basis(g)$ sono due matrici
		triangolari superiori. Non è generalmente vero che
		due endomorfismi simultaneamente triangolabili
		commutano; è tuttavia vero il viceversa. Se infatti $f$
		e $g$ sono due endomorfismi triangolabili tali che $f \circ g = g \circ f$, allora si può riapplicare, con le dovute modifiche, il precedente algoritmo di triangolarizzazione (anche questa volta dimostrabile per induzione):
		\begin{enumerate}
			\itemsep 0pt
			\item Si calcolino le basi degli autospazi di $f$ e si consideri $\restr{f}{U}$, dove $U = \eigsp 1 \oplus \cdots \oplus \eigsp k$,
			\item Si cerchi una base $\basis_U$ in cui $\restr{f}{U}$ e $\restr{g}{U}$ sono simultaneamente diagonalizzabili (osservando che $g$ è $U$-invariante),
			\item Si estenda tale base $\basis_U$ ad una base $\basis$ di $V$ e si chiami $W$ il sottospazio $\Span(\basis_W)$, dove $\basis_W := \basis \setminus \basis_U$,
			\item Si considerino la matrice associata di $f$ e di $g$  nella base $\basis$, della forma: \setlength{\extrarowheight}{1.3pt}
			\begin{gather*}
				M_\basis(f) = \begin{pmatrix}
					A
					& \rvline & B \\
					\hline
					0 & \rvline &
					C
				\end{pmatrix}, \\
				M_\basis(g) = \begin{pmatrix}
					A'
					& \rvline & B' \\
					\hline
					0 & \rvline &
					C'
				\end{pmatrix},
			\end{gather*}
			\setlength{\extrarowheight}{0pt}dove $A$ e $A'$ sono matrici diagonali contenente gli autovalori dei rispettivi endomorfismi,
			\item Se le due matrici sono triangolari superiori, l'algoritmo termina. Altrimenti si ripeta l'algoritmo su $C$ e $C'$ (ossia sugli endomorfismi $p_W \circ \restr{f}{W}$, $p_W \circ \restr{g}{W} \in \End(W)$, i
			quali commutano, dal momento che vale l'identità $C C' = C' C$, dedotta moltiplicando le due matrici associate di sopra).
		\end{enumerate}
	
		\subsubsection{Polinomio minimo}
		
		Sia $f \in \End(V)$. Si può allora definire l'applicazione $\sigma_f : \KK[x] \to \End(V)$
		tale per cui $\sigma_f(p) = p(f)$, dove per $p(f)$ s'intende
		la riscrittura di $p$ a cui si sostituisce all'usuale
		somma e all'usuale prodotto, la somma di applicazioni
		e la composizione (intendendo, in particolare, i termini
		noti come multipli dell'identità $f^0 := \Idv$). In particolare $\sigma_f$ è un omomorfismo di anelli,
		ed è dunque anche un'applicazione lineare. $\sigma_f$ non
		è mai iniettiva, ed esiste dunque sempre un polinomio $p$
		tale per cui $\sigma_f(p) = 0$, l'applicazione nulla (è
		sufficiente prendere $n^2+1$ potenze di $f$ e osservare
		che devono essere linearmente dipendenti). Poiché
		$\KK[x]$ è un PID, $\Ker \sigma_f$ è un ideale principale,
		e quindi esiste un polinomio monico $\varphi_f$, detto
		polinomio minimo di $f$, tale per cui
		$\Ker \sigma_f = (\varphi_f)$.
		
		\begin{itemize}
			\item $\varphi_f \mid p_f$ (teorema di Hamilton-Cayley),
			\item $\deg \varphi_f = d$ se e solo se $\Idv$, $f$, ...,
			$f^{d-1}$ sono linearmente indipendenti e $f^d \in \Span(\Idv, f, \ldots, f^{d-1})$,
			\item $\dim \KK[f] = \deg \varphi_f$ (infatti, per
			il primo teorema di omomorfismo $\KK[f] \cong \KK[x]\quot(\varphi_f)$, da cui si ricava
			facilmente la dimensione dello spazio),
			\item $\Idv$, $f$, ..., $f^{d-1}$ formano una base
			di $\KK[f]$ (per i precedenti risultati), se $d = \deg \varphi_f$,
			\item $\varphi_f$ e $p_f$ condividono gli stessi fattori
			primi (se infatti non comparisse un autovalore come radice di $\varphi_f$, $\varphi_f(f)$ non sarebbe nullo),
			\item gli esponenti dei fattori lineari di $\varphi_f$
			sono esattamente gli indici di Fitting degli autospazi
			generalizzati di $f$,
			\item gli autovalori hanno moltiplicità algebrica $1$ in $\varphi_f$ se e solo se $f$ è diagonalizzabile (è sufficiente utilizzare il precedente risultato, o considerare la forma canonica di Jordan),
			\item se $f$ è nilpotente, $\varphi_f(t) = t^k$, dove $k$ è l'indice di Fitting
			di $\Ker f$ (discende direttamente dalla forma di $p_f$ se $f$ è nilpotente),
			\item se $p \in \KK[x]$ è tale per cui $p = p_1 \cdots p_k$ con $p_1$, ..., $p_k \in \KK[x]$ coprimi, allora $\Ker p(f) = \Ker p_1(f) \oplus \cdots \oplus \Ker p_k(f)$ (teorema di decomposizione primaria; si dimostra facilmente attraverso il teorema di Bezout),
			\item $V = \gensp 1 \oplus \cdots \oplus \gensp k$, se $\lambda_1$, ..., $\lambda_k$ sono tutti gli autovalori di $f$ (deriva direttamente dal teorema
			di Hamilton-Cayley e dal teorema di decomposizione primaria, o, alternativamente,
			dalla decomposizione di Fitting).
		\end{itemize}
	
		Sia $\v \in V$. Si definisce allora l'applicazione
		$\val_{f, \v} : \KK[x] \to V$ in modo tale
		che $\val_{f, \v}(p) = p(f)(\v)$. Come prima,
		$\val_{f,\v}$ è un'applicazione lineare. Si osserva
		ancora che $\Ker \val_{f, \v}$ è un'ideale,
		e quindi che esiste un polinomio $\varphi_{f, \v}$
		tale per cui $\Ker \val_{f, \v} = (\varphi_{f, \v})$.
		Tale polinomio viene denotato come polinomio minimo
		relativo al vettore $\v$. Si definisce in particolare
		$\KK[f](\v) := \Im \val_{f, \v}$.
		
		\begin{itemize}
			\item $\varphi_{f, \v} \mid \varphi_f$ (infatti $\varphi_f(f)=0$, e dunque $\varphi_f(f)$ annulla $\v$),
			\item $\deg \varphi_{f, \v} = d$ se e solo se $\v$, $f(\v)$, ..., $f^{d-1}(\v)$ sono linearmente indipendenti
			e $f^d(\v) \in \Span(\v, \ldots, f^{d-1}(\v))$,
			\item $\dim \KK[f](\v) = \deg \varphi_{f, \v}$ (si dimostra allo stesso modo in cui si è dimostrata la proposizione analoga per $\varphi_f$),
			\item $\v$, ..., $f^{d-1}(\v)$ formano una base
			di $\KK[f](\v)$, se $d = \deg \varphi_{f, \v}$.
			\item se $\vv 1$, ..., $\vv k$ sono generatori
			di $V$, allora $\varphi_f = \mcm(\varphi_{f, \vv 1}, \ldots, \varphi_{f, \vv k})$ (è sufficiente mostrare
			che $\varphi_f$ annulla una base e che il grado è minimale).
			\item se $\v$, ..., $f^{k}(\v)$ sono linearmente indipendenti per qualche $\v \in V$, allora $\deg \varphi_f \geq \varphi_{f, \v} \geq k + 1$.
			\item esiste sempre un vettore $\v$ tale per cui
			$\varphi_f = \varphi_{f, \v}$ (se $\KK$ è infinito).
			\item $p(f)$ è invertibile $\iff \Ker p(f) = \zerovecset$ $\iff \MCD(\varphi_f, p) \in \KK^*$, se $p \in \KK[x]$ (è sufficiente
			applicare il teorema di Bezout).
		\end{itemize}
		
		Un vettore $\v$ si dice ciclico rispetto a $f$ se
		gli $n$ vettori $\v$, ..., $f^{n-1}(\v)$ formano
		una base di $V$, in tal caso detta base ciclica
		di $V$.
		
		Se $\KK$ è infinito, $V$ ammette una base ciclica se e solo se $p_f = \pm \varphi_f$ (infatti esiste sempre un vettore $\v$ tale per cui $\varphi_f = \varphi_{f, \v}$). In
		una base ciclica $\basis$ la matrice associata si
		scrive nel seguente modo:
		\[ M_\basis(f) = \Matrix{1 & & & -a_0 \\ & \ddots & & \vdots \\ & & 1 & -a_{n-1}}, \]
		
		dove $\varphi_f(x) = x^n + a_{n-1} x^{n-1} + \ldots + a_0$.
		Tale matrice viene detta matrice compagna del polinomio
		$p := \varphi_f$ (e dunque ogni polinomio monico è in particolare
		il polinomio minimo di un qualche endomorfismo; analogamente
		ogni polinomio monico è, a meno del segno, un polinomio
		caratteristico).
		
		\subsection{La forma canonica di Jordan}
		
		Si definisce blocco di Jordan di taglia $k$ relativo
		all'autovalore $\lambda$ la seguente matrice:
		\[J_{\lambda, k} :=\begin{pmatrix}
			\lambda&1&0&\cdots&0         \\                       
			0&\ddots&\ddots&&\vdots      \\
			\vdots&\ddots&\ddots&\ddots&0\\
			\vdots&&\ddots&\ddots&1      \\
			0&\cdots&\cdots&0&\lambda    
		\end{pmatrix},\]
		
		ossia la matrice che ha solo $\lambda$ sulla diagonale, $1$ sulla
		sopradiagonale e $0$ nelle altre posizioni. Si può
		sempre restringere un blocco di Jordan a un blocco nilpotente
		considerando $J = J_{\lambda, k} - \lambda I_k$. Tale blocco
		ha come polinomio minimo $\varphi_J(t) = t^k$, e dunque
		$\varphi_{J_{\lambda, k}}(t) = (t-\lambda)^k$. Allo stesso
		modo si calcola $p_{J_{\lambda, k}}(t) = (t-\lambda)^k$. Si osserva dunque
		che $\mu_{a, J_{\lambda, k}}(\lambda) = \mu_{a, J}(0)$.
		
		Poiché il polinomio caratteristico ed il polinomio minimo coincidono a meno
		del segno, esiste sempre una base ciclica per la quale $J_{\lambda, k}$
		si scrive come matrice compagna di $\varphi_{J_{\lambda, k}}$.
		
		Si definisce forma canonica di Jordan di un endomorfismo $f$
		una sua matrice associata in una base $\basis$ tale per cui:
		\[M_\basis(f) = \begin{pmatrix}
			J_1 &          & 0   \\
			& \ddots &     \\ 
			0 &          & J_s \end{pmatrix}, \]

		dove $J_1$, ..., $J_s$ sono blocchi di Jordan. La forma canonica
		di Jordan esiste sempre ed è unica a meno di permutazione dei blocchi,
		se tutti gli autovalori di $f$ sono in $\KK$ (teorema di Jordan; se
		gli autovalori di $f$ non sono tutti in $\KK$, si può sempre considerare
		un'estensione di campo in cui esistono).
		
		Si definisce autospazio generalizzato relativo all'autovalore $\lambda$ di
		$f \in \End(V)$ lo spazio:
	
		\[ \Gensp = \Ker (f - \lambda \Idv)^n. \]
		
		Una definizione alternativa, ma equivalente di $\Gensp$ è la seguente:
		
		\[ \Gensp = \{ \v \in V \mid \exists k \in \NN \mid (f-\lambda \Idv)^k = \vec 0 \}, \]
		
		ossia $\Gensp$ è lo spazio dei vettori $\v \in V$ tali per cui, applicando ripetutamente $f-\lambda \Idv$, si ottiene un autovettore relativo a $\lambda$ (per
		dimostrare l'equivalenza delle due dimostrazioni è sufficiente considerare la
		decomposizione di Fitting). In generale, dalla catena della decomposizione
		di Fitting, si deduce in realtà che:
		
		\[ \Gensp = \Ker (f - \lambda \Idv)^q \; \forall q \geq k, \]
		
		dove $k$ è la molteplicità algebrica di $\lambda$ in $\varphi_f$ (in particolare
		si ottiene sempre l'autospazio generalizzato sostituendo $\mu_a(\lambda)$ a $q$,
		dacché $\mu_a(\lambda) \geq k$).
		
		In generale vale che:
		\[ V = \gensp 1 \oplus \cdots \oplus \gensp k, \]
		se $\lambda_1$, ..., $\lambda_k$ sono tutti gli autovalori di $f$ (vd.~polinomio minimo). Inoltre, $\restr{f}{\Gensp}$ ammette come autovalore soltanto $\lambda$
		(pertanto $\dim \Gensp = \mu_{a, f}(\lambda)$, confrontando i polinomi caratteristici). Si osserva inoltre che $\Gensp$ è sempre $f$-invariante. Infatti ogni $f$ induce due catene di inclusione:
		
		\begin{gather*}
			\Ker f^0 = \zerovecset \subsetneqq \Ker f^1 \subsetneqq \cdots \subsetneqq \Ker f^k = \Ker f^{k+1} = \cdots, \\
			\Im f^0 = V \supsetneqq \Im f^1 \supsetneqq \cdots \supsetneqq \Im f^k = \Im f^{k+1} = \cdots,
		\end{gather*}
		
		dove $k$ è detto indice di Fitting di $f$. Vale in particolare la decomposizione di Fitting:
		\[ V = \Ker f^k \oplus \Im f^k, \]
		
		dove $\restr{f}{\Ker f^k}$ è nilpotente (e dunque ammette solo $0$ come autovalore;
		infatti $(\restr{f}{\Ker f^k})^k = \restr{f^k}{\Ker f^k} = 0$),
		mentre $\restr{f}{\Im f^k}$ è invertibile (e dunque non ammette $0$ come autovalore;
		infatti tale endomorfismo mantiene le dimensioni delle immagini).
		
		\begin{itemize}
			\itemsep 0pt
			\item esiste sempre almeno un blocco di Jordan relativo a $\lambda$ di ordine $k$,
			dove $k$ è la molteplicità algebrica di $\lambda$ in $\varphi_f$,
			\item la successione di $\Ker (f-\lambda \Idv)^t - \Ker (f - \lambda \Idv)^{t-1}$
			all'aumentare di $t$ è decrescente ed è definitivamente $0$,
			\item il numero di blocchi di Jordan di taglia maggiore o uguale a $t$ relativi
			a $\lambda$ è esattamente $\Ker (f-\lambda \Idv)^t - \Ker (f - \lambda \Idv)^{t-1}$,
			\item il numero di blocchi di Jordan di taglia $t$ relativi a $\lambda$
			è esattamente:
			\begin{gather*}
				2 \dim \Ker (f-\lambda \Idv)^t - \dim \Ker (f-\lambda \Idv)^{t+1} \\ - \dim \Ker (f-\lambda \Idv)^{t-1},
			\end{gather*}
			riscrivibile anche come:
			\begin{gather*}
				\rg (f-\lambda \Idv)^{t+1} + \rg (f-\lambda \Idv)^{t-1} - \\
				2 \rg (f-\lambda \Idv),
			\end{gather*}
			(da queste due identità risulta evidente l'unicità della forma canonica di Jordan),
			\item esistono esattamente $\mu_g(\lambda) = \dim \Ker (f - \lambda \Idv)$ blocchi
			relativi all'autovalore $\lambda$,
			\item $\mu_g(\lambda) = 1$ $\forall \lambda \in \Sp(f)$ implica
			che vi sia un solo blocco relativo ad ogni $\lambda \in \Sp(f)$; dal
			momento che ne deve esiste uno di ordine massimo, tale blocco ha taglia
			$k$, dove $k$ è la molteplicità algebrica di $\lambda$ in $\varphi_f$,
			\item $\mu_g(\lambda) = 1$ $\forall \lambda \in \Sp(f)$ implica che
			$p_f = \pm \varphi_f$ (e dunque che $f$ ammette una base ciclica; segue
			direttamente dal precedente risultato),
			\item una base di $\Ker (f - \lambda \Idv)^t$ è data dai primi $t$ vettori
			di ogni blocco relativo a $\lambda$,
			\item due matrici $A$, $B$ sono simili se e solo se condividono la stessa
			forma canonica di Jordan (a meno di permutazione di blocchi; dunque la
			forma canonica di Jordan è un invariante completto della similitudine),
			\item Se $\KK=\CC$, vale l'identità:
			\[ \conj{\Ker (f - \lambda \Idv)^k} = \Ker (f - \conj{\lambda} \Idv)^k, \]
			
			da cui è possibile ottenere una base dell'autospazio generalizzato relativo
			a $\conj{\lambda}$ coniugando una base dell'autospazio generalizzato relativo
			a $\lambda$ (in particolare i due spazi hanno la stessa dimensione),
			\item Se $\KK=\CC$, la forma canonica di Jordan contiene tanti blocchi di taglia $t$
			relativi a $\lambda$ quanti ve ne sono di relativi a $\conj{\lambda}$,
			\item Esistono e sono unici i due endomorfismi $\mu$, $\delta \in \End(V)$
			tale che $\mu$ sia diagonalizzabile, $\delta$ sia nilpotente e che
			$f = \mu + \delta$ (se esiste la forma canonica di Jordan; decomposizione di
			Jordan-Chevalley),
			\item Se $\forall \lambda \in \Sp(f)$, $\mu_g(\lambda) = 1$, allora
			esiste un numero finito di sottospazi invarianti e sono tutte le possibili
			somme dirette dei sottospazi degli autospazi generalizzati, %TODO: migliorare
			\item Se $\KK$ è infinito ed esiste $\lambda \in \Sp(f)$ tale per cui
			$\mu_g(\lambda) > 1$, allora esiste un numero infinito di sottospazi invarianti
			per ogni dimensione, da $1$ a $\dim V -1$. %TODO: migliorare + sono esattamente $\sum_{k=0}^{n}\binom{n}{k}=2^n
		\end{itemize}
		
		\subsubsection{Calcolo di una base di Jordan}
		
		Si dice base di Jordan una qualsiasi base $\basis$ tale per cui
		$M_\basis(f)$ è una forma canonica di Jordan, se $f \in \End(V)$.
		Per calcolare una base di Jordan si può seguire il seguente
		algoritmo:
		
		\begin{enumerate}
			\item Si calcoli il polinomio caratteristico $p_f$ di $f$ e se
			ne estragga lo spettro $\Sp(f)$,
			\item Si consideri una base $\basis$ di $V$ e si ponga
			$A := M_\basis(f)$,
			\item Si consideri ogni autovalore $\lambda \in \Sp(f)$:
				\begin{enumerate}[a.]
					\item Si consideri $B := A - \lambda I_n$. Si calcoli
					il rango di $B$ per ricavare $\mu_g(\lambda)$, indicante
					il numero di blocchi relativi a $\lambda$,
					\item Se possibile, si facciano considerazioni riguardo
					a come deve essere la forma canonica di Jordan. Altrimenti
					si calcoli il numero di blocchi tramite la formula
					presentata precedentemente,
					\item Si calcolino le matrici della forma $B^i$ con $2 \leq i \leq k-1$,
					dove $k$ è la taglia del blocco più grande,
					\item Si calcolino le basi dei sottospazi $U_i$ tali per cui:
					\begin{flalign*}
						&\Ker B^k = \Ker B^{k-1} \oplus U_1, \\
						&\Ker B^{k-1} = \Ker B^{k-2} \oplus B(U_1) \oplus U_2, \\
						&\,\vdots \\
						&\Ker B = B^{k-1}(U_1) \oplus B^{k-2}(U_2) \oplus \cdots \oplus U_k;
					\end{flalign*}
					\item Si scelgano da queste basi i vettori che generano ogni blocco
					relativo a $\lambda$ (in particolare ogni vettore di base di $U_i$ genera
					un blocco di taglia $k-1+i$),
					\item Per ogni blocco, generato dal vettore $\v$, si costruisca una base ordinata nel seguente modo:
					\[ \basis' = \{B^{t-1} \v ,\ldots , B \v, \v\}, \]
					dove $t$ è l'indice minimo per cui $B^t \v = 0$;
				\end{enumerate}
			\item Si uniscano ordinatamente a catena le basi ottenute in una base $\basis_J$. La base $[]_\basis\inv \basis_J$ è allora base di Jordan. In particolare, se
			$P = \Matrix{\v_1 \cdots \v_n}$, dove $\basis_J= \{\v_1, \ldots, \v_n\}$, vale
			che $J = P\inv A P$ è esattamente la forma canonica di Jordan individuata
			da tale base.
		\end{enumerate}
		
		Se $f$ è nilpotente, l'algoritmo può essere velocizzato notevolmente considerando
		solamente $B := A$. Se $f$ ha un solo autovalore $\lambda$ e ammette una base ciclica (ossia esiste un solo blocco di Jordan), considerando $B := A - \lambda I_n$,
		quasi ogni vettore è un vettore ciclico (è pertanto consigliato cercare un vettore
		in modo casuale, piuttosto che estendere tutte le basi dei kernel).
		
		\subsubsection{La forma canonica di Jordan reale}

		Sia $A \in M(n, \RR)$. Allora
		la forma canonica di Jordan reale è una variante reale della forma canonica di
		Jordan che esiste sempre (infatti gli autovalori di $A$ non sono forzatamente
		in $\RR$, e potrebbero dunque essere in $\CC \setminus \RR$). La forma canonica di
		Jordan reale si costruisce a partire da una forma canonica di Jordan $J$
		e una sua base di Jordan $\basis$ associata. Tale forma canonica
		si costruisce mediante il seguente algoritmo:
		
		\begin{enumerate}
			\item Si scelga un autovalore $z$, se non si è già considerato il
			suo coniugato $\conj z$:
			\begin{enumerate}[a.]
				\item Si prenda la base $\basis_z = \{\vv 1, \ldots, \vv k, \conj{\vv 1}, \ldots, \conj{\vv k}\}$ che
				genera i blocchi di $z$ e $\conj z$ e si consideri la nuova
				base $\basis_z' = \{ \Re(\vv 1), \imm(\vv 1), \ldots, \Re(\vv k), \imm(\vv k) \}$,
				\item In tale base la forma canonica di Jordan varia eliminando i blocchi
				di $\conj z$, sostituendo all'autovalore $z = a + bi$ il seguente blocco:
				\[ \Matrix{
					a & -b \\ b & a
				}, \]
				ed ingrandendo gli eventuali $1$ mediante l'identità $I_2$ (tale processo prende
				il nome di complessificazione).
			\end{enumerate}
			\item La matrice ottenuta dopo aver considerato tutti gli eventuali autovalori complessi è una forma canonica di Jordan reale, e la base ottenuta mediante
			tutti i processi di complessificazione è una base di Jordan reale.
		\end{enumerate}

		\subsection{Prodotto scalare e congruenza}
		Si consideri una mappa $\varphi : V \times V \to \KK$. Si dice che
		$\varphi$ è un prodotto scalare (e quindi che $\varphi \in \PS(V)$, lo spazio dei prodotti scalari) se è una forma bilineare simmetrica.
		In particolare vale la seguente identità:
		
		\[ \varphi\left( \sum_{i=1}^s a_i \vv i, \sum_{j=1}^t b_j \ww j \right) =
		\sum_{i=1}^s  \sum_{j=1}^t a_i b_j \varphi(\vv i, \ww j). \]
		
		Se $\basis = \{ \vv 1, \ldots ,\vv n \}$ è una base di $V$, si definisce $M_\basis(\varphi) = (\varphi(\vv i, \vv j))_{i,j=1\mbox{--}n}$ come la matrice associata al prodotto scalare $\varphi$. In particolare,
		se $a_\varphi : V \to V^*$ è la mappa lineare che associa a $\v$ il funzionale $\varphi(\v, \cdot) \in V^*$
		tale che $\varphi(\v, \cdot)(\w) = \varphi(\v, \w)$. Si scrive $(V, \varphi)$ per indicare uno
		spazio vettoriale $V$ dotato del prodotto scalare $\varphi$.
		
		Si definisce prodotto scalare \textit{standard} il prodotto $\varphi$ tale che
		$\varphi(\v, \w) = [\v]_\basis^\top [\w]_\basis$.
		
		Si dice che due vettori $\v$, $\w \in V$ sono ortogonali tra loro, scritto come $\v \perp \w$, se
		$\varphi(\v, \w) = 0$. Dato $W$ sottospazio di $V$, si definisce $W^\perp$ come il sottospazio di $V$ dei vettori ortogonali a tutti i vettori di $W$. Si dice che $\varphi$ è non degenere se $V^\perp = \zerovecset$.
		Si scrive in particolare che $V^\perp = \Rad(\varphi)$.
		
		Si dice che $V = U \oplus^\perp W$ (ossia che $U$ e $W$ sono in somma diretta ortogonale) se $V = U \oplus W$ e $U \subseteq W^\perp$. Sia $i : W \to V$ tale che $\w \mapsto \w$. Si scrive $\restr{\varphi}{W}$ intendendo $\restr{\varphi}{W \times W}$.
		
		Ad ogni prodotto scalare si può associare una forma quadratica (e viceversa) $q : V \to \KK$ tale che
		$q(\v) = \varphi(\v, \v)$. Un vettore $\v \in V$ si dice isotropo se $q(\v) = 0$ (altrimenti si dice
		anisotropo). Si definisce il cono isotropo $\CI(\varphi)$ come l'insieme dei vettori isotropi di $V$.
		
		Se $\KK = \RR$, si dice che $\varphi$ è semidefinito positivo ($\varphi \geq 0$) se $q(\v) \geq 0$ $\forall \v \in V$, e che è semidefinito negativo ($\varphi \leq 0$) se $q(\v) \leq 0$ $\forall \v \in V$. Si dice
		che $\varphi$ è definito positivo ($\varphi > 0$) se $\varphi \geq 0$ e se $q(\v) = 0 \iff \v = \vec 0$,
		e che è definito negativo ($\varphi < 0$) se $\varphi \leq 0$ e se $q(\v) = 0 \iff \v = \vec 0$.
		
		Si dice che $\varphi$ è definito se è definito positivo o definito negativo. Analogamente $\varphi$
		è semidefinito se è semidefinito positivo o semidefinito negativo.
		
		Si scrive $\v^\perp$ per indicare tutti i vettori ortogonali a $\v$ (e quindi
		$\v^\perp = \Span(\v)^\perp$). Si definisce $\iota : W \to V$ come l'applicazione
		tale per cui $\iota(\w) = \w$. Si scrive $\restr{\varphi}{U}$ con $U$ sottospazio
		di $V$ per indicare il prodotto scalare $\restr{\varphi}{U \times U}$.
		
		Sia ora $V$ di dimensione finita.
		
		\begin{itemize}
			\item $M_\basis(\varphi)$ è simmetrica,
			\item $\varphi(\v, \w) = [\v]_\basis^\top M_\basis(\varphi) [\w]_\basis$,
			\item $M_\basis(\varphi) = M^\basis_{\basis^*}(a_\varphi)$,
			\item $\Ker a_\varphi = V^\perp$,
			\item $\varphi$ è non degenere se e solo se $M_\basis(\varphi)$ è invertibile,
			\item $W^\perp = \Ker i^\top \circ a_\varphi$,
			\item $a_\varphi(W^\perp) = \Ann(W) \cap \Im a_\varphi$,
			\item $\dim W + \dim W^\perp = \dim V + \dim (W \cap V^\perp)$ (da sopra),
			\item $V = W \oplus^\perp W^\perp$ se e solo se $\restr{\varphi}{W}$ è non degenere ($\iff W \cap W^\perp = \Rad(\restr{\varphi}{W}) = \zerovecset$),
			\item $V = W + W^\perp$ se e solo se $\Rad(\restr{\varphi}{W}) \subseteq \Rad(\varphi)$,
			\item se $V = U \oplus^\perp W$, allora $\Rad(\varphi) = \Rad(\restr{\varphi}{U}) \oplus \Rad(\restr{\varphi}{W})$,
			\item $V = \Span(\w) \oplus^\perp \Span(\w)^\perp \iff q(\w) \neq 0 \iff \w \notin \CI(\varphi)$,
			\item $\Span(\w) \subseteq \Span(\w)^\perp \iff \w \in \CI(\varphi)$,
			\item $(W^\perp)^\perp = W^\dperp = W + \Rad(\varphi) = W + V^\perp$,
			\item $\v \in V^\perp \iff \Span(\v)^\perp = V$,
			\item $W^\perp = (\Span(W))^\perp$,
			\item $W^\perp = \bigcap_{\w \in U} \w^\perp$,
			\item se $\ww 1$, ..., $\ww k$ generano $W$, allora $W^\perp = \bigcap_{i = 1}^k \ww i ^\perp$,
			\item $W^\perp = \alpha_\varphi\inv(\Ann(W))$,
			\item $\alpha_\varphi(W^\perp) = \Ann(W) \cap \Im \alpha_\varphi$,
			\item $V \subseteq W \implies W^\perp \subseteq V^\perp$,
			\item $(U + W)^\perp = U^\perp \cap W^\perp$,
			\item $((W^\perp)^\perp)^\perp = (W + V^\perp)^\perp = W^\perp \cap (V^\perp)^\perp = W^\perp \cap V = W^\perp$,
			\item $(U \cap W)^\perp \supseteq U^\perp + W^\perp$,
			\item $(U \cap W)^\perp = U^\perp + W^\perp$, se $\varphi$ è non degenere,
			\item $\varphi$ è definito $\iff$ $\CI(\varphi) = \zerovecset$,
			\item $\varphi$ è (semi)definito $\implies$ ogni sua restrizione è (semi)definita,
			\item $\varphi$ è semidefinito $\iff$ $\CI(\varphi) = V^\perp = \Rad(\varphi)$ (considera l'esistenza
			di due vettori $\v$, $\w \in V$ con forme quadratiche discordi, osserva che sono linearmente indipendenti
			e trova un $\lambda \in \KK$ tale per cui $\v + \lambda \w$ crea un assurdo),
			\item $\Im(\alpha_\varphi) \subseteq \Ann(V^\perp)$ (se $V$ è di dimensione infinita),
			\item $\Im(\alpha_\varphi) = \Ann(V^\perp)$ (se $V$ è di dimensione finita),
			\item $\Rad(\restr{\varphi}{U}) = U^\perp \cap U$,
			\item $\CI(\restr{\varphi}{U}) = \CI(\varphi) \cap U$,
		\end{itemize}
		
		Se $U$ è un sottospazio di $V$, $\varphi$ induce un prodotto scalare $\hat \varphi : V/U \times V/U \to \KK$ tale che $\hat \varphi([\vv 1]_U, [\vv 2]_U) = \varphi(\vv 1, \vv 2)$ se e solo se $U \subseteq V^\perp$. In particolare, se $U = V^\perp$,
		$\hat \varphi$ è anche non degenere.

		Due esempi classici di prodotto scalare sono $\varphi(A, B) = \tr(AB)$ e
		$\psi(A, B) = \tr(AB^\top)$, entrambi su $M(n, \KK)$. I due prodotti sono
		entrambi non degeneri, e vale che:
		
		\begin{itemize}
			\item $\restr{\varphi}{\Sym(n, \KK)} = \restr{\psi}{\Sym(n, \KK)}$,
			\item $\restr{\varphi}{\Lambda(n, \KK)} = -\restr{\psi}{\Lambda(n, \KK)}$,
			\item se $\Char \KK \neq 2$, $V = \Sym(n, \KK) \oplus^\perp \Lambda(n, \KK)$,
			per ambo i prodotti scalari.
		\end{itemize}
		
		Se $\basis'$ è un'altra base di $V$, vale il seguente \textit{teorema di cambiamento di base}:
		
		\[ M_{\basis'}(\varphi) = M_{\basis}^{\basis'}(\Idv)^\top \, M_\basis(\varphi) \, M_{\basis}^{\basis'}(\Idv). \]
		
		Si definisce relazione di congruenza la relazione di equivalenza $\cong$ (o $\equiv$) definita
		su $\Sym(n, \KK)$ nel seguente modo:
		
		\[ A \cong B \iff \exists P \in \GL(n, \KK) \mid A = P^\top B P. \]
		
		
		\begin{itemize}
			\item $A \cong B \implies \rg(A) = \rg(B)$ (il rango è invariante per congruenza; e dunque si può
			definire $\rg(\varphi)$ come il rango di una qualsiasi matrice associata a $\varphi$),
			\item $A \cong B \implies \det(A) \det(B) \geq 0$ (in $\KK = \RR$ il segno del determinante è invariante per congruenza),
			\item Due matrici associate a $\varphi$ in basi diverse sono congruenti per la formula
			di cambiamento di base.
		\end{itemize}
		
		Si definiscono i seguenti tre indici per $\KK = \RR$:
		
		\begin{itemize}
			\item $\iota_+ = \max\{ \dim W \mid W \subseteq V \E \restr{\varphi}{W} > 0 \}$,
			\item $\iota_- = \max\{ \dim W \mid W \subseteq V \E \restr{\varphi}{W} < 0 \}$,
			\item $\iota_0 = \dim V^\perp$,
		\end{itemize}
		
		e si definisce segnatura di $\varphi$ la terna $\sigma = (\iota_+, \iota_-, \iota_0)$.
		
		Si dice che una base $\basis$ di $V$ è ortogonale se i suoi vettori sono a due a due ortogonali (e
		quindi la matrice associata in tale base è diagonale). Se $\Char \KK \neq 2$, valgono i seguenti risultati:
		
		\begin{itemize}
			\item $\varphi(\v, \w) = \frac{q(\v + \w) - q(\v) - q(\w)}{2}$ (formula di polarizzazione; $\varphi$ è
			completamente determinata dalla sua forma quadratica),
			
			\item Esiste sempre una base ortogonale $\basis$ di $V$ (teorema di Lagrange; è sufficiente considerare
			l'esistenza di un vettore anisotropo $\w \in V$ ed osservare che $V = W \oplus^\perp W^\perp$, dove $W = \Span(\w)$, concludendo per induzione; o in caso di non esistenza di tale $\w$, concludere per il
			risultato precedente),
			
			\item (se $\KK = \CC$) Esiste sempre una base ortogonale $\basis$ di $V$ tale che:
			
			\[ M_\basis(\varphi) = \Matrix{I_r & \rvline & 0 \\ \hline 0 & \rvline & 0\,}, \]
			
			\vskip 0.05in
			
			dove $r = \rg(\varphi)$ (teorema di Sylvester, caso complesso; si consideri una base ortogonale e se
			ne normalizzino i vettori anisotropi),
			
			\item Due matrici simmetriche ad elementi complessi con stesso rango allora non solo sono SD-equivalenti, ma sono
			anche congruenti,
			
			\item (se $\KK = \RR$) Esiste sempre una base ortogonale $\basis$ di $V$ tale che:
			
			\[ M_\basis(\varphi) = \Matrix{I_{\iota_+} & \rvline & 0 & \rvline & 0 \\ \hline 0 & \rvline & -I_{\iota_-} & \rvline & 0 \\ \hline 0 & \rvline & 0 & \rvline & 0\cdot I_{\iota_0} }. \]
			
			\vskip 0.05in
			
			Inoltre $\sigma$ è un invariante completo per la congruenza, e vale che, su una qualsiasi base ortogonale $\basis'$ di $V$, $\iota_+$ è esattamente il numero
			di vettori anisotropi di base con forma quadratica positiva, che $\iota_-$ è il numero di vettori con forma
			negativa e che $\iota_0$ è il numero di vettori isotropi (teorema di Sylvester, caso reale; si consideri
			una base ortogonale e se ne normalizzino i vettori anisotropi, facendo infine eventuali considerazioni
			dimensionali per dimostrare la seconda parte dell'enunciato),
			
			\item $\varphi > 0 \iff \sigma = (n, 0, 0)$,
			\item $\varphi < 0 \iff \sigma = (0, n, 0)$,
			\item $\varphi \geq 0 \iff \sigma = (n - k, 0, k)$,
			\item $\varphi \leq 0 \iff \sigma = (0, n - k, k)$,
			con $0 \leq k \leq n$ tale che $k = \dim V^\perp$,
			
			\item I vettori isotropi di una base ortogonale sono una base di $V^\perp$,
			
			\item $\rg(\varphi) = \iota_+ + \iota_-$,
			
			\item $n = \iota_+ + \iota_- + \iota_0$,
			
			\item Se $W$ è un sottospazio di $V$, $\iota_+(\varphi) \geq \iota_+(\restr{\varphi}{W})$ e
			$\iota_-(\varphi) \geq \iota_-(\restr{\varphi}{W})$,
			
			\item Se $V = U \oplus^\perp W$, $\sigma(\varphi) = \sigma(\restr{\varphi}{U}) + \sigma(\restr{\varphi}{W})$,
			
			\item Se $\KK = \RR$ e $A = M_\basis(\varphi)$, allora: 
			\[ \sigma = \textstyle \left( \sum_{\substack{\lambda \in \Sp(\varphi) \\ \lambda > 0}} \mu_a(\lambda), \; \sum_{\substack{\lambda \in \Sp(A) \\ \lambda < 0}} \mu_a(\lambda), \; \mu_0(\lambda) \right), \]			
			come conseguenza del teorema spettrale reale.
		\end{itemize}
		
		Si chiama matrice di Sylvester una matrice della forma vista nell'enunciato del teorema di Sylvester
		reale, e si dice che una base $\basis$ è una base di Sylvester se la matrice ad essa associata è di
		Sylvester. Per il teorema di Sylvester, tale base esiste sempre, e la matrice di Sylvester è unica per
		ogni prodotto scalare $\varphi$.
		
		Se $M \in \Sym(2, \RR)$, $\det(M) < 0 \iff \sigma(M) = (1, 1, 0)$ (e dunque
		se e solo se $M$ rappresenta un piano iperbolico). Al contrario $\det(M) > 0$
		se e solo se $M$ è definita (e in tal caso è definita positiva se il suo
		primo elemento è positivo, e negativa se è negativo). Se $\KK=\RR$, $q(\v) > 0$ e $q(\w) < 0$,
		allora $\v$ e $\w$ sono linearmente indipendenti; in particolare $\Span(\v, \w)$ è
		un piano iperbolico ed esistono $\lambda_1$, $\lambda_2 \in \RR$ tali per
		cui $\lambda_1 \v + \lambda_2 \w$ è isotropo.
		
		\subsection{Prodotto hermitiano}
		
		Sia $V$ un $\CC$-spazio. Allora una mappa $\varphi : V \times V \to \CC$ si
		dice prodotto hermitiano (e quindi si dice che $\varphi \in \PH(V)$, l'$\RR$-spazio dei
		prodotti hermitiani\footnote{Infatti, se $\lambda \in \CC \setminus \RR$ e $\varphi \in \PH(V)$, $\lambda \varphi$ \underline{non} è un prodotto hermitiano, mancando della proprietà del coniugio.}) se è una forma sesquilineare, ossia se è antilineare
		nel primo argomento ed è lineare nel secondo\footnote{In realtà questa convenzione è spesso e volentieri implementata nelle ricerche di Fisica, mentre in Matematica si tende in realtà a mettere l'antilinearità nel secondo argomento. Il corso ha comunque implementato la prima delle due convenzioni, e così si è riportato in queste schede la convenzione scelta.}, e se il coniugio applicato a $\varphi$ ne inverte gli argomenti. In particolare $\varphi$ è un prodotto hermitiano se:
		
		\begin{enumerate}[(i)]
			\item $\varphi(\v, \lambda \U + \w) = \lambda \varphi(\v, \U) + \varphi(\v, \w)$,
			$\forall \v$, $\U$, $\w \in V$, $\lambda \in \KK$,
			\item $\conj{\varphi(\v, \w)} = \varphi(\w, \v)$.
		\end{enumerate}
		
		Un prodotto hermitiano $\varphi$ si comporta pressoché come un prodotto
		scalare su $\RR$ (le definizioni principali sono infatti le medesime). Se $\basis$ è una base di $V$, la matrice associata $M_\basis(\varphi)$ è definita in
		modo tale che $M_\basis(\varphi)_{ij} = \varphi(\vv i, \vv j)$. Infatti tale prodotto soddisfa le seguenti proprietà:
		
		\begin{itemize}
			\item $\varphi(\lambda \v + \w, \U) = \conj{\lambda} \varphi(\v, \U) + \varphi(\w, \U)$, $\forall \v$, $\w$, $\U \in V$, $\lambda \in \CC$, 
			\item $V^\perp = []_\basis \inv (\Ker M_\basis(\varphi))$,
			\item $\varphi$ è non degenere se e solo se $\Ker M_\basis(\varphi) = \zerovecset$,
			\item $\dim W + \dim W^\perp = \dim V + \dim (W \cap V^\perp)$ (formula delle dimensioni),
			\item $\varphi(\v, \w) = [\v]_\basis^* M_\basis(\varphi) [\w]_\basis$,
			\item $M_{\basis'}(\varphi) = \left(M_\basis^{\basis'}(\Idv)\right)^* M_\basis(\varphi) \, M_\basis^{\basis'}(\Idv)$ (formula del cambiamento di base),
			\item si può definire una relazione di equivalenza analoga alla congruenza: $A \sim_* B \defiff \exists M \in \GL(n, \CC) \mid A = M^* B M$,
			\item $\varphi$ è completamente determinato dalla sua forma quadratica $q$ secondo le seguenti due formule di polarizzazione:
			\begin{itemize}
				\item $q(\v + \w) - q(\v) - q(\w) = 2 \Re(\varphi(\v, \w))$,
				\item $q(\v + i \w) - q(\v) - q(\w) = 2 i \imm(\varphi(\v, \w))$,
			\end{itemize}
			\item esiste sempre una base ortogonale per $\varphi$ (\textit{teorema di Lagrange}),
			\item vale il teorema di Sylvester reale e la segnatura in senso hermitiano è un invariante per la relazione $\sim_*$,
			\item $\varphi > 0 \iff \sigma(\varphi) = (n, 0, 0)$,
			\item $\varphi < 0 \iff \sigma(\varphi) = (0, n, 0)$,
			\item $\varphi \geq 0 \iff \sigma(\varphi) = (n-k, 0, k)$, dove $k = \dim V^\perp = \dim \Ker M_\basis(\varphi)$,
			\item $\varphi \leq 0 \iff \sigma(\varphi) = (0, n-k, k)$, dove $k = \dim V^\perp = \dim \Ker M_\basis(\varphi)$.
		\end{itemize}
		
		Esiste un unico modo per complessificare un prodotto scalare $\varphi$, ossia
		esiste un unico prodotto hermitiano $\varphi_\CC$ tale per cui $\varphi_\CC(\v, \w) = \varphi(\v, \w)$ se $\v$, $\w$ sono vettori della parte reale dello spazio complessificato. In particolare $\varphi_\CC$ è determinato dalla seguente
		formula:
		\begin{multline*}
			\varphi_\CC(\vv 1 + i \vv 2, \ww 1 + i \ww 2) = \varphi(\vv 1, \ww 1) + \varphi(\vv 2, \ww 2) \\ + i (\varphi(\vv 1, \ww 2) - \varphi(\vv 2, \ww 1)).
		\end{multline*}
		
		\subsubsection{Funzionali rappresentabili}
		
		Un funzionale $f \in \dual V$ si dice rappresentabile tramite $\varphi$ se
		$f \in \Im \alpha_\varphi$, ossia se $\exists \v \in V \mid f = \varphi(\v, \cdot)$.
		Dal momento che $\Im(\alpha_\varphi) = \Ann(V^\perp)$ (l'inclusione verso destra è facile da dimostrare e l'uguaglianza è data dall'uguaglianza dimensione), $f$ è rappresentabile
		se e solo se $V^\perp \subseteq \Ker f$.
		
		Se $\varphi$ è non degenere, ogni funzionale $f$ è rappresentabile in modo unico (\textit{teorema
		di rappresentazione di Riesz}; infatti $\alpha_\varphi$ sarebbe in tal caso
		un isomorfismo). In particolare, se $\varphi$ è un prodotto scalare, tale
		vettore $\v$, data una base ortogonale $\basis = \{ \vv 1, \ldots, \vv n\}$, è
		determinato nel seguente modo:
		
		\[ \v = \sum_{i=1}^n \frac{f(\vv i)}{\varphi(\vv i, \vv i)} \, \vv i. \]
		
		Se invece $\varphi$ è un prodotto hermitiano, tale vettore $\v$ si determina
		nel seguente altro modo:
		
		\[ \v = \sum_{i=1}^n \conj{\left(\frac{f(\vv i)}{\varphi(\vv i, \vv i)}\right)} \, \vv i. \]
		
		In generale, $f$ è rappresentabile se e solo se, scelta una base $\basis$ di
		$V$, il sistema $M_\basis(\varphi) \x = [f]_{\dual \basis}$ è risolvibile.
		
		Se $W$ è un supplementare di $V^\perp$, e dunque $V = W \oplus V^\perp$, allora
		$\restr{\varphi}{W}$ è non degenere, e dunque $\restr{\alpha_{\varphi}}{W} : W \to \Im \alpha_\varphi$ è
		un isomorfismo da $W$ a $\Im \alpha_\varphi$ (quindi se $f$ è rappresentabile,
		lo è tramite un unico vettore di $W$).
		
		In particolare, se $\v$ rappresenta $f$, allora $\Ker f = \v^\perp$; da cui
		segue che $(\Ker f)^\perp = \v^\dperp = \Span(\v) + V^\perp$. Se $f$ non è
		l'applicazione nulla, $\vec v \notin V^\perp$, e quindi $\Span(\v) \cap V^\perp = \zerovecset \implies (\Ker f)^\perp = \Span(\v) \oplus^\perp V^\perp$. Quindi,
		per computare un vettore $\vv 0$ che rappresenti $f$ è sufficiente prendere
		un supplementare $\Span(\U)$ di $V^\perp$ in $(\Ker f)^\perp$ (infatti l'aggiunta
		di un vettore di $V^\perp$ non varierebbe l'immagine secondo $\alpha_\varphi$) e
		computare $\lambda \in \KK \mid \vv 0 = \lambda \U$ nel seguente
		modo:
		
		\[ \lambda = \frac{\varphi(\lambda \U, \w)}{\varphi(\U, \w)} = \frac{f(\w)}{\varphi(\U, \w)}, \]
		
		dove $\w \notin \Ker f$.
		
		\subsubsection{Algoritmo di ortogonalizzazione di Gram-Schmidt}
		
		Data una base $\basis$ di $V$, se $\abs{\CI(\varphi) \cap \basis} \leq 1$ (ossia se ogni vettore di
		$\basis$ è anisotropo o al più vi è un vettore isotropo, posto in fondo come $\vv n$), si può
		trovare una base ortogonale $\basis' = \{ \vv 1', \ldots, \vv n' \}$ a partire da $\basis$ tale che ne mantenga la stessa bandiera, ossia tale che:
		\[ \Span(\vv 1', \ldots, \vv i') = \Span(\vv 1, \ldots, \vv i) \forall 1 \leq i \leq n. \]
		
		Si definisce $C(\w, \v) = \frac{\varphi(\v, \w)}{\varphi(\w, \w)}$ come il coefficiente di Fourier
		di $\v$ rispetto a $\w$. L'algoritmo allora funziona nel seguente modo:
		
		\begin{enumerate}
			\item Si prenda in considerazione $\vv 1$ e si sottragga ad ogni altro vettore $\vv i$ della base il
			vettore $C(\vv 1, \vv i) \, \vv 1$,
			\item Si ripeta il processo considerando come $\basis$ tutti i vettori di $\basis$ con $\vv 1$ escluso,
			o si termini l'algoritmo una volta che è rimasto un solo vettore.
		\end{enumerate}
		
		Dal momento che l'algoritmo mantiene invariata la bandiera della base,
		una matrice triangolabile è anche ortogonalmente triangolabile se la base
		non contiene alcun (o contiene al più un) vettore isotropo secondo un
		certo prodotto scalare.

		\subsubsection{Metodo di Jacobi per il calcolo della segnatura}
		Sia $A = M_\basis(\varphi)$ una matrice associata a $\varphi$ nella base $\basis$.
		Sia $d_0 := 1$. Se $d_i = \det(A_{1, \ldots, i}^{1, \ldots, i})$ (è possibile anche
		prendere un'altra sequenza di minori, a patto che essi siano principali e che siano
		crescenti per inclusione) è diverso da zero
		per ogni $1 \leq i \leq n-1$, allora $\iota_+$ è il numero di permanenze di segno
		di $d_i$ (zero escluso), $\iota_-$ è il numero di variazioni di segno (zero escluso), e $\iota_0$ è $1$ se
		$d_n = 0$ o $0$ altrimenti.
		
		In generale, se $W$ è un sottospazio di $W'$, $W$ ha codimensione $1$ rispetto a $W'$ e $\det(M_{\basis_W}(\restr{\varphi}{W})) \neq 0$ per una base $\basis_W$ di $W$, allora la segnatura
		di $\restr{\varphi}{W'}$ è la stessa di $\restr{\varphi}{W}$, dove si aggiunge
		$1$ a $\iota_+$, se i determinanti $\det(M_{\basis_W}(\restr{\varphi}{W}))$ e $\det(M_{\basis_{W'}}(\restr{\varphi}{W}))$ (dove $\basis_{W'}$ è una base di $W'$) concordano di segno, $1$ a $\iota_-$, se
		sono discordi, o $1$ a $\iota_0$ se l'ultimo di questi due determinanti è nullo.
		
		Dal metodo di Jacobi si deduce il criterio di definitezza di Sylvester: $A$ è
		definita positiva se e solo se $d_i > 0$ $\forall 1 \leq i \leq n$; $A$ è
		definita negativa se e solo se $(-1)^i d_i > 0$ $\forall 1 \leq i \leq n$.
		
		\subsubsection{Sottospazi isotropi e indice di Witt}
		
		Si dice che un sottospazio $W$ di $V$ è isotropo se $\restr{\varphi}{W} = 0$, o
		equivalentemente se $W \subseteq W^\perp$ (i.e.~se $W \cap W^\perp = W$, e quindi
		se $\Rad(\restr{\varphi}{W}) = W$). Si definisce allora l'indice di Witt $W(\varphi)$ come
		la dimensione massima di un sottospazio isotropo di $V$.
		
		\begin{itemize}
			\item $V^\perp$ è un sottospazio isotropo,
			\item Se $W$ è isotropo, allora $\dim W \leq  \lfloor \frac{\dim V + \dim \Rad(\varphi)}{2} \rfloor$,
			\item Se $W$ è isotropo e $\varphi$ è non degenere, allora $\dim W \leq \lfloor \frac{1}{2} \dim V \rfloor$,
			\item Se $\KK = \RR$, allora $W(\varphi) = \min\{ i_+, i_- \} + i_0$ (è sufficiente considerare
			una base di Sylvester e costruire un nuovo insieme linearmente indipendente $\basis_W$ i cui i vettori sono o isotropi o della forma $\vv i - \ww i$, dove $q(\vv i) = 1$ e $q(\ww i) = 1$, mostrando che $W = \Span(\basis_W)$ è isotropo, concludendo con discussioni dimensionali),
			\item Se $\KK = \CC$, allora $W(\varphi) = \lfloor \frac{\dim V + \dim V^\perp}{2} \rfloor$ (è sufficiente considerare una base di Sylvester per $\varphi$, costruire un nuovo insieme linearmente indipendente $\basis_W$ prendendo quante più coppie $(\vv i, \vv j)$ possibili di vettori della base non isotropi poi associate al vettore $\vv i + i \vv j$, mostrando infine che $W = \Span(\basis_W)$ è isotropo e che è sicuramente massimale perché realizza la dimensione massima possibile secondo le precedenti proposizioni),
			\item Se $\KK=\RR$ e $\varphi$ è definito, allora $W(\varphi) = 0$,
			\item Se $\KK=\RR$ e $\varphi$ è semidefinito, allora $W(\varphi) = i_0$ (e $W = V^\perp$ è un sottospazio
			isotropo di tale dimensione).
		\end{itemize}
		
		\subsubsection{Isometrie tra spazi vettoriali}

		Due spazi vettoriali $(V, \varphi)$ e $(W, \psi)$ su $\KK$ si dicono isometrici tra loro se
		esiste un isomorfismo $f : V \to W$ tale che $\varphi(\vv 1, \vv 2) = \psi(f(\vv 1), f(\vv 2))$.

		Se $f$ è un isomorfismo tra $V$ e $W$, sono equivalenti le seguenti affermazioni:

		\begin{enumerate}[(i)]
			\item $(V, \varphi)$ e $(W, \psi)$ sono isometrici tra loro tramite $f$,
			\item $\forall \basis$ base di $V$, $M_\basis(\varphi) = M_{f(\basis)}(\psi)$,
			\item $\exists \basis$ base di $V$, $M_\basis(\varphi) = M_{f(\basis)}(\psi)$.
		\end{enumerate}
		
		Inoltre, $V$ e $W$ sono isometrici se e solo se hanno la stessa dimensione e le matrici associate
		a $\varphi$ e $\psi$ in due basi di $V$ e di $W$ sono congruenti (infatti, in tal caso, esistono due
		basi di $V$ e di $W$ che condividono la stessa matrice associata, ed è possibile associare ad uno
		ad uno gli elementi di queste basi).
		
		Pertanto, se $\basis_V$ e $\basis_W$ sono due basi di $V$ e di $W$, $\KK = \RR$ e $M_{\basis_V}(\varphi)$ e $M_{\basis_W}(\psi)$ condividono la stessa segnatura, allora $V$ e $W$ sono
		isometrici tra loro (come conseguenza del teorema di Sylvester reale).
		
		Analogamente, se $\KK = \CC$ e $M_{\basis_V}(\varphi)$ e $M_{\basis_W}(\psi)$ condividono lo stesso
		rango, allora $V$ e $W$ sono isometrici tra loro (come conseguenza stavolta del teorema di Sylvester
		complesso).
		
		\subsection{Trasposta e aggiunta di un'applicazione}

		Sia $(V, \varphi)$ uno spazio dotato di un prodotto $\varphi$ non degenere. Allora si definisce $f^* \in \End(V)$ (talvolta indicato come $f^\top$ se $\varphi$ non è hermitiano, quando è chiaro che non ci stia riferendo alla trasposizione dell'operatore $f$)
		come l'unico operatore tale per cui $\varphi(f^*(\v), \w) = \varphi(\v, f(\w))$.
		In particolare, se $\varphi$ non è hermitiano, tale operatore soddisfa la seguente relazione:
		\[ \alpha_\varphi \circ f^* = f^\top \circ \alpha_\varphi, \]
		
		dove con $f^\top$ si indica l'applicazione trasposta di $f$. Scelta allora una
		base $\basis$ di $V$, sempre se $\varphi$ non è hermitiano, si può scrivere in relazione a $M_\basis(f)$ la
		matrice associata a $f^*$:
		\[ M_\basis(f^*) = M_\basis(\varphi)\inv M_\basis(f)^\top M_\basis(\varphi). \]
		
		Se invece $\varphi$ è hermitiano, vale la seguente relazione:
		\[ M_\basis(f^*) = M_\basis(\varphi)\inv M_\basis(f)^* M_\basis(\varphi). \]
		Se $\varphi$ è un prodotto scalare, $f^* = f^\top$ si chiama \textit{trasposto}
		di $f$, mentre se $\varphi$ è hermitiano $f^*$ si dice \textit{aggiunto} di $f$.

		D'ora in poi si intenderà con $f^\top$ il trasposto di $f$ (con $\varphi$ scalare) e con $f^*$ l'aggiunto di $f$ (con $\varphi$ hermitiano). \\ \vskip 0.05in
		
		Un'operatore $f$ si dice \textit{simmetrico} se $f = f^\top$ e quindi se $\varphi(\v, f(\w)) = \varphi(f(\v), \w)$ (analogamente un'operatore si dice \textit{hermitiano} se $f = f^*$). \\
		
		Un'operatore $f$ si dice \textit{ortogonale} se è un'isometria da
		$(V, \varphi)$ in $(V, \varphi)$, ossia se e solo se $\varphi(\v, \w) = \varphi(f(\v), f(\w))$ (analogamente un'operatore si dice \textit{unitario} se è un'isometria con
		$\varphi$ prodotto hermitiano). \\
		
		Sia $\basis$ una base di $V$.
		
		\begin{itemize}
			\item se $\basis$ è una base ortonormale, $M_\basis(f^\top) = M_\basis(f)^\top$ (infatti in tal caso $M_\basis(\varphi) = I_n$),
			\item se $\basis$ è una base ortonormale, $M_\basis(f^*) = M_\basis(f)^*$ (come sopra),
			\item $f$ è simmetrico $\iff$ $f = f^\top$ $\iff$ $M_\basis(\varphi)\inv M_\basis(f)^\top M_\basis(\varphi) = M_\basis(f^\top) = M_\basis(f)$ $\iff$ $M_\basis(\varphi) M_\basis(f)$ è simmetrica,
			\item se $\basis$ è una base ortonormale, $f$ è simmetrico $\iff$ $f = f^\top$ $\iff$ $M_\basis(f) = M_\basis(f)^\top$ $\iff$ $M_\basis(f)$ è simmetrica,
			\item $f$ è hermitiano $\iff$ $f = f^*$ $\iff$ $M_\basis(\varphi)\inv M_\basis(f)^* M_\basis(\varphi) = M_\basis(f^*) = M_\basis(f)$ $\iff$ $M_\basis(\varphi) M_\basis(f)$ è hermitiana,
			\item se $\basis$ è una base ortonormale, $f$ è hermitiana $\iff$ $f = f^*$ $\iff$ $M_\basis(f) = M_\basis(f)^*$ $\iff$ $M_\basis(f)$ è hermitiana,
			\item $f$ è ortogonale $\iff$ $f \circ f^\top = f^\top \circ f = \Idv$,
			\item se $\basis$ è una base ortonormale, $f$ è ortogonale $\iff$ $M_\basis(f)$ è ortogonale,
			\item $f$ è unitario $\iff$ $f \circ f^* = f^* \circ f = \Idv$,
			\item se $\basis$ è una base ortonormale, $f$ è unitaria $\iff$ $M_\basis(f)$ è unitaria,
			\item $(f^\top)^\top = f$,
			\item $(f^*)^* = f$,
			\item $(\lambda f)^\top = \lambda f^\top$,
			\item $(\lambda f)^* = \conj{\lambda} f^*$,
			\item $(f + g)^\top = f^\top + g^\top$,
			\item $(f + g)^* = f^* + g^*$,
			\item $(f \circ g)^\top = g^\top \circ f^\top$,
			\item $(f \circ g)^* = g^* \circ f^*$,
			\item se $f$ è invertibile, $(f^\top)\inv = (f\inv)^\top$ (è sufficiente mostrare che $\varphi((f^\top \circ (f\inv)^\top)(\v), \w) = \varphi(\v, \w)$ e dedurre,
			sottraendo i due membri, che deve valere $f^\top \circ (f\inv)^\top = \Idv$),
			\item se $f$ è invertibile, $(f^*)\inv = (f\inv)^*$ (come sopra),
			\item $\Ker f^\top = (\Im f)^\perp$,
			\item $\Ker f^* = (\Im f)^\perp$,
			\item $\Im f^\top = (\Ker f)^\perp$,
			\item $\Im f^* = (\Ker f)^\perp$,
			\item se $W$ è un sottospazio di $V$, allora $W$ è $f$-invariante se e solo
			se $W^\perp$ è $f^\top$-invariante (o $f^*$-invariante),
			\item se $f$ è simmetrico (o hermitiano), allora $W$ è $f$-invariante se e solo se $W^\perp$ è $f$-invariante,
			\item l'operatore $\top \in \End(\End(V))$ è sempre diagonalizzabile
			e ha spettro $\{\pm 1\}$, dal momento che il suo polinomio minimo divide $x^2-1$ (infatti $(f^\top)^\top = f$),
			\item l'autospazio $V_1$ di $\top$ raccoglie gli operatori simmetrici, mentre
			$V_{-1}$ raccoglie gli operatori antisimmetrici.
		\end{itemize}

		\subsubsection{Operatori normali}
		
		Un operatore $f$ in uno spazio euclideo reale
		si dice normale se commuta col suo trasposto,
		ossia se $f \circ f^\top = f^\top \circ f$, mentre
		si dice normale in uno spazio euclideo complesso
		se commuta col suo aggiunto, ossia se $f \circ f^* = f^* \circ f$. \\ \vskip 0.05in
		
		Analogamente una matrice si dice
		normale se commuta con la sua trasposta (se è a elementi reali) o con la sua aggiunta (se è a elementi complessi). Una matrice contemporaneamente
		normale e triangolare è necessariamente una matrice
		diagonale. \\ \vskip 0.05in
		
		In uno spazio euclideo complesso, $f$ è normale $\iff$ $f$ è unitariamente diagonalizzabile ($f$ è triangolarizzabile con una base ortonormale $\basis$ tramite
		l'algoritmo di ortogonalizzazione di Gram-Schmidt, e quindi la matrice $M_\basis(f)$ è sia normale che
		triangolare, e quindi diagonale). In uno spazio
		euclideo reale, se $f$ è triangolarizzabile e
		normale, allora $f$ è diagonalizzabile (come prima).
		
		\subsection{Spazi euclidei reali e complessi}
		
		Si dice che $(V, \varphi)$ è uno spazio euclideo reale se $V$ è un $\RR$-spazio e se
		$\varphi$ è un prodotto scalare definito positivo. Si dice che $(V_\CC, \varphi_\CC)$ è uno spazio euclideo complesso se $V_\CC$ è un $\CC$-spazio e se $\varphi_\CC$ è un
		prodotto hermitiano definito positivo.
		
		Questi due tipi di spazi hanno in comune alcune proprietà particolari. Si definisce
		innanzitutto la norma euclidea per uno spazio euclideo $(V, \varphi)$ come:
		
		\[ \norm{\v} = \sqrt{q(\v)} = \sqrt{\varphi(\v, \v)}. \]
		
		Tale norma soddisfa alcune proprietà:
		
		\begin{itemize}
			\item $\norm{\lambda \v} = \abs{\lambda} \norm{\v}$,
			\item $\norm{\v} \norm{\w} \geq \abs{\varphi(\v, \w)}$ (disuguaglianza di Cauchy-Schwarz),
			\item $\norm{\v + \w} \leq \norm{\v} + \norm{\w}$ (disuguaglianza triangolare).
		\end{itemize}
		
		Su questi due spazi possono essere definiti due particolare operatori: la
		proiezione ortogonale e l'inversione ortogonale.
		
		Si definisce proiezione ortogonale su un sottospazio $W \neq \zerovecset$ l'operatore $\pr_W \in \End(V)$ tale
		che $\pr_W(\v) = \w$, dove $\v = \w + \w^\perp$, con $\w \in W$ e $\w^\perp \in W^\perp$. Tale decomposizione è ben definita e unica dacché $V = W \oplus^\perp W^\perp$ (infatti $\varphi$ è definita positiva). Una proiezione ortogonale
		soddisfa la relazione $\pr_W^2 = \pr_W$, da cui si ricava che $\varphi_{\pr_W} \mid x(x-1)$ (implicandone la diagonalizzabilità). Infatti $V_1 = \Ker(\pr_W - \Idv) = W$ e $V_0 = \Ker(\pr_W) = W^\perp$ (per cui $\varphi_{\pr_W}(x) = x(x-1)$). La
		proiezione ortogonale è un operatore simmetrico (se lo spazio è euclideo reale)
		o hermitiano (se lo spazio è euclideo complesso); infatti vale che
		$\varphi(\pr_W(\v), \w) = \varphi(\pr_W(\v), \pr_W(\w) + \pr_{W^\perp}(\w)) =
		\varphi(\pr_W(\v), \pr_W(\w)) = \varphi(\pr_W(\v) + \pr_{W^\perp}(\v), \pr_W(\w)) =
		\varphi(\v, \pr_W(\w))$.
		
		Si definisce inversione ortogonale su un sottospazio $W \neq \zerovecset$ l'operatore $\rho_W \in \End(V)$ tale
		che $\rho_W(\v) = \w - \w^\perp$, dove $\v = \w + \w^\perp$, con $\w \in W$ e $\w^\perp \in W^\perp$. Come prima, tale decomposizione è unica e ben definita. Un'inversione ortogonale
		soddisfa la relazione $\rho_W^2 = \Idv$, da cui si ricava che $\varphi_{\rho_W} \mid (x+1)(x-1)$ (implicandone la diagonalizzabilità). Infatti $V_1 = \Ker(\rho_W - \Idv) = W$ e $V_{-1} = \Ker(\rho_W + \Idv) = W^\perp$ (per cui $\varphi_{\rho_W}(x) = (x+1)(x-1)$). Se $\dim W = \dim V - 1$, allora si dice che l'inversione ortogonale
		è una riflessione ortogonale. L'inversione ortogonale è sempre un operatore
		ortogonale (se lo spazio euclideo è reale) o unitario (se lo spazio euclideo è
		complesso); infatti vale che $\varphi(\v, \w) = \varphi(\pr_W(\v) + \pr_{W^\perp}(\v), \pr_W(\w) + \pr_{W^\perp}(\w)) = \varphi(\pr_W(\v), \pr_W(\w)) + \varphi(\pr_{W^\perp}(\v), \pr_{W^\perp}(\w)) = \varphi(\pr_W(\v), \pr_W(\w)) + \varphi(-\pr_{W^\perp}(\v), -\pr_{W^\perp}(\w)) = \varphi(\pr_W(\v) - \pr_{W^\perp}(\v), \pr_W(\w) - \pr_{W^\perp}(\w)) = \varphi(\rho_W(\v), \rho_W(\w))$.

		\subsection{Teorema spettrale reale e complesso}
		
		Sia $(V, \varphi)$ uno spazio euclideo reale. Se $f$ è un operatore simmetrico, allora
		$f$ ammette solo autovalori reali. Prendendo infatti il prodotto hermitiano complessificato di $\varphi$, allora, se $\lambda$ è un autovalore in $\CC$ di $f$,
		$\conj{\lambda} \varphi(\v, \v) = \varphi(\lambda \v, \v)$ $= \varphi(f(\v), \v) = \varphi(\v, f(\v)) = \varphi(\v, \lambda \v) = \lambda \varphi(\v, \v)$, dove $\v$ è un autovettore non nullo relativo
		a $\lambda$; pertanto $\lambda = \conj{\lambda} \implies \lambda \in \RR$ dacché $\varphi(\v, \v) \neq 0$). Seguendo gli stessi passaggi algebrici si mostra
		che se $f$ è un operatore hermitiano uno spazio euclideo complesso, questo
		ammette solo autovalori reali. \\ \vskip 0.05in
		
		Se $f$ è simmetrico o hermitiano, allora $V_\lambda \perp V_\mu$ se $\lambda$ e
		$\mu$ sono due autovalori distinti. Infatti, se $\v$ è un autovettore relativo a $\lambda$ e $\w$ è un autovettore relativo a $\mu$, allora\footnote{$\lambda$ non è stato coniugato come argomento del prodotto dal momento che per il risultato precedente è reale, e quindi $\conj \lambda = \lambda$.} $\lambda \varphi(\v, \w) =
		\varphi(\lambda \v, \w) = \varphi(\v, \mu \w) = \mu \varphi(\v, \w)$; poiché
		$\lambda \neq \mu$ deve allora per forza valere $\varphi(\v, \w) = 0$.
		
		Se $f$ è simmetrico o hermitiano, esiste sempre una
		base ortonormale di autovettori (\textit{teorema spettrale}). Se così non fosse, detto $W = V_{\lambda_1} \oplus^\perp \cdots \oplus^\perp V_{\lambda_k}$, $W^\perp$ sarebbe $f$-invariante e simmetrico/hermitiano, e dunque ammetterebbe un autovalore reale, contrariamente a quanto ipotizzato, \Lightning. Alternativamente, poiché
		$f$ è simmetrico (e in tal caso anche perché il
		polinomio caratteristico è completamente fattorizzabile in $\RR$) o hermitiano, $f$ è anche
		normale, ed è dunque diagonalizzabile; allora, poiché gli autospazi sono in somma diretta ortogonale, $f$ è anche ortogonalmente o unitariamente diagonalizzabile. \\ \vskip 0.05in
		
		In termini matriciali, se $A$ è una matrice
		simmetrica a elementi reali (o hermitiana a elementi complessi), esiste una matrice $O \in O(n)$ (o $U \in U(n)$) tale per cui $O^\top A O$ (o $U \in U(n)$) è diagonale. Infatti $f_A$, l'operatore
		indotto da $A$ nella base ortonormale di $\RR^n$ (o $\CC^n$), è un operatore simmetrico (o hermitiano) rispetto al prodotto standard dello
		spazio euclideo che si sta studiando. \\
		
		Una matrice reale è simmetrica se e solo se è ortogonalmente diagonalizzabile.
		Una matrice complessa è hermitiana se e solo se è unitariamente diagonalizzabile
		con autovalori reali.
		
		\subsubsection{Radice quadrata di una matrice simmetrica, decomposizione polare e simultanea ortogonalizzabilità}
		
		Se $A \in \Sym(n, \RR)$ è semidefinita positiva, allora
		esiste sempre una matrice $S \in \Sym(n, \RR)$ tale
		per cui $S^2 = A$. Se si suppone anche che $S$ è
		semidefinita positiva, tale matrice diventa unica e
		viene detta \textit{radice quadrata} di $A$, indicata come $\sqrt{A}$. \\
		
		Per costruire tale radice quadrata è sufficiente
		considerare $P \in O(n)$ tale per cui
		$P^\top A P = D$, dove $D \in M(n, \RR)$ è
		diagonale, secondo il teorema spettrale. Poiché $A$ è semidefinita positiva, $D$
		si compone di soli elementi non negativi, ed è
		dunque possibile costruire la matrice $\sqrt{D} \in M(n, \RR)$ dove $\sqrt{D}_{ii} = \sqrt{D_{ii}}$ (da cui si deduce che $\sqrt{D}^2 = D$ e che $\sqrt{D}$ è esattamente la radice quadrata di $D$).
		Si consideri dunque $S = P \sqrt{D} P^\top$; vale
		che $S^2 = P D P^\top = A$, e dunque $S$ è la
		radice quadrata $\sqrt{A}$ di $A$ (per dimostrare l'unicità di tale matrice è sufficiente ridursi all'uguaglianza negli autospazi). Si osserva che
		se $A$ è definita positiva, anche $S$ lo è. \\ \vskip 0.05in
		
		Se $A \in M(n, \RR)$ esistono e sono uniche le
		matrici $P \in O(n)$, $S \in \Sym(n, \RR)$, con $S$ semidefinita positiva, tali per
		cui $A = PS$. In particolare vale che $S = \sqrt{A A^\top}$; se dunque $A \in \GL(n, \RR)$, $S$ è
		definita positiva (in tal caso $\Ker A^\top A = \Ker A = \zerovecset$ -- come visto nella sezione sulle matrici --, e dunque $\vec x^\top A^\top A \vec x = \innprod{A \vec x, A \vec x} > 0$ $\implies A^\top A > 0 \implies S > 0$).
		
		Se $A \in \GL(n, \RR)$, esistono e sono unici $P \in O(n)$,
		$S \in \Sym(n, \RR)$ tali per cui $A = PS$ (in particolare $S = \sqrt{A A^\top}$).
		
		Due prodotti $\varphi$, $\psi$ si dicono simultaneamente ortogonalizzabili se esiste una
		base $\basis$ tale per cui sia che $M_\basis(\varphi)$ che $M_\basis(\psi)$ sono
		diagonali (ossia se esiste una base ortogonale per
		entrambi i prodotti). \\ \vskip 0.05in

		Se $\KK=\RR$ o $\KK=\CC$, $\varphi$ è definito positivo, allora
		i due prodotti $\varphi$ e $\psi$ sono sempre simultaneamente ortogonalizzabili. È sufficiente infatti prendere
		una base $\basis$ ortonormale di $\varphi$, e trovare la base ortonormale $\basis'$ di autovettori
		che rende $M_\basis(\psi)$ diagonale. In tale base $\basis'$, $M_{\basis'}(\varphi)$
		è l'identità e $M_\basis(\psi)$ è diagonale: dunque la base è ortogonale per ambo
		i prodotti.
		
		\subsection{Azioni di gruppo}
		Sia $G$ un gruppo e $X$ un insieme. Un'azione sinistra\footnote{Un'azione sinistra induce sempre anche un'azione destra, ponendo $x \cdot g=g^{-1} \cdot x$.} di $G$ su $X$ a sinistra un'applicazione $\cdot : G \times X \rightarrow X$, per la quale si pone $g \cdot x := \cdot(g, x)$, tale che:

		\begin{enumerate}[(i)]
			\item $e \cdot x=x$, $\forall x \in X$, dove $e$ è l'identità di $G$,
			\item $g \cdot (h \cdot x)=(gh) \cdot x$, $\forall g, h \in G$, $\forall x \in X$.
		\end{enumerate}

		Si definisce l'applicazione $f_g:X\rightarrow X$ indotta dalla relazione $f_g(x)=g \cdot x$; tale applicazione è bigettiva. Se $\cdot$ è un'azione sinistra di $G$ su $X$, si dice che $G$ opera a sinistra su $X$ e che $X$ è un $G$-insieme. \\
		
		\vskip 0.05in
		
		Si definisce \textit{stabilizzatore} di $x \in X$ il sottogruppo di $G$ $\Stab_G(x)$
		tale che:
		\[ \Stab_G(x)= \{g\in G \mid g \cdot x=x\}, \]
		dove si scrive $\Stab(x)$ per indicare $\Stab_G(x)$ qualora non fosse ambigua
		l'azione a cui ci si riferisce. \\

		Si può costruire un omomorfismo $\tau : G \rightarrow S_X$, dove $(S_x, \circ)$ è il gruppo delle bigezioni di $X$, dove $\tau(g) = f_g$. Si dice che l'azione di $G$ su $X$ è \textit{fedele} se l'omomorfismo $g \rightarrow f_g$ è iniettivo, ossia
		se e solo se:
		\[ f_g = \IdV{X} \implies g = e, \]
		
		ossia se e solo se:
		\[ \bigcap_{x \in X} \Stab(x) = \{e\}. \]
		
		Per esempio, $S_X$ opera fedelmente su $X$ tramite l'azione indotta dalla relazione $g \cdot x=g(x)$ (ed è in realtà anche un'azione transitiva). $G$ stesso opera su $G$
		tramite l'azione banale indotta dalla relazione $g \cdot g'=gg'$.
		
		Si definisce su $X$ la relazione $x \sim_G y \iff \exists g \in G$ t.c.~$y=g \cdot x$.
		La relazione $\sim_G$ è una relazione d'equivalenza: due elementi equivalenti tramite $\sim_G$ si dicono coniugati tramite $G$. Le classi di equivalenza si dicono orbite di $G$. In particolare si definisce $\Orb(x) = O_x$, con $x \in X$, come $[x]_{\sim_G}$,
		ossia come la classe di equivalenza di $x$ rispetto a $\sim_G$.
		
		Si presentano alcuni esempi di orbite:
		\begin{enumerate}
			\item $\GL(n,\KK)$ opera su $M(n,\KK)$ tramite la similitudine e le orbite sono le classi di matrici simili, rappresentate dalle forme canoniche di Jordan,
			\item $\GL(n,\KK)$ opera su $\Sym(n,\KK)$ tramite la congruenza e le orbite sono le classi di matrici congruenti, rappresentate in $\RR$ dalle matrici diagonali con $1$, $-1$ e $0$ come elementi, e in $\CC$ dalle stesse matrici rappresentanti delle classi
			di equivalenza della SD-equivalenza, ossia le matrici del tipo $I_r^{m \times n}$,
			\item $O_n$ agisce naturalmente su $\RR^n$ e l'orbita di $\vec x \in \RR^n$ è la sfera di raggio $\norm{\vec x}$ secondo il prodotto scalare standard di $\RR^n$. 
		\end{enumerate}
		
		Vale il teorema di orbita-stabilizzatore: l'applicazione $f:G/\Stab_G(x) \rightarrow \Orb(x)$ tale che $g \Stab_G(x) \mapsto g \cdot x$ è una bigezione tra
		$G/\Stab_G(x)$ e $\Orb(x)$ (tale teorema è un analogo del primo teorema di
		omomorfismo per i gruppi). Se $G$ è finito, vale allora che $\abs{G} = \abs{\Stab_G(x)} \cdot \abs{\Orb(x)}$.
		
		Si dice che $G$ opera \textit{liberamente} su $X$ se $\forall x \in X$ l'applicazione
		da $G$ in $X$ tale che $g \mapsto g \cdot x$ è iniettiva, ossia se e solo se $\Stab_G(x)=\{e\}$, $\forall x \in X$. Se $G$ opera liberamente su $X$,
		$G$ opera anche fedelmente su $X$.
		
		Si dice che $G$ opera \textit{transitivamente }su $X$ se $x \sim_G y$, $\forall x,y \in X$, cioè se esiste un'unica orbita, che coincide dunque con $G$. In tal caso
		si dice che $X$ è \textit{omogeneo} per l'azione di $G$, oppure che
		$X$ è $G$-omogeneo.
		
		Si presentano alcuni esempi di azioni transitive:
		\begin{enumerate}
			\item $O_n$ opera transitivamente sulla sfera $n$-dimensionale di $\RR^n$,
			\item Sia $\Gr_k(\RR^n)=\{W \text{ sottospazio di } \RR^n | \dim W=k\}$ la Grassmanniana di ordine $k$ su $\RR^n$. Allora $O_n$ opera transitivamente su $\Gr_k(\RR^n)$.
		\end{enumerate}
		
		Si dice che $G$ opera in maniera \textit{semplicemente transitiva} su $X$ se opera transitivamente e liberamente su $X$; in tal caso si dice che $X$ è un insieme $G$-omogeneo principale. Equivalentemente $G$ opera in maniera semplicemente transitiva se $\exists x\in X$ t.c.~$g\rightarrow g \cdot x$ è una bigezione.
		
		Se $X$ è un insieme $G$-omogeneo e $G$ è abeliano, allora $G$ agisce fedelmente su $X$ $\iff$ $X$ è $G$-insieme omogeneo principale (per dimostrare l'implicazione a destra è sufficiente mostrare che, se $x \in X$, $g \in \Stab(x) \implies f_g = \IdV{X}$, da cui si conclude
		che $g = e$ per la fedeltà dell'azione).
		
		\subsection{Proprietà generali di uno spazio affine}
		Si dice spazio affine $E$ un qualunque insieme $V$-omogeneo principale, dove
		$V$ è uno spazio vettoriale, inteso in tal senso come il gruppo abeliano
		$(V, +)$. Si scrive in tal caso l'azione $\v \cdot P$ come $P + \v$. Equivalentemente $E$ è uno spazio affine se $\forall P$, $Q \in E$, $\exists! \, \v\in V$ t.c. $P + \v = Q$. In particolar modo, ci si riferisce a $\v \mid P + \v = Q$ come $Q - P$ o
		$\overrightarrow{PQ}$.
		
		Valgono le seguenti proprietà generali:
		\begin{itemize}
			\item fissato $\v \in V$, l'applicazione da $E$ in $E$ tale che $P \mapsto P+\v$ è una bigezione,
			\item fissato $O \in E$, l'applicazione da $V$ in $E$ tale che $\v \mapsto O+\v$ è una bigezione,
			\item fissato $O \in E$, l'applicazione da $E$ in $V$ tale che $P \mapsto P-O$ è una bigezione ed è l'inversa della bigezione presentata nello scorso punto.
		\end{itemize}
		
		Siano $P_1$, ..., $P_k \in E$ e $\lambda_1$, ..., $\lambda_k \in \KK$. Siano inoltre
		$O$, $O' \in E$. Allora se si pone $P=O+\sum_{i=1}^{k}\lambda_i (P_i-O)$ e $P'=O'+\sum_{i=1}^{k}\lambda_i (P_i-O')$, vale che:
		\[P=P' \, \forall O, O' \in E \iff\sum_{i=1}^{k}\lambda_i=1\]
		
		Pertanto un punto $P\in E$ si dice \textit{combinazione affine} dei punti $P_1$, ..., $P_k$ se $\exists \lambda_1$, ..., $\lambda_k \in \KK$ tali che $\sum_{i=1}^{k}\lambda_i=1$ e che $\forall O \in E$,
		$P=O+\sum_{i=1}^{k}\lambda_i (P_i-O)$. Si scrive in tal caso $P=\sum_{i=1}^{k}\lambda_i P_i$ (la notazione è ben definita dal momento che
		non dipende da $O$ per l'asserzione precedente).
		
		Un sottoinsieme $D\subseteq E$ si dice \textit{sottospazio affine} se è chiuso per combinazioni affini. Il sottospazio affine $D \subseteq E$ generato da un sottoinsieme $S \subseteq E$ è l'insieme delle combinazioni affini (finite) dei punti di $S$;
		si denota tale sottospazio affine $D$ come $\Aff(S)$. Vale inoltre che $\Aff(S)$ è il
		più piccolo sottospazio affine contenente $S$.
		        
        Ogni spazio vettoriale $V$ su $\KK$ induce uno spazio affine tramite l'azione banale che compie $(V, +)$ su $(V, +)$, ossia con $\v \cdot \w=\v+\w=\w+\v$, dove l'operazione $+$ coincide sia con la somma affine che
        con quella vettoriale.
        In questo caso una combinazione affine diventa un caso particolare di combinazione lineare. Lo spazio affine
        generato in questo modo su $\KK^n$ viene detto \textit{spazio affine standard} ed è indicato come $\AnK$. \\ \vskip 0.05in
        
        Se $E$ è uno spazio affine sul $\KK$-spazio $V$, allora ogni scelta di un punto $O \in E$ e di una base $\mathcal{B}$ di $V$ induce la bigezione naturale 
        $\varphi_{O,\mathcal{B}} : E \to \AnK$ tale che $\varphi_{O,\mathcal{B}}(P)=[P-O]_\basis$, dove $P \in E$.

        Un sottoinsieme $D \subseteq E$ è un sottospazio affine $\iff \forall P_0 \in D$,
        $D_0=\{P-P_0 \mid P\in D\}\subseteq V$ è un sottospazio vettoriale di $V$.
        Si può allora scrivere che $D=P_0+D_0$, ossia si deduce che $D$ è il traslato di $D_0$ per $P_0$, e quindi
        che ogni sottospazio affine è in particolare il traslato
        di un sottospazio vettoriale.
        L'insieme $D_0$, scritto anche come $\Giac(D)$, è detto \textit{direzione} (o \textit{giacitura}) del sottospazio affine $D$ ed è invariante per la scelta
        del punto $P_0$; in particolare vale che $D_0 = \{ Q - P \mid P, Q \in D \}$.

        Si definisce la dimensione di un sottospazio affine $D$ come la dimensione della sua direzione $D_0$. In particolare $\dim E = \dim V$. Quindi, così come accade per gli spazi vettoriali, i sottospazi affini di dimensione nulla corrispondono ai punti di $E$, quelli di dimensione unitaria corrispondono alle \textit{rette} di $E$, quelli di dimensione $2$ corrispondono ai \textit{piani}, mentre quelli di codimensione unitaria (ossia di dimensione $\dim V - 1$) corrispondono agli \textit{iperpiani affini}.

        Due sottospazi affini con la stessa direzione si
        dicono \textit{paralleli} se sono distinti, o \textit{coincidenti} se sono uguali. Due sottospazi
        affini paralleli hanno sempre intersezione vuota e si ottengono l'uno dall'altro mediante traslazione.

	    Dei punti $P_1$, ..., $P_k \in E$ si dicono \textit{affinemente indipendenti} se per
	    $P \in \Aff(P_1, \ldots, P_k)$ esistono unici
	    $\lambda_1$, ..., $\lambda_k$ tali per cui
	    $P = \sum_{i=1}^k \lambda_i P_i$ è una combinazione
	    affine. Un sottoinsieme $S \subseteq E$ si dice affinemente indipendente se ogni suo sottoinsieme finito è affinemente indipendente.

        I punti $P_1$, ..., $P_k$ sono affinemente indipendenti se e solo se $\forall i=1\text{---}k$ i vettori $P_j-P_i$ con $j \neq i$ sono linearmente indipendenti in $V$ $\iff \forall i=1\text{---}k$, $P_i \notin \Aff(S \setminus \{P_i\})$,
        dove $S = \{P_1, \ldots, P_k\}$. Pertanto, possono
        esistere al più $\dim D_0 + 1$ punti affinemente
        indipendenti in $D$. In particolare, se si scelgono
        $n+1$ punti $P_0$, ..., $P_n \in E$ affinemente
        indipendenti, vale che $\Aff(P_0, \ldots, P_n) = E$ (in tal caso infatti la direzione sarebbe tutto $V$).
        Esistono sempre $P_0$, ..., $P_n$ punti di $D$ tali
        che $\Aff(P_0, \ldots, P_n) = D$, se $\dim D = n$;
        in tal caso l'insieme di questi punti viene detto
        \textit{riferimento affine}. Ogni riferimento affine ha
        lo stesso numero di elementi (in generale valgono
        le stesse proprietà di una base vettoriale, mediante
        cui se ne dimostra l'esistenza).

        Sia $E = \AnK$ allora $\ww 1$, ..., $\ww n \in E$ sono affinemente indipendenti se e solo se i vettori $\hat{\ww 1}$, ..., $\hat{\ww n}$ con $\hat{\ww i}=\Matrix{\ww i \\[0.03in] \hline 1} = \iota(\ww i) \in \KK^{n+1}$ sono linearmente indipendenti. \\ \vskip 0.05in

		Siano $P_0$, ..., $P_k$ i punti di un riferimento
		affine per il sottospazio affine $D$. Allora ogni
		punto $P \in D$ è univocamente determinato dagli
		scalari $\lambda_i$ in $\KK$ tali per cui $P = \sum_{i=0}^k \lambda_i P_i$, eccetto per uno di questi scalari che è già determinato dagli altri (infatti vale sempre $\sum_{i=0}^k \lambda_i = 1$). Vi è dunque una bigezione tra $D$ e $\mathcal{A}_k(\KK)$. L'immagine di $P$
		tramite questa bigezione è un vettore contenente
		le cosiddette \textit{coordinate affini} di $P$.

		Si dice \textit{combinazione convessa} una qualsiasi
		combinazione affine finita in un insieme di punti affinemente indipendenti $S$ in cui ogni coordinata affine è maggiore o
		uguale a zero. Si pone in particolare $\IC(S)$ come
		l'insieme di questo tipo di combinazioni (intuitivamente un inviluppo convesso è l'insieme dei punti contenuti "dentro" il riferimento affine scelto; per tre punti è il triangolo, per due punti è il segmento). Si scrive $\IC(P_1, \ldots, P_k)$ per indicare $\IC(\{P_1, \ldots, P_k\})$. \\ \vskip 0.05in 
		
		Si osserva che $\IC(S)$ è un insieme
		convesso (ossia $\forall P, Q \in \IC(S)$, $[P, Q] \subseteq \IC(S)$, dove $[P, Q] := \IC(\{P, Q\})$ è il segmento congiungente $P$ e $Q$).

        Si definisce il \textit{baricentro geometrico} di $P_1$, ..., $P_n\in E$ come la seguente combinazione convessa: 
        \[ G=\frac{1}{n}\sum_{i=1}^{n}P_i \in \IC(P_1, \ldots, P_n). \]
        Se $A \subseteq E$ è finito, si definisce $G_A$ come il baricentro geometrico dei punti di $A$. Inoltre,
        se $A$ è un'unione di insiemi disgiunti, $G_A$ è
        una combinazione convessa dei baricentri di questi insiemi con peso la loro cardinalità divisa per la
        cardinalità di $A$; in altre parole se $A=B \sqcup C$ (i.e.~$A = B \cup C \land B \cap C = \emptyset$), allora:
        \[ G_A = \frac{\abs B}{\abs A} G_B + \frac{\abs C}{\abs A} G_C. \]
     	In questo modo si dimostra facilmente che in un triangolo il baricentro geometrico giace sulle
     	congiungenti dei punti medi con i vertici opposti.

		Si osserva che se $A$ e $B$ sono due sottospazi affini, allora anche
		$A \cap B$ è un sottospazio affine se $A \cap B \neq \emptyset$. Inoltre, se $A \cap B \neq \emptyset$,
		allora $(A \cap B)_0 = A_0 \cap B_0$. \\ \vskip 0.05in
		
		Si definisce \textit{somma affine} $A + B$ di due sottospazi affini $A$ e $B$ di $E$
		il sottospazio affine $\Aff(A \cup B)$. In generale vale la seguente
		uguaglianza:
		\[ (A + B)_0 = A_0 + B_0 + \Span(P_0' - P_0), \quad P_0' \in A, P_0 \in B. \]
		Inoltre $\Span(P_0' - P_0) \subseteq A_0 + B_0 \iff A \cap B \neq \emptyset$,
		altrimenti $(A + B)_0 = A_0 + B_0 \oplus \Span(P_0' - P_0)$. Pertanto, se $A \cap B \neq \emptyset$, continua a valere la formula di Grassmann:
		\[ \dim (A + B) = \dim A + \dim B - \dim (A \cap B) \quad \se A \cap B \neq \emptyset, \]
		altrimenti vale la formula di Grassmann modificata:
		\[ \dim (A + B) = \dim A + \dim B - \dim (A \cap B) + 1 \quad \se A \cap B = \emptyset. \]

        \subsection{Applicazioni affini e affinità}
        Siano $E$ spazio affine su $V$, $E'$ spazio affine su $V'$ sullo stesso campo $\KK$.
        
        Un'applicazione $f:E\rightarrow E'$ si dice \textit{applicazione affine} se conserva le combinazioni affini, ossia se:
        \[ f\left( \sum_{i=1}^k \lambda_i P_i \right) = \sum_{i=1}^k \lambda_i f(P_i) \impliedby \sum_{i=1}^k \lambda_i = 1. \]

        Se $D$ è un sottospazio affine di $E$ $f$-invariante,
        allora $\restr{f}{D}$ è ancora un'applicazione affine.
        Esiste ed è unica l'applicazione lineare $g : V \rightarrow V'$ tale che $f(P)=f(O)+g(P-O)$ per ogni scelta di $P$, $O \in E$; tale applicazione lineare
        si denota con $df$ e viene detta \textit{differenziale} di $g$. Analogamente si può sempre costruire un'applicazione affine tale per cui $df=g$, data
        $g \in \mathcal{L}(V, V')$.

        Nel caso in cui $E=\mathcal{A}_n(\KK)$, $E'=\mathcal{A}_m(\KK)$, un'applicazione affine $f$ è della forma $f(\x)=f(\Vec{0})+g(\x)=A \x+\Vec{b}$ con $A\in M(m,n,\KK)$ e $\Vec{b} \in A_m(\KK)$. \\ \vskip 0.05in

        Sia $E''$ un altro spazio affine associato a $V''$. Se $f':E'\rightarrow E''$ è affine, allora $f'\circ f:E\rightarrow E''$ è affine e vale $d(f' \circ f) = df' \circ df$.

        Si dice che $f : E \rightarrow E$ è un'\textit{affinità} di $E$ se $f$ è un'applicazione affine bigettiva; si definisce $A(E)$ come il gruppo
        delle affinità di $E$ rispetto alla composizione. Vale
        in particolare che $f$ è un'affinità di $E$ $\iff$ $df$ è invertibile.

        L'applicazione $\pi : A(E)\rightarrow \GL(V) : f  \mapsto g$ è un omomorfismo surgettivo il cui nucleo è dato dalle traslazioni, che pertanto formano un sottogruppo normale.

        Sia $f \in A(E)$. Allora $d(f\inv) = df\inv$; in particolare, se $E = \AnK$, $f\inv(\vec x) = A\inv \vec x - A\inv \vec b$, dove $f(\vec x) = A \vec x + \vec b$.

        Si definisce l'applicazione affine $\iota : \AnK \to 
        \mathcal{A}_{n+1}(\KK)$ in modo tale che $\x \mapsto \hat{\x}=\Matrix{\x \\[0.03in] \hline 1}$. Si osserva che $\iota$
        è un isomorfismo affine tra $\AnK$ e l'iperpiano $H_{n+1}=\{\x\in \mathcal{A}_{n+1}(\KK)\mid$ $ x_{n+1}=1\}\subset \mathcal{A}_{n+1}(\KK)$.

        Sia $f \in A(\AnK)$ data da $f(\x)=A\x+\Vec{b}$.
        Allora esiste un'unica applicazione lineare $\hat f \in \End(\KK^{n+1})$ che estende $f$ in $\mathcal{A}_{n+1}(\KK)$ tale per cui
        $f(\iota(\x)) = \iota(f(\x))$; in particolare tale
        applicazione è rappresentata dalla matrice $\hat A$, dove:
        \[ \hat{A}=\Matrix{ A & \rvline & \vec b \, \\ \hline 0 & \rvline & 1 \, }. \]
        In particolare $\hat A$ dipende da $n^2 + n$ parametri; se si fosse posto $f(D) = D$, sarebbe dipesa invece da $k(k+1) + n(n-k)$ parametri. Le matrici di questa forma formano un sottogruppo di $\GL_{n+1}(\KK)$ isomorfo ad $A(\AnK)$.

        Sia $E$ spazio affine di dimensione $n$.
        
        \begin{enumerate}[(i)]
            \item se $f\in A(E)$ e i punti $P_0$, ..., $P_n \in E$ sono affinemente indipendenti, allora anche i punti $f(P_0)$, ..., $f(P_n)$ sono affinemente indipendenti,
            \item se $\dim E_0 = n$, i punti $P_0$, ..., $P_n$ sono affinemente indipendenti e anche i punti $Q_0$, ... $Q_n$ sono affinemente indipendenti, allora esiste ed è unica l'affinità $f : E \rightarrow E$ tale che $f(P_i)=Q_i \forall i=1\text{---}n$,
            \item se $f\in A(E)$, $D \subseteq E$ sottospazio affine $\implies f(D)$ è un sottospazio affine della stessa dimensione.
        \end{enumerate}
        
        Siano ($P_1$, $P_2$, $P_3$), ($Q_1$, $Q_2$, $Q_3$) due terne di punti distinti di $\mathcal{A}_1(\KK)$. Allora esiste ed è unica l'applicazione affine $f \in A(\mathcal{A}_1(\KK))$ tale che $f(P_i)=Q_i$ $\forall i=1,$ $2$, $3$ $\iff \lambda(P_1, P_2, P_3)=\lambda(Q_1, Q_2, Q_3)$, dove $\lambda(P_1,P_2,P_3)$ è detto \textit{rapporto semplice} ed è definito come:
        \[ \lambda(P_1, P_2, P_3) = \frac{P_3 - P_1}{P_2 - P_1}. \]
        Infatti $f$ è già unica ponendo $f(P_1) = Q_1$ e $f(P_2) = Q_2$; allora, poiché
        $\mathcal{A}_1(\KK)$ è di dimensione unitaria, $P_3$ deve scriversi come combinazione
        affine di $P_1$ e $P_2$ in modo tale che $P_3 = P_1 + \lambda (P_2 - P_1)$. In questo
        modo, poiché $f(P_3) = Q_3$, anche $Q_3 = Q_1 + \lambda (Q_2 - Q_1)$, da cui la
        motivazione dietro all'uguaglianza dei rapporti semplici.

		\begin{itemize}
			\item $A(\AA_1(\KK))$ agisce transitivamente su $\mathcal{A}_1(\KK)$,
			$\Stab(x_0)=\{f\mid f(x_0)=x_0\} \cong \GL_1(\KK)$ (infatti la matrice associata all'affinità dipende da un solo parametro),
			\item $\abs{\Fix(f)} \leq 1$, e
			$\abs{\Fix(f)}=0 \iff f$ è una traslazione, dove $\Fix(f) = \{x\in \mathcal{A}_1(\KK)\mid f(x)=x\}$,
			\item $A(\mathcal{A}_1(\KK))$ agisce in maniera semplicemente transitiva sulle coppie di punti $(P_1,P_2) \in \AA_1(\KK) \times \AA_1(\KK)$ con $P_1\neq P_2$.
		\end{itemize}
		
		Sia $f(\x) = M\x + \vec t$ un'affinità di $A(\AnK)$. Allora, se $1$ non è un autovalore
		di $M$, $f$ ha un unico punto fisso (in tal caso, infatti $(M-I)$ è invertibile, e quindi $(M-I)\x = -\vec t$ ammette un'unica soluzione).
        
        \subsection{Spazio proiettivo}
        Si definisce \textit{spazio proiettivo} relativo a $\KK^{n+1}$ l'insieme delle rette di $\KK^{n+1}$. Tale spazio viene denotato come $\PP(\KK^{n+1})=\PP^n(\KK)$ (intuitivamente lo spazio proiettivo perde una dimensione rispetto allo spazio di partenza perché è la proiezione di tutte le rette in un unico punto, eccetto per i punti all'infinito).
        
        Equivalentemente lo spazio proiettivo è l'insieme quoziente di $\KK^{n+1}$ tramite
        la relazione di equivalenza $\sim$ dove $\vec x \sim \vec y \defiff \exists \lambda \in \KK, \lambda \neq 0 \mid \vec x = \lambda \vec y$.  

        Ogni punto $\vec x \in \KK^n$ individua un unico sottospazio di dimensione unitaria in $\KK^{n+1}$ tramite $\iota$, ossia: $\Span(\iota(\vec x)) = \Span(\projT\x)$. L'insieme di rette non individuate tramite elementi di $\KK^n$ è in particolare formato dalle rette appartenenti al piano $\{\x \in \KK^{n+1} \mid x_{n+1}=0\}\cong \KK^n$; dal momento che queste rette si identificano come tutte le rette di $\KK^n$, esse rappresentano in particolare lo spazio proiettivo di una dimensione ancora minore, $\PP^{n-1}(\KK)$.

        Le rette appartenenti al piano $\{\x \in \KK^{n+1} \mid x_{n+1}=0\}$ sono dette \textit{punti all'infinito} di $\PP^n(\KK)$ (intuitivamente un punto all'infinito indica la direzione dei vari infiniti del piano).

        Si può ricoprire $\PP^n(\KK)$ con gli iperpiani $H_i=\{\x\in \mathcal{A}_{n+1}(\KK)\mid x_{i}=1\}$ dal momento che ogni retta deve intersecare almeno uno di questi iperpiani in un punto.
		
		\subsection{Coniche e quadriche}
		
		\setlength{\extrarowheight}{4pt}
	
		Si definisce \textit{quadrica} il luogo di zeri in $\AA_n(\KK)$ di un polinomio
		di secondo grado $p(x_1, \ldots, x_n) \in \KK[x_1, \ldots, x_n]$ in $n$ variabili, dove si identifica con
		una $n$-upla $(x_1, \ldots, x_n)$ di variabili che sono zeri del polinomio
		un elemento di $\AA_n(\KK)$ con le stesse coordinate.
        Una \textit{conica} è in particolare una quadrica in due variabili. Si osserva che
        una quadrica è invariante per la moltiplicazione per uno scalare diverso
        da $0$.

        D'ora in poi si intenderà con $\x$ l'$n$-upla $(x_1, \ldots, x_n)$ e si supporrà
        $\Char \KK \neq 2$. Il polinomio $p(x_1, \ldots, x_n)$ può essere descritto come:
        \[ p(x_1, \ldots, x_n) = \sum_{i=1}^n a_{ii} x_i^2 + \sum_{1 \leq i<j\leq n} 2 a_{ij} x_i x_y + \sum_{i=1}^n 2 b_i x_i + c. \]
        
        Si definiscono le seguenti quantità:
        
        \begin{itemize}
        	\item la \textit{parte quadratica} $\AA(p) \in \Sym(n, \KK)$ di $p$, dove:
        	\[ \AA(p)_{ij} = \begin{cases}
        		a_{ij} & \se i \leq j, \\
        		a_{ji} & \se i > j.
        	\end{cases} \]
        	\item la \textit{parte lineare} $\Ll(p) \in M(n, 1, \KK)$ di $p$, dove $\Ll(p)_{i1} = b_i$,
        	\item il \textit{termine noto} $c(p) \in \KK$ di $p$, dove $c(p) = c$.
        \end{itemize}
        Allora il polinomio può essere riscritto come:
        \[ p(\x) = \x^\top \AA(p) \, \x + 2 \Ll(p)^\top \x + c(p). \]

        Si definisce inoltre la matrice $\MM(p)$, dove:
        \[ \MM(p) =\Matrix{\AA(p) & \rvline & \Ll(p) \, \\ \hline \Ll(p) & \rvline & c(p) } \in \Sym(n+1, \KK),\] la quale viene detta \textit{matrice associata alla quadrica} in $p$. Si dice che la quadrica relativa a $p$ è \textit{non degenere} se $\rg(\MM(p)) = n+1$. Allora, tramite l'identificazione di $\Aa_n(\KK)$ in $H_{n+1} \subset \KK^{n+1}$ mediante $\iota$, vale che:
        \[ p(\x) = \hat \x ^\top \MM(p) \hat x, \quad \dove \hat \x = \iota(\vec x) = \projT{\x}. \]
        
        Si deduce allora che una quadrica altro non è che la controimmagine tramite $\iota$
        dell'intersezione tra $H_{n+1}$ e il cono isotropo $\CI(\varphi_{\MM(p)})$, dove
        $\varphi_{\MM(p)}$ è il prodotto scalare indotto da $\MM(p)$ in $\KK^{n+1}$ (i.e.~la quadrica è esattamente $\iota\inv(H_{n+1} \cap \CI(\varphi_{\MM(p)}))$).

        Nel caso di una conica determinata dal polinomio $p(x,y)=$ $ax^2+by^2+$ $cxy+dx+ey+f$, vale
        che:

        \[ \MM(p) = \Matrix{\NMatrix{a & \nicefrac{c}{2} \\ \nicefrac{c}{2} & b} & \rvline & \NMatrix{\nicefrac{d}{2} \\ \nicefrac{e}{2}} \, \\ \hline \NMatrix{\nicefrac{d}{2} & \nicefrac{e}{2}} & \rvline & f }. \]

        Sia $f\in A(\AnK)$ tale per cui $f(\x)=M\x + \vec t$, con $M\in \GL(n, \KK), \vec t \in \KK^n$. Si definisce allora un'azione destra di $A(\AnK)$ sulle quadriche di
        $n$ variabili, dove $p \cdot f$ è indicato come $p \circ f$, che a sua volta
        indica il polinomio $p(f(\x)) = p(M\x + \vec t)$. Il luogo di zeri $Z(p)$ di una quadrica
        su cui agisce $A(\AnK)$ varia a sua volta secondo l'affinità; in particolare vale che:
        \[ Z(p) = f(Z(p \circ f)). \]
        Si definisce allora una relazione di equivalenza detta \textit{equivalenza affine},
        dove:
        \[ p \sim q \defiff \exists \lambda \in \KK^*, f \in A(\AnK) \mid p = \lambda (q \circ f). \]
        Vale inoltre la seguente identità:
        \[p(f(\x))=\x^{\top} A' \x+2\vec{b'}^\top\x+c',\]

        dove $\AA'=M^\top \AA M$, $\vec{b'}= M^\top(\AA\Vec{t}+\Vec{b})$ e $c'=p(\vec{t})$. Pertanto la matrice $\MM(p \circ f)$ può essere scritta come:

		\begin{gather*}
			\MM(p \circ f) = {\hat M}^\top \MM(p) \hat M = \\
			 \Matrix{M^\top \AA(p) M & \rvline & M^\top(\AA(p) \vec t + \Ll(p)) \, \\ \hline \, \left(M^\top(\AA(p) \vec t + \Ll(p))\right)^\top & \rvline & p(\vec t)}.
		\end{gather*}
		
		Se $f$ è solo una traslazione (i.e.~$M = I_n$), la formula si semplifica:
		\begin{gather*}
			\MM(p \circ f) = {\hat M}^\top \MM(p) \hat M = \\
			\Matrix{\AA(p) & \rvline & \AA(p) \vec t + \Ll(p) \, \\ \hline \, 	\left(\AA(p) \vec t + \Ll(p)\right)^\top & \rvline & p(\vec t)}.
		\end{gather*}

  
        Una quadrica relativa al polinomio $p$ si dice \textit{a centro} se
        $\exists \vec{x_0} \in \AnK \mid p(\vec{x_0} + \vec t) = p(\vec{x_0} - \vec t)$,
        dove $\vec t \in \AnK$; in particolare tale $\vec{x_0}$ è detto \textit{centro di simmetria}. Vale in particolare che $\vec 0$ è un centro di simmetria di $p$ se
        e solo se $\Ll(p) = \vec 0$. \\ \vskip 0.05in
        
        Inoltre vale che $\vec{x_0}$ è un centro di simmetria di $p$ se e solo se $p \circ f$ ha centro di simmetria
        $\vec 0$ tramite
        la traslazione $f(\x) = \x + \vec{x_0}$; pertanto $p$ è a centro se e solo se è risolvibile in $\vec{x_0}$ il sistema:
        \[ \Ll(p \circ f) = \AA(p) \vec {x_0} + \Ll(p) = \vec 0.\]
        Poiché allora i centri della quadriche sono soluzione di un sistema lineare, se esistono, essi formano un sottospazio affine di dimensione $n-\rg(\AA)$; in particolare, se $\vec{x_0}$ è un particolare centro, tale sottospazio affine $C$ è
        esattamente dato da:
        \[ C = \vec{x_0} + \Ker \AA(p). \]

        \subsubsection{Classificazione delle coniche in $\CC$ ed $\RR$}
        Esistono due tipi di classificazioni: una \textit{affine}, dove per ogni quadrica si trova una forma canonica per equivalenza affine tramite la moltiplicazione per scalare $\lambda$ e per applicazione delle affinità di $A(\AnK)$, e una \textit{isometrica}, dove si classificano le quadriche rispetto alle isometrie del gruppo delle isometrie $\Iso(\AnK)$ (e.g.~le ellissi in generale sono affinemente equivalenti, ma non sono isometricamente equivalenti), dove $\Iso(\AnK)$ è composto dalla affinità di $A(\AnK)$
        che preservano la distanza tra punti, qualora definibile.

		Sia $\KK=\CC$. Allora ogni conica è affinemente equivalente ad
		un'equazione canonica della seguente tabella, unicamente
		determinata dagli invarianti $\rg(\MM(p))$ e $\rg(\AA(p))$.
		
		\begin{center}
			\tiny
			\begin{tabular}{|l|l|l|l|l|}
				\hline
				& $\rg(\MM(p))$ & $\rg(\AA(p))$ & Eq.~canonica & A centro \\ \hline
				$\mathcal{C}_1$ & 3             & 2             & $x^2+y^2=1$        & Sì       \\ \hline
				$\mathcal{C}_2$ & 3             & 1             & $x^2=y$            & No       \\ \hline
				$\mathcal{C}_3$ & 2             & 2             & $x^2+y^2=0$        & Sì       \\ \hline
				$\mathcal{C}_4$ & 2             & 1             & $x^2=1$          & Sì       \\ \hline
				$\mathcal{C}_5$ & 1             & 1             & $x^2=0$            & Sì       \\ \hline
			\end{tabular}
		\end{center}

		Tra le uniche coniche non degeneri di $\CC$,
		$\mathcal{C}_1$ prende il nome di \textit{ellisse} e $\mathcal{C}_2$
		di \textit{parabola}. $\mathcal{C}_3$ rappresenta invece una
		\textit{coppia di rette incidenti}, $\mathcal{C}_4$ una \textit{coppia di rette 
		parallele} e $\mathcal{C}_5$ un singolo \textit{punto}.

		Sia $\KK=\RR$. Allora ogni conica è affinemente equivalente ad
		un'equazione canonica della seguente tabella, unicamente
		determinata dagli invarianti $\rg(\MM(p))$, $\rg(\AA(p))$,
		$S(\MM(p)) := \abs{\iota_+(\MM(p)) - \iota_-(\MM(p))}$ e
		$S(\AA(p)) := \abs{\iota_+(\AA(p)) - \iota_-(\AA(p))}$. \\[0.1in]
		
		\begin{center}
			\tiny
			\begin{tabular}{|l|l|l|l|l|l|}
				\hline
				& $\rg(\MM(p))$ & $\rg(\AA(p))$ & $S(\MM(p))$ & $S(\AA(p))$ & Eq.~canonica \\ \hline
				$\mathcal{C}_1$ & 3 & 2 & 1 & 2 & $x^2+y^2-1=0$ \\ \hline
				$\mathcal{C}_2$ & 3 & 2 & 1 & 0 & $x^2-y^2-1=0$ \\ \hline
				$\mathcal{C}_3$ & 3 & 1 & 1 & 1 & $x^2-y=0$ \\ \hline
				$\mathcal{C}_4$ & 2 & 2 & 0 & 0 & $x^2-y^2=0$ \\ \hline
				$\mathcal{C}_5$ & 2 & 1 & 0 & 1 & $x^2-1=0$ \\ \hline
				$\mathcal{C}_6$ & 1 & 1 & 1 & 1 & $x^2=0$ \\ \hline
				$\mathcal{C}_7$ & 3 & 2 & 3 & 2 & $x^2+y^2+1=0$ \\ \hline
				$\mathcal{C}_8$ & 2 & 2 & 2 & 2 & $x^2+y^2=0$ \\ \hline
				$\mathcal{C}_9$ & 2 & 1 & 2 & 1 & $x^2+1=0$ \\ \hline
			\end{tabular}
		\end{center}
		
		Per $\KK=\RR$, $\mathcal{C}_1$ prende il nome di \textit{ellisse reale}, $\mathcal{C}_2$
		di \textit{iperbole} e $\mathcal{C}_3$ di \textit{parabola}. $\mathcal{C}_4$ rappresenta invece una
		\textit{coppia di rette reali incidenti}, $\mathcal{C}_5$ una \textit{coppia di rette
		reali parallele} e $\mathcal{C}_6$ un singolo \textit{punto}. $\mathcal{C}_7$ è
		una \textit{ellisse immaginaria}, $\mathcal{C}_8$ una \textit{coppia di rette
		immaginarie incidenti} e $\mathcal{C}_9$ una \textit{coppia di rette immaginarie
		parallele}. Tutte queste coniche sono a centro eccetto per la parabola
		($\mathcal{C}_3$).
		
		\vfill
		\hrule
		~\\
		Ad opera di Gabriel Antonio Videtta, \url{https://poisson.phc.dm.unipi.it/~videtta/}.
		~\\Reperibile su
		\url{https://g1.hearot.it}.
	\end{multicols}
	
\end{document}

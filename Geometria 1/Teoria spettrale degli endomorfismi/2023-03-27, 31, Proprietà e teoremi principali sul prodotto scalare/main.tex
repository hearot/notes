\documentclass[11pt]{article}
\usepackage{personal_commands}
\usepackage[italian]{babel}

\title{\textbf{Note del corso di Geometria 1}}
\author{Gabriel Antonio Videtta}
\date{27 e 31 marzo 2023}

\begin{document}
	
	\maketitle
	
	\wip
	
	\begin{center}
		\Large \textbf{Proprietà e teoremi principali sul prodotto scalare}
	\end{center}
	
	\begin{note}
		Nel corso del documento, per $V$ si intenderà uno spazio vettoriale di dimensione
		finita $n$ e per $\varphi$ un suo prodotto scalare.
	\end{note}
	
	\begin{proposition} (formula delle dimensioni del prodotto scalare)
		Sia $W \subseteq V$ un sottospazio di $V$. Allora vale la seguente identità:
		
		\[ \dim W + \dim W^\perp = \dim V + \dim (W \cap V^\perp). \]
	\end{proposition}

	\begin{proof}
		Si consideri l'applicazione lineare $f : V \to \dual W$ tale che $f(\vec v)$ è un funzionale di $\dual W$ tale che
		$f(\vec v)(\vec w) = \varphi(\vec v, \vec w)$ $\forall \vec w \in W$. Si osserva che $W^\perp = \Ker f$, da cui,
		per la formula delle dimensioni, $\dim V = \dim W^\perp + \rg f$. Inoltre, si osserva anche che
		$f = i^\top \circ a_\varphi$, dove $i : W \to V$ è tale che $i(\vec w) = \vec w$, infatti $f(\vec v) = a_\varphi(\vec v) \circ i$ è un funzionale di $\dual W$ tale che $f(\vec v)(\vec w) = \varphi(\vec v, \vec w)$. Pertanto
		$\rg f = \rg (i^\top \circ a_\varphi)$. \\
		
		Si consideri ora l'applicazione $g = a_\varphi \circ i : W \to \dual W$. Sia ora $\basis_W$ una base di $W$ e
		$\basis_V$ una base di $V$. Allora le matrice associate di $f$ e di $g$ sono le seguenti:
		
		\begin{enumerate}[(i)]
			\item $M_{\dual \basis_W}^{\basis_V}(f) = M_{\dual \basis_W}^{\basis_V}(i^\top \circ a_\varphi) =
			\underbrace{M_{\dual \basis_W}^{\dual \basis_V}(i^\top)}_A \underbrace{M_{\dual \basis_V}^{\basis_V}(a_\varphi)}_B = AB$,
			\item $M_{\dual \basis_V}^{\basis_W}(g) = M_{\dual \basis_V}^{\basis_W}(a_\varphi \circ i) =
			\underbrace{M_{\dual \basis_V}^{\basis_V}(a_\varphi)}_B \underbrace{M_{\basis_V}^{\basis_W}(i)}_{A^\top} = BA^\top \overbrace{=}^{B^\top = B} (AB)^\top$.
		\end{enumerate}
	
		Poiché $\rg(A) = \rg(A^\top)$, si deduce che $\rg(f) = \rg(g) \implies \rg(i^\top \circ a_\varphi) = \rg(a_\varphi \circ i) = \rg(\restr{a_\varphi}{W}) = \dim W - \dim \Ker \restr{a_\varphi}{W} = \dim W - \dim (W \cap \underbrace{\Ker a_\varphi}_{V^\perp}) = \dim W - \dim (W \cap V^\perp)$. Si conclude allora, sostituendo quest'ultima
		identità nell'identità ricavata a inizio dimostrazione che $\dim V = \dim W^\top + \dim W - \dim (W \cap V^\perp)$,
		ossia la tesi.
	\end{proof}

	\begin{remark}
		Si possono fare alcune osservazioni sul radicale di un solo elemento $\vec w$ e su quello del suo sottospazio
		generato $W = \Span(\vec w)$: \\
		
		\li $\vec w ^\perp = W^\perp$, \\
		\li $\vec w \notin W^\perp \iff \Rad (\restr{\varphi}{W}) = W \cap W^\perp \iff \vec w \text{ non è isotropo } = \zerovecset \iff
		V = W \oplus W^\perp$.
	\end{remark}

	\begin{definition}
		Si definisce \textbf{base ortogonale} di $V$ una base $\vv 1$, ..., $\vv n$ tale per cui $\varphi(\vv i, \vv j) = 0
		\impliedby i \neq j$, ossia per cui la matrice associata del prodotto scalare è diagonale. 
	\end{definition}

	\begin{proposition} (formula di polarizzazione)
		Se $\Char \KK \neq 2$, un prodotto scalare è univocamente determinato dalla sua forma quadratica $q$.
	\end{proposition}

	\begin{proof}
		Si nota infatti che $q(\vec v + \vec w) - q(\vec v) - q(\vec w) = 2 \varphi(\vec v, \vec w)$, e quindi,
		poiché $2$ è invertibile per ipotesi, che $\varphi(\vec v, \vec w) = 2\inv (q(\vec v + \vec w) - q(\vec v) - q(\vec w))$.
	\end{proof}

	\begin{theorem}(di Lagrange)
		Ogni spazio vettoriale $V$ su $\KK$ tale per cui $\Char \KK \neq 2$ ammette una base ortogonale.
	\end{theorem}

	\begin{proof}
		Sia dimostra il teorema per induzione su $n := \dim V$. Per $n \leq 1$, la dimostrazione è triviale. Sia
		allora il teorema vero per $i \leq n$. Se $V$ ammette un vettore non isotropo $\vec w$, sia $W = \Span(\vec w)$ e si consideri la decomposizione $V = W \oplus W^\perp$. Poiché $W^\perp$ ha dimensione $n-1$, per ipotesi induttiva
		ammette una base ortogonale. Inoltre, tale base è anche ortogonale a $W$, e quindi l'aggiunta di $\vec w$ a
		questa base ne fa una base ortogonale di $V$. Se invece $V$ non ammette vettori non isotropi, ogni forma quadratica
		è nulla, e quindi il prodotto scalare è nullo per la proposizione precedente.
	\end{proof}


	\begin{note}
		D'ora in poi, nel corso del documento, si assumerà $\Char \KK \neq 2$.
	\end{note}

	\begin{theorem} (di Sylvester, caso complesso)
		Sia $\KK$ un campo i cui elementi sono tutti quadrati di un
		altro elemento del campo (e.g.~$\CC$). Allora esiste una base
		ortogonale $\basis$ tale per cui:
		
		\[ M_\basis(\varphi) = \Matrix{I_r & \rvline & 0 \\ \hline 0 & \rvline & 0\,}. \]
	\end{theorem}

	\begin{proof}
		Per il teorema di Lagrange, esiste una base ortogonale $\basis'$ di $V$.
		Si riordini la base in modo tale che la forma quadratica valutata nei primi elementi sia sempre diversa da zero. Allora, poiché ogni
		elemento di $\KK$ è per ipotesi quadrato di un altro elemento
		di $\KK$, si sostituisca $\basis'$ con una base $\basis$ tale per
		cui, se $q(\vv i) = 0$, $\vv i \mapsto \vv i$, e altrimenti
		$\vv i \mapsto \frac{\vv i}{\sqrt{q(\vv i)}}$. Allora $\basis'$
		è una base tale per cui la matrice associata del prodotto scalare
		in tale base è proprio come desiderata nella tesi, dove $r$ è
		il numero di elementi tali per cui la forma quadratica valutata
		in essi sia diversa da zero.
	\end{proof}

	\begin{remark}
		Si possono effettuare alcune considerazioni sul teorema di Sylvester
		complesso. \\
		
		\li Si può immediatamente concludere che il rango è un invariante
		completo per la congruenza in un campo in cui tutti gli elementi
		sono quadrati, ossia che $A \cong B \iff \rg(A) = \rg(B)$, se $A$ e
		$B$ sono matrici simmetriche: infatti
		ogni matrice simmetrica rappresenta una prodotto scalare, ed è
		pertanto congruente ad una matrice della forma desiderata
		nell'enunciato del teorema di Sylvester complesso. Poiché il rango
		è un invariante della congruenza, si ricava che $r$ nella forma
		della matrice di Sylvester, rappresentando il rango, è anche
		il rango di ogni sua matrice congruente. In particolare, se due
		matrici simmetriche hanno stesso rango, allora sono congruenti
		alla stessa matrice di Sylvester, e quindi, essendo la congruenza
		una relazione di congruenza, sono congruenti a loro volta. \\
		\li Due matrici simmetriche con stesso rango, allora, non solo
		sono SD-equivalenti, ma sono anche congruenti. \\
		\li Ogni base ortogonale deve quindi avere lo stesso numero
		di elementi nulli.
	\end{remark}

	\begin{note}
		La notazione $\varphi > 0$ indica che $\varphi$ è definito positivo.
		Analogamente $\varphi < 0$ indica che $\varphi$ è definito negativo.
	\end{note}

	\begin{definition}
		Data una base ortogonale $\basis$ di $V$ rispetto al prodotto
		scalare $\varphi$,
		si definiscono i seguenti indici:
		
		\begin{align*}
			\iota_+(\varphi) &= \max\{ \dim W \mid W \subseteq V \E \restr{\varphi}{W} > 0 \}, &\text{(}\textbf{indice di positività}\text{)} \\
			\iota_-(\varphi) &= \max\{ \dim W \mid W \subseteq V \E \restr{\varphi}{W} < 0 \}, &\text{(}\textbf{indice di negatività}\text{)}\\
			\iota_0(\varphi). &= \dim V^\perp &\text{(}\textbf{indice di nullità}\text{)}
		\end{align*}
	
		Quando il prodotto scalare $\varphi$ è noto dal contesto, si omette
		e si scrive solo $\iota_+$, $\iota_-$ e $\iota_0$. In particolare,
		la terna $\sigma = (i_+, i_-, i_0)$ è detta \textbf{segnatura} del
		prodotto $\varphi$.
	\end{definition}
	
	\begin{theorem} (di Sylvester, caso reale) Sia $\KK$ un campo ordinato
		i cui elementi positivi sono tutti quadrati (e.g.~$\RR$). Allora
		esiste una base ortogonale $\basis$ tale per cui:
		
		\[ M_\basis(\varphi) = \Matrix{I_{\iota_+} & \rvline & 0 & \rvline & 0 \\ \hline 0 & \rvline & -I_{\iota_-} & \rvline & 0 \\ \hline 0 & \rvline & 0 & \rvline & 0\cdot I_{\iota_0} }. \]
		
		\vskip 0.05in
		
		Inoltre, per ogni base ortogonale, esistono esattamente
		$\iota_+$ vettori della base con forma quadratica positiva,
		$\iota_-$ con forma negativa e $\iota_0$ con
		forma nulla.
	\end{theorem}

	\begin{proof}
		Per il teorema di Lagrange, esiste una base ortogonale $\basis'$ di $V$.
		Si riordini la base in modo tale che la forma quadratica valutata nei primi elementi sia strettamente positiva, che nei secondi elementi sia strettamente negativa e che negli ultimi sia nulla. Si sostituisca
		$\basis'$ con una base $\basis$ tale per cui, se $q(\vv i) > 0$,
		allora $\vv i \mapsto \frac{\vv i}{\sqrt{q(\vv i)}}$; se
		$q(\vv i) < 0$, allora $\vv i \mapsto \frac{\vv i}{\sqrt{-q(\vv i)}}$;
		altrimenti $\vv i \mapsto \vv i$. Si è allora trovata una base
		la cui matrice associata del prodotto scalare è come desiderata nella
		tesi. \\
		
		Sia ora $a$ il numero di vettori della base con forma quadratica
		positiva, $b$ il numero di vettori con forma negativa e $c$ quello
		dei vettori con forma nulla. Si consideri $W_+ = \Span(\vv 1, ..., \vv a)$, $W_- = \Span(\vv{a+1}, ..., \vv b)$, $W_0 = \Span(\vv{b+1}, ..., \vv c)$. \\
		
		Sia $M = M_\basis(\varphi)$. Si osserva che $c = n - \rg(M) = \dim \Ker(M) = \dim V^\perp = \iota_0$. Inoltre $\forall \v \in W_+$, dacché
		$\basis$ è ortogonale,
		$q(\v) = q(\sum_{i=1}^a \alpha_i \vv i) = \sum_{i=1}^a \alpha_i^2 q(\vv i) > 0$, e quindi $\restr{\varphi}{W_+} > 0$, da cui $\iota_+ \geq a$.
		Analogamente $\iota_- \geq b$. \\
		
		Si mostra ora che è impossibile che $\iota_+ > a$. Se così infatti
		fosse, sia $W$ tale che $\dim W = \iota_+$ e che $\restr{\varphi}{W} > 0$. $\iota_+ + b + c$ sarebbe maggiore di $a + b + c = n := \dim V$. Quindi, per la formula di Grassman, $\dim(W + W_- + W_0) = \dim W +
		\dim(W_- + W_0) - \dim (W \cap (W_- + W_0)) \implies \dim (W \cap (W_- + W_0)) =  \dim W +
		\dim(W_- + W_0) - \dim(W + W_- + W_0) > 0$, ossia esisterebbe
		$\v \neq \{\vec 0\} \mid \v \in W \cap (W_- + W_0)$. Tuttavia
		questo è assurdo, dacché dovrebbe valere sia $q(\v) > 0$ che
		$q(\v) < 0$, \Lightning. Quindi $\iota_+ = a$, e analogamente
		$\iota_- = b$.
	\end{proof}

	\begin{definition}
		Si dice \textbf{base di Sylvester} una base di $V$ tale per cui la
		matrice associata di $\varphi$ sia esattamente nella forma
		vista nella dimostrazione del teorema di Sylvester. Analogamente
		si definisce tale matrice come \textbf{matrice di Sylvester}.
	\end{definition}

	\begin{remark} \nl
		\li Si può dunque definire la segnatura di una matrice simmetrica
		come la segnatura di una qualsiasi sua base ortogonale, dal
		momento che tale segnatura è invariante per cambiamento di base. \\
		\li La segnatura è un invariante completo per la congruenza nel caso reale. Se infatti due matrici hanno la stessa segnatura, sono
		entrambe congruenti alla matrice come vista nella dimostrazione
		della forma reale del teorema di Sylvester, e quindi, essendo
		la congruenza una relazione di equivalenza, sono congruenti
		tra loro. Analogamente vale il viceversa, dal momento che ogni
		base ortogonale di due matrici congruenti devono contenere gli
		stessi numeri $\iota_+$, $\iota_-$ e $\iota_0$ di vettori
		di base con forma quadratica positiva, negativa e nulla. \\
		\li Se $\ww 1$, ..., $\ww k$ sono tutti i vettori di una base
		ortogonale $\basis$ con forma quadratica nulla, si osserva che $W = \Span(\ww 1, ..., \ww k)$ altro non è che $V^\perp$ stesso. Infatti, come
		visto anche nella dimostrazione del teorema di Sylvester reale, vale
		che	$\dim W = \dim \Ker (M_\basis(\varphi)) = \dim V^\perp$. Inoltre,
		se $\w \in W$ e $\v \in V$, $\varphi(\w, \v) = \varphi(\sum_{i=1}^k \alpha_i \ww i, \sum_{i=1}^k \beta_i \ww i + \sum_{i=k+1}^n \beta_i \vv i) = \sum_{i=1}^k \alpha_i \beta_i q(\ww i) = 0$, e quindi
		$W \subseteq V^\perp$, da cui si conclude che $W = V^\perp$. \\
		\li Vale in particolare che $\rg(\varphi) = \iota_+ + \iota_-$, mentre
		$\dim \Ker(\varphi) = \iota_0$, e quindi $n = \iota_+ + \iota_- + \iota_0$.
	\end{remark}

	\begin{definition}
		Dati due spazi vettoriali $(V, \varphi)$ e
		$(V', \varphi')$ dotati di prodotto scalare sullo stesso campo $\KK$, si dice che
		$V$ e $V'$ sono \textbf{isometrici} se esiste un isomorfismo
		$f$, detto isometria, che preserva tali che prodotti, ossia tale che:
		
		\[ \varphi(\vec v, \vec w) = \varphi'(f(\vec v), f(\vec w)). \]
	\end{definition}

	\begin{exercise}\nl
		\li $f : V \to V'$ è un isometria $\iff$ per una base $\basis =
		\{\vv1, ..., \vv k\}$ di $V$, $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\iff$ vale per ogni base.
	\end{exercise}

	\begin{proposition}
		Per $(V, \varphi)$ e $(V', \varphi')$ sono equivalenti:
		
		\begin{enumerate}[(i)]
			\item $V$ e $V'$ sono isometrici;
			\item $\forall$ base $\basis$ di $V$, $\basis'$ di $V'$,
			$M_\basis(\varphi)$ e $M_\basis'(\varphi')$ sono congruenti;
			\item lo stesso ma per una base.
		\end{enumerate}
	\end{proposition}

	\begin{proof}
		(1-2)
		
		 \rightproof Sia $\basis'' = f(\basis)$. Allora $M_{\basis''}(\varphi')=
		 (\varphi'(f(\vv i), f(\vv j))) = (\varphi(\vv i, \vv j))$. Allora,
		 per la formula di cambiamento di base, le matrici
		 $M_{\basis'}(\varphi')$ e $M_{\basis'}(\varphi')$ sono congruenti.
		 
		 \leftproof Sia $A = M_\basis(\varphi) = P^\top B P$ e
		 $B = M_{\basis'}(\varphi')$. Allora $a_{ij} = \varphi(\vv i, \vv j) =
		 ... = \varphi'(\vec{v_i^{ii}}, \vec{v_j^{ii}})$, dove
		 $\vec{v_i^{ii}}$ è base perché $P$ è invertibile. Allora
		 l'applicazione $f : V \to V'$ che manda $\vv i \mapsto \vec{v_{i}^{''}}$ è un isometria.
		 
		 (2-3) esercizio.
	\end{proof}

	\begin{proposition} $(V, \varphi)$ e $(V', \varphi')$ spazi vettoriali
		su $\RR$ sono
		isometrici $\iff$ $\varphi$ e $\varphi'$ hanno la stessa segnatura.
	\end{proposition}

	\begin{proof}
		\rightproof Basta che prendi la solita base. \\
		\leftproof Siano $\basis$, $\basis'$ basi di Sylvester di $V$
		e di $V'$. Si definisce allora l'applicazione $f : V \to V'$ tale
		che $f(\vv i) = \ww i$: essa è un isometria.
	\end{proof}

	\begin{corollary}
		Due matrici simmetriche sono congruenti se e solo se hanno
		la stessa segnatura.
	\end{corollary}

	\begin{definition} (somma diretta ortogonale)
		$V = U \oplusperp W$.
	\end{definition}

	\begin{remark}\nl
		\li se $V = U \oplusperp W$, allora $\iota_+(\varphi) = \iota_+(\restr{\varphi}{V}) + \iota_+(\restr{\varphi}{W})$, e
		analogamente per gli altri indici.
	\end{remark}

	\begin{example}
		Per $\varphi = x_1 y_1 + x_2 y_2 - x_3 y_3$. %TODO: guarda dalle slide.
	\end{example}

	\begin{definition}
		Sia $\KK$ qualunque. $W \subseteq V$ si dice sottospazio isotropo
		se $\restr{\varphi}{W} = 0$.
	\end{definition}

	\begin{remark}\nl
		\li $V^\perp$ è isotropo, \\
		\li $\vec{v}$ è un vettore isotropo $\iff$ $W = \Span(\vec v)$ è sottospazio isotropo, \\
		\li $W \subseteq V$ è isotropo $\iff$ $W \subseteq W^\perp$.
	\end{remark}

	\begin{proposition}
		Sia $\varphi$ non degenere. $W \subseteq V$ isotropo, allora
		$\dim W \leq \frac{1}{2} \dim V$.
	\end{proposition}

	\begin{proof}
		$W \subseteq W^\perp \implies \dim W \leq \dim W^\perp \implies
		\dim W \leq \dim V - \dim W \implies \dim W \leq \frac{1}{2} \dim V$.
	\end{proof}

	\begin{definition}
		Si definisce \textbf{indice di Witt} $W(\varphi)$ di $(V, \varphi)$
		come la massima dimensione di un sottospazio isotropo. 
	\end{definition}

	\begin{remark}\nl
		\li Se $\varphi > 0$, $W(\varphi) = 0$.
	\end{remark}

	\begin{proposition}
		Per $\KK = \RR$ e $\sigma(\varphi) = (\iota_+(\varphi), \iota_-(\varphi), \iota_0(\varphi))$, con $\varphi$ non degenere,
		$W(\varphi) = \min\{\iota_+(\varphi), \iota_-(\varphi)\}$.
	\end{proposition}

	\begin{proof}
		Sia ad esempio $\iota_-(\varphi) \leq \iota_+(\varphi)$. Se $W$
		è un sottospazio con $\dim W > \iota_-(\phi)$, e $W^+$ è
		un sottospazio con $\dim W^+ = \iota_+(\varphi)$ e $\restr{\varphi}{W^+} > 0 \implies \dim (W \cap W^+) > 0$,
		e quindi $W$ non è isotropo (quindi $W(\varphi) < \iota_-(\varphi)$). \\
		
		Sia $\basis$ una base di Sylvester. Per costruirlo prendi
		coppie della base originale facendo la differenza e nota
		che ne prendi esattamente quante iota-.
	\end{proof}
\end{document}
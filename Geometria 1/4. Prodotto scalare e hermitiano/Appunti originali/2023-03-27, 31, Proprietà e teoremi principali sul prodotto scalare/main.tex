\documentclass[11pt]{article}
\usepackage{personal_commands}
\usepackage[italian]{babel}

\title{\textbf{Note del corso di Geometria 1}}
\author{Gabriel Antonio Videtta}
\date{27 e 31 marzo 2023}

\begin{document}
	
	\maketitle
	
	\begin{center}
		\Large \textbf{Proprietà e teoremi principali sul prodotto scalare}
	\end{center}
	
	\begin{note}
		Nel corso del documento, per $V$ si intenderà uno spazio vettoriale di dimensione
		finita $n$ e per $\varphi$ un suo prodotto scalare. Analogamente si intenderà lo stesso
		per $V'$ e $\varphi'$.
	\end{note}
	
	\begin{proposition}[formula delle dimensioni del prodotto scalare]
		Sia $W \subseteq V$ un sottospazio di $V$. Allora vale la seguente identità:
		
		\[ \dim W + \dim W^\perp = \dim V + \dim (W \cap V^\perp). \]
	\end{proposition}

	\begin{proof}
		Si consideri l'applicazione lineare $a_\varphi$ introdotta precedentemente. Si osserva che $W^\perp = \Ker (i^\top \circ a_\varphi)$, dove $i : W \to V$ è tale che $i(\vec w) = \vec w$. Allora,
		per la formula delle dimensioni, vale la seguente identità: 
		
		\begin{equation}
			\label{eq:dim_formula_dimensioni_1}
			\dim V = \dim W^\perp + \rg (i^\top \circ a_\varphi). 
		\end{equation}
		
		\vskip 0.05in
		
		Sia allora $f = i^\top \circ a_\varphi$.
		Si consideri ora l'applicazione $g = a_\varphi \circ i : W \to \dual V$. Sia ora $\basis_W$ una base di $W$ e
		$\basis_V$ una base di $V$. Allora le matrici associate di $f$ e di $g$ sono le seguenti:
		
		\begin{enumerate}[(i)]
			\item $M_{\dual \basis_W}^{\basis_V}(f) = M_{\dual \basis_W}^{\basis_V}(i^\top \circ a_\varphi) =
			\underbrace{M_{\dual \basis_W}^{\dual \basis_V}(i^\top)}_A \underbrace{M_{\dual \basis_V}^{\basis_V}(a_\varphi)}_B = AB$,
			\item $M_{\dual \basis_V}^{\basis_W}(g) = M_{\dual \basis_V}^{\basis_W}(a_\varphi \circ i) =
			\underbrace{M_{\dual \basis_V}^{\basis_V}(a_\varphi)}_B \underbrace{M_{\basis_V}^{\basis_W}(i)}_{A^\top} = BA^\top \overbrace{=}^{B^\top = B} (AB)^\top$.
		\end{enumerate}
	
		Poiché $\rg(A) = \rg(A^\top)$, si deduce che $\rg(f) = \rg(g) \implies \rg(i^\top \circ a_\varphi) = \rg(a_\varphi \circ i) = \rg(\restr{a_\varphi}{W}) = \dim W - \dim \Ker \restr{a_\varphi}{W}$, ossia che:
		
		\begin{equation}
			\label{eq:dim_formula_dimensioni_2}
			\rg(i^\top \circ a_\varphi) = \dim W - \dim (W \cap \underbrace{\Ker a_\varphi}_{V^\perp}) = \dim W - \dim (W \cap V^\perp).
		\end{equation}
		
		Si conclude allora, sostituendo l'equazione \eqref{eq:dim_formula_dimensioni_2} nell'equazione \eqref{eq:dim_formula_dimensioni_1}, che $\dim V = \dim W^\top + \dim W - \dim (W \cap V^\perp)$, ossia la tesi.
	\end{proof}

	\begin{remark} Si identifica $\w^\perp$ come il sottospazio di tutti i vettori di $V$ ortogonali a $\w$.
		In particolare, se $W = \Span(\vec w)$ è il sottospazio generato da $\vec w \neq \vec 0$, $\vec w \in V$, allora $W^\perp = \w^\perp$. Inoltre valgono le seguenti equivalenze: $\vec w \notin W^\perp \iff$ $\Rad (\restr{\varphi}{W}) = W \cap W^\perp = \zerovecset$ $\iff \vec w \text{ non è isotropo } \iff$ $V = W \oplus W^\perp$.
	\end{remark}

	\begin{proposition}[formula di polarizzazione]
		Se $\Char \KK \neq 2$, un prodotto scalare è univocamente determinato dalla sua forma quadratica $q$.
		In particolare vale la seguente identità:
		
		\[ \varphi(\v, \w) = \frac{q(\v + \w) - q(\v) - q(\w)}{2}. \]
		
		\vskip 0.05in
	\end{proposition}

	\begin{proof}
		Si osserva che $q(\vec v + \vec w) - q(\vec v) - q(\vec w) = 2 \varphi(\vec v, \vec w)$, e quindi,
		poiché $2$ è invertibile per ipotesi, si deduce che $\varphi(\vec v, \vec w) = \frac{q(\vec v + \vec w) - q(\vec v) - q(\vec w)}{2}$.
	\end{proof}

	\begin{definition}
		Si definisce \textbf{base ortogonale} di $V$ una base $\vv 1$, ..., $\vv n$ tale per cui $\varphi(\vv i, \vv j) = 0
		\impliedby i \neq j$, ossia una base per cui la matrice associata del prodotto scalare è diagonale. 
	\end{definition}

	\begin{theorem}[di Lagrange]
		Ogni spazio vettoriale $V$ su $\KK$ tale per cui $\Char \KK \neq 2$ ammette una base ortogonale.
	\end{theorem}

	\begin{proof}
		Si dimostra il teorema per induzione su $n := \dim V$. Per $n \leq 1$, la tesi è triviale (se esiste una base, tale base è
		già ortogonale). Sia
		allora il teorema vero per $i \leq n$. Se $V$ ammette un vettore non isotropo $\vec w$, sia $W = \Span(\vec w)$ e si consideri la decomposizione $V = W \oplus W^\perp$. Poiché $W^\perp$ ha dimensione $n-1$, per ipotesi induttiva
		ammette una base ortogonale. Inoltre, tale base è anche ortogonale a $W$, e quindi l'aggiunta di $\vec w$ a
		questa base ne fa una base ortogonale di $V$. Se invece $V$ non ammette vettori non isotropi, ogni forma quadratica
		è nulla, e quindi il prodotto scalare è nullo per la proposizione precedente. Allora in questo caso
		ogni base è una base ortogonale, completando il passo induttivo, e dunque la dimostrazione.
	\end{proof}


	\begin{note}
		D'ora in poi, nel corso del documento, si assumerà $\Char \KK \neq 2$.
	\end{note}

	\begin{theorem}[di Sylvester, caso complesso]
		Sia $\KK$ un campo i cui elementi sono tutti quadrati di un
		altro elemento del campo (e.g.~$\CC$). Allora esiste una base
		ortogonale $\basis$ tale per cui:
		
		\[ M_\basis(\varphi) = \Matrix{I_r & \rvline & 0 \\ \hline 0 & \rvline & 0\,}. \]
	\end{theorem}

	\begin{proof}
		Per il teorema di Lagrange, esiste una base ortogonale $\basis'$ di $V$.
		Si riordini allora la base $\basis'$ in modo tale che la forma quadratica valutata nei primi elementi sia sempre diversa da zero. Allora, poiché ogni
		elemento di $\KK$ è per ipotesi quadrato di un altro elemento
		di $\KK$, si sostituisca $\basis'$ con una base $\basis$ tale per
		cui, se $q(\vv i) = 0$, $\vv i \mapsto \vv i$, e altrimenti
		$\vv i \mapsto \frac{\vv i}{\sqrt{q(\vv i)}}$. Allora $\basis$
		è una base tale per cui la matrice associata del prodotto scalare
		in tale base è proprio come desiderata nella tesi, dove $r$ è
		il numero di elementi tali per cui la forma quadratica valutata
		in essi sia diversa da zero.
	\end{proof}

	\begin{remark}\nl
		\li Si può immediatamente concludere che il rango è un invariante
		completo per la congruenza in un campo $\KK$ in cui tutti gli elementi
		sono quadrati, ossia che $A \cong B \iff \rg(A) = \rg(B)$, se $A$ e
		$B$ sono matrici simmetriche con elementi in $\KK$. \\
		
		Ogni matrice simmetrica rappresenta infatti un prodotto scalare, ed è
		pertanto congruente ad una matrice della forma desiderata
		nell'enunciato del teorema di Sylvester complesso. Poiché il rango
		è un invariante della congruenza, si ricava che $r$ nella forma
		della matrice di Sylvester, rappresentando il rango, è anche
		il rango di ogni sua matrice congruente. \\
	
		In particolare, se due
		matrici simmetriche hanno lo stesso rango, allora sono congruenti
		alla stessa matrice di Sylvester, e quindi, essendo la congruenza
		una relazione di equivalenza, sono congruenti a loro volta tra di loro. \\

		\li Due matrici simmetriche in $\KK$ con stesso rango, allora, non solo
		sono SD-equivalenti, ma sono anche congruenti. \\

		\li Ogni base ortogonale deve quindi avere lo stesso numero
		di vettori isotropi, dal momento che tale numero rappresenta
		la dimensione del radicale $V^\perp$.
	\end{remark}

	\begin{definition}[somma diretta ortogonale]
		Siano i sottospazi $U$ e $W \subseteq V$ in somma diretta. Allora si dice che $U$ e $W$ sono in \textbf{somma
		diretta ortogonale} rispetto al prodotto scalare $\varphi$ di $V$, ossia che $U \oplus W = U \oplus^\perp W$, se $\varphi(\vec u, \vec w) = 0$ $\forall \vec u \in U$, $\vec w \in W$.
	\end{definition}

	\begin{definition}[cono isotropo]
		Si definisce \textbf{cono isotropo} di $V$ rispetto al prodotto scalare $\varphi$ il seguente insieme:
		
		\[ \CI(\varphi) = \{ \v \in V \mid \varphi(\v, \v) = 0 \}, \]
		
		\vskip 0.05in
		
		ossia l'insieme dei vettori isotropi di $V$.
	\end{definition}

	\begin{note}
		La notazione $\varphi > 0$ indica che $\varphi$ è definito positivo (si scrive $\varphi \geq 0$ se invece è semidefinito
		positivo).
		Analogamente $\varphi < 0$ indica che $\varphi$ è definito negativo (e $\varphi \leq 0$ indica che è semidefinito negativo).
	\end{note}

	\begin{exercise} Sia $\Char \KK \neq 2$.
		Siano $\vv1$, ..., $\vv k \in V$ e sia $M = \left( \varphi(\vv i, \vv j) \right)_{i, j = 1\textrm{---}k} \in M(k, \KK)$,
		dove $\varphi$ è un prodotto scalare di $V$. Sia inoltre $W = \Span(\vv 1, ..., \vv k)$. Si dimostrino
		allora le seguenti affermazioni.
		
		\begin{enumerate}[(i)]
			\item Se $M$ è invertibile, allora $\vv 1$, ..., $\vv k$ sono linearmente indipendenti.
			
			\item Siano $\vv 1$, ..., $\vv k$ linearmente indipendenti. Allora $M$ è invertibile $\iff$ $\restr{\varphi}{W}$ è non degenere $\iff$ $W \cap W^\perp = \zerovecset$.
			
			\item Siano $\vv1$, ..., $\vv k$ a due a due ortogonali tra loro. Allora $M$ è invertibile $\iff$ nessun
			vettore $\vv i$ è isotropo.
			
			\item Siano $\vv1$, ..., $\vv k$ a due a due ortogonali tra loro e siano anche linearmente indipendenti.
			Allora $M$ è invertibile $\implies$ si può estendere $\basis_W = \{\vv 1, \ldots, \vv k\}$ a una base ortogonale di $V$.
			
			\item Sia $\KK = \RR$. Sia inoltre $\varphi > 0$. Allora $\vv 1$, ..., $\vv k$ sono linearmente
			indipendenti $\iff$ $M$ è invertibile.
			
			\item Sia $\KK = \RR$. Sia ancora $\varphi > 0$. Allora se $\vv 1$, ..., $\vv k$ sono a due a due
			ortogonali e sono tutti non nulli, sono anche linearmente indipendenti.
		\end{enumerate}
	\end{exercise}

	\begin{solution}
		\begin{enumerate}[(i)]
			\item Siano $a_1$, ..., $a_k \in \KK$ tali che $a_1 \vv 1 + \ldots + a_k \vv k = 0$. Vale in
			particolare che $\vec 0 = \varphi(\vv i, \vec 0) = \varphi(\vv i, a_1 \vv 1 + \ldots + a_k \vv k) =
			\sum_{j=1}^k a_j \varphi(\vv i, \vv j)$ $\forall 1 \leq i \leq k$. Allora $\sum_{j=1}^k a_j M^j = 0$.
			Dal momento che $M$ è invertibile, $\rg(M) = k$, e quindi l'insieme delle colonne di $M$ è linearmente
			indipendente, da cui si ricava che $a_j = 0$ $\forall 1 \leq j \leq k$, e quindi che $\vv 1$, ...,
			$\vv k$ sono linearmente indipendenti.
			
			\item Poiché $\vv 1$, ..., $\vv k$ sono linearmente indipendenti, tali vettori formano una base di
			$W$, detta $\basis$. In particolare, allora, vale che $M = M_\basis(\restr{\varphi}{W})$. Pertanto,
			se $M$ è invertibile, $\Rad(\restr{\varphi}{W}) = \Ker M = \zerovecset$, e dunque $\restr{\varphi}{W}$
			è non degenere. Se invece $\restr{\varphi}{W}$ è non degenere, $\zerovecset = \Rad(\restr{\varphi}{W}) = W \cap W^\perp$. Infine, se $W \cap W^\perp = \zerovecset$, $\zerovecset = W \cap W^\perp = \Rad(\restr{\varphi}{W}) = \Ker M$, e quindi $M$ è iniettiva, e dunque invertibile.
			
			\item Dal momento che $\vv 1$, ..., $\vv k$ sono ortogonali tra loro, $M$ è una matrice diagonale.
			Pertanto $M$ è invertibile se e solo se ogni suo elemento diagonale è diverso da $0$, ossia
			se $\varphi(\vv i, \vv i) \neq 0$ $\forall 1 \leq i \leq k$, e dunque se e solo se nessun vettore
			$\vv i$ è isotropo.
			
			\item Se $M$ è invertibile, da (ii) si deduce che $\Rad(\restr{\varphi}{W}) = W \cap W^\perp = \zerovecset$,
			e quindi che $W$ e $W^\perp$ sono in somma diretta. Inoltre, per la formula delle dimensioni del prodotto
			scalare, $\dim W + \dim W^\perp = \dim V + \underbrace{\dim (W \cap V^\perp)}_{\leq \dim (W \cap W^\perp) = 0} = \dim V$. Pertanto $V = W \oplus^\perp W^\perp$. \\ 
			
			Allora, dacché $\Char \KK \neq 2$, per il teorema di Lagrange, $W^\perp$ ammette una base ortogonale $\basis_{W^\perp}$. Si conclude
			dunque che $\basis = \basis_W \cup \basis_{W^\perp}$ è una base ortogonale di $V$.
			
			\item Se $M$ è invertibile, da (i) $\vv1$, ..., $\vv k$ sono linearmente indipendenti. Siano ora
			invece $\vv 1$, ..., $\vv k$ linearmente indipendenti per ipotesi. Siano $a_1$, ..., $a_k \in \KK$ tali
			che $a_1 M^1 + \ldots + a_k M^k = 0$, allora $a_1 \varphi(\vv i, \vv 1) + \ldots + a_k \varphi(\vv i, \vv k) = 0$ $\forall 1 \leq i \leq k$. Pertanto, detto $\v = a_1 \vv 1 + \ldots + a_k \vv k$, si ricava che:
			
			\[ \varphi(\v, \v) = \sum_{i=1}^k \sum_{j=1}^k a_j \, \varphi(\vv i, \vv j) = 0. \]
			
			Tuttavia questo è possibile solo se $\v = a_1 \vv 1 + \ldots + a_k \vv k = 0$. Dal momento che
			$\vv 1$, ..., $\vv k$ sono linearmente indipendenti, si conclude che $a_1 = \cdots = a_k = 0$, ossia
			che le colonne di $M$ sono tutte linearmente indipendenti e quindi che $\rg(M) = k \implies$ $M$ è invertibile.
			
			\item Poiché $\vv 1$, ..., $\vv k$ sono ortogonali a due a due tra loro, $M$ è una matrice diagonale.
			Inoltre, dacché $\varphi > 0$ e $\vv i \neq \vec 0$ $\forall 1 \leq i \leq k$, gli elementi diagonali di $M$ sono sicuramente tutti diversi da zero, e quindi $\det (M) \neq 0$ $\implies$ $M$ è invertibile. Allora,
			per il punto (v), $\vv 1$, ..., $\vv k$ sono linearmente indipendenti.
		\end{enumerate}
	\end{solution}

	\begin{definition} [indici e segnatura]
		Data una base ortogonale $\basis$ di $V$ rispetto al prodotto
		scalare $\varphi$,
		si definiscono i seguenti indici:
		\begin{align*}
			\iota_+(\varphi) &= \max\{ \dim W \mid W \subseteq V \E \restr{\varphi}{W} > 0 \}, &\text{(}\textbf{indice di positività}\text{)} \\
			\iota_-(\varphi) &= \max\{ \dim W \mid W \subseteq V \E \restr{\varphi}{W} < 0 \}, &\text{(}\textbf{indice di negatività}\text{)}\\
			\iota_0(\varphi) &= \dim V^\perp. &\text{(}\textbf{indice di nullità}\text{)}
		\end{align*}
	
		Quando il prodotto scalare $\varphi$ è noto dal contesto, si
		semplifica la notazione
		scrivendo solo $\iota_+$, $\iota_-$ e $\iota_0$. In particolare,
		la terna $\sigma(\varphi) = \sigma = (i_+, i_-, i_0)$ è detta \textbf{segnatura} del
		prodotto $\varphi$.
	\end{definition}
	
	\begin{theorem}[di Sylvester, caso reale] Sia $\KK$ un campo ordinato
		i cui elementi positivi sono tutti quadrati (e.g.~$\RR$). Allora
		esiste una base ortogonale $\basis$ tale per cui:
		
		\[ M_\basis(\varphi) = \Matrix{I_{\iota_+} & \rvline & 0 & \rvline & 0 \\ \hline 0 & \rvline & -I_{\iota_-} & \rvline & 0 \\ \hline 0 & \rvline & 0 & \rvline & 0\cdot I_{\iota_0} }. \]
		
		\vskip 0.05in
		
		Inoltre, per ogni base ortogonale, esistono esattamente
		$\iota_+$ vettori della base con forma quadratica positiva,
		$\iota_-$ con forma negativa e $\iota_0$ con
		forma nulla.
	\end{theorem}

	\begin{proof}
		Per il teorema di Lagrange, esiste una base ortogonale $\basis'$ di $V$.
		Si riordini la base in modo tale che la forma quadratica valutata nei primi elementi sia strettamente positiva, che nei secondi elementi sia strettamente negativa e che negli ultimi sia nulla. Si sostituisca
		$\basis'$ con una base $\basis$ tale per cui, se $q(\vv i) > 0$,
		allora $\vv i \mapsto \frac{\vv i}{\sqrt{q(\vv i)}}$; se
		$q(\vv i) < 0$, allora $\vv i \mapsto \frac{\vv i}{\sqrt{-q(\vv i)}}$;
		altrimenti $\vv i \mapsto \vv i$. Si è allora trovata una base
		la cui matrice associata del prodotto scalare è come desiderata nella
		tesi. \\
		
		Sia ora $\basis$ una qualsiasi base ortogonale di $V$.
		Siano inoltre $a$ il numero di vettori della base con forma quadratica
		positiva, $b$ il numero di vettori con forma negativa e $c$ quello
		dei vettori con forma nulla. Si consideri $W_+ = \Span(\vv 1, ..., \vv a)$, $W_- = \Span(\vv{a+1}, ..., \vv b)$, $W_0 = \Span(\vv{b+1}, ..., \vv c)$. \\
		
		Sia $M = M_\basis(\varphi)$. Si osserva che $c = n - \rg(M) = \dim \Ker(M) = \dim V^\perp = \iota_0$. Inoltre $\forall \v \in W_+$, dacché
		$\basis$ è ortogonale,
		$q(\v) = q(\sum_{i=1}^a \alpha_i \vv i) = \sum_{i=1}^a \alpha_i^2 q(\vv i) > 0$, e quindi $\restr{\varphi}{W_+} > 0$, da cui $\iota_+ \geq a$.
		Analogamente $\iota_- \geq b$. \\
		
		Si mostra ora che è impossibile che $\iota_+ > a$. Se così infatti
		fosse, sia $W$ tale che $\dim W = \iota_+$ e che $\restr{\varphi}{W} > 0$. $\iota_+ + b + c$ sarebbe maggiore di $a + b + c = n := \dim V$. Quindi, per la formula di Grassman, $\dim(W + W_- + W_0) = \dim W +
		\dim(W_- + W_0) - \dim (W \cap (W_- + W_0)) \implies \dim (W \cap (W_- + W_0)) =  \dim W +
		\dim(W_- + W_0) - \dim(W + W_- + W_0) > 0$, ossia esisterebbe
		$\v \neq \{\vec 0\} \mid \v \in W \cap (W_- + W_0)$. Tuttavia
		questo è assurdo, dacché dovrebbe valere sia $q(\v) > 0$ che
		$q(\v) < 0$, \Lightning. Quindi $\iota_+ = a$, e analogamente
		$\iota_- = b$.
	\end{proof}

	\begin{definition}
		Si dice \textbf{base di Sylvester} una base di $V$ tale per cui la
		matrice associata di $\varphi$ sia esattamente nella forma
		vista nell'enunciato del teorema di Sylvester. Analogamente
		si definisce tale matrice come \textbf{matrice di Sylvester}.
	\end{definition}

	\begin{remark} \nl
		\li Come conseguenza del teorema di Sylvester reale, si osserva che la segnatura di una matrice simmetrica reale
		è invariante per cambiamento di base, se la base è ortogonale. \\

		\li La segnatura è un invariante completo per la congruenza nel caso reale. Se infatti due matrici hanno la stessa segnatura, queste sono
		entrambe congruenti alla stessa matrice di Sylvester, e quindi, essendo
		la congruenza una relazione di equivalenza, sono congruenti
		tra loro. Analogamente vale il viceversa, dal momento che ogni
		base ortogonale di due matrici congruenti deve contenere gli
		stessi numeri $\iota_+$, $\iota_-$ e $\iota_0$ di vettori
		di base con forma quadratica positiva, negativa e nulla. \\

		\li Se $\ww 1$, ..., $\ww k$ sono tutti i vettori di una base
		ortogonale $\basis$ con forma quadratica nulla, si osserva che $W = \Span(\ww 1, ..., \ww k)$ altro non è che $V^\perp$ stesso. \\
		
		Infatti, come
		visto anche nella dimostrazione del teorema di Sylvester reale, vale
		che	$\dim W = \dim \Ker (M_\basis(\varphi)) = \dim V^\perp$.
		Sia allora la base $\basis = \{\ww 1, \ldots, \ww k, \vv{k+1}, \ldots, \vv n\}$ un'estensione di $\{\ww 1, \ldots, \ww k\}$. Se $\w \in W$ e $\v \in V$, $\varphi(\w, \v) = \varphi(\sum_{i=1}^k
		\alpha_i \ww i, \sum_{i=1}^k \beta_i \ww i + \sum_{i=k+1}^n \beta_i \vv i)
		= \sum_{i=1}^k \alpha_i \beta_i q(\ww i) = 0$ (dove $\alpha_i$ e $\beta_i \in \KK$ rappresentano la $i$-esima coordinata di $\w$ e $\v$ nella base $\basis$), e quindi
		$W \subseteq V^\perp$. Si conclude allora, tramite l'uguaglianza
		dimensionale, che $W = V^\perp$. \\

		\li Poiché $\dim \Ker(\varphi) = \iota_0$, vale in particolare che $\rg(\varphi) = n - \iota_0 = \iota_+ + \iota_-$ (infatti vale che $n = \iota_+ + \iota_- + \iota_0$, dal momento che $n$ rappresenta il numero di elementi di una base ortogonale). \\

		\li Se $V = U \oplusperp W$, allora $\iota_+(\varphi) = \iota_+(\restr{\varphi}{V}) + \iota_+(\restr{\varphi}{W})$.
		Analogamente vale la stessa cosa per gli altri indici. Infatti,
		prese due basi ortogonali $\basis_U$, $\basis_W$ di $U$ e $W$,
		la loro unione $\basis$ è una base ortogonale di $V$. Pertanto
		il numero di vettori della base $\basis$ con forma quadratica positiva
		è esattamente $\iota_+(\restr{\varphi}{V}) + \iota_+(\restr{\varphi}{W})$.
	\end{remark}

	\begin{definition}[isometria tra due spazi vettoriali]
		Dati due spazi vettoriali $(V, \varphi)$ e
		$(V', \varphi')$ dotati di prodotto scalare sullo stesso campo $\KK$, si dice che
		$V$ e $V'$ sono \textbf{isometrici} se esiste un isomorfismo
		$f$, detto \textit{isometria}, che preserva tali che prodotti, ossia tale che:
		
		\[ \varphi(\vec v, \vec w) = \varphi'(f(\vec v), f(\vec w)). \]
	\end{definition}

	\begin{exercise} Sia $f : V \to V'$ un isomorfismo. Allora $f$ è un'isometria $\iff$ $\forall$ base $\basis = \{ \vv 1, \ldots, \vv n \}$ di $V$, $\basis' = \{ f(\vv 1), \ldots, f(\vv n) \}$ è una base di $V'$ e $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$ $\iff$ $\exists$ base $\basis = \{ \vv 1, \ldots, \vv n \}$ di $V$ tale che $\basis' = \{ f(\vv 1), \ldots, f(\vv n) \}$ è una base di $V'$ e $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$.
	\end{exercise}

	\begin{solution} Se $f$ è un'isometria, detta $\basis$ una base di $V$, $\basis' = f(\basis)$ è una base di $V'$
		dal momento che $f$ è anche un isomorfismo. Inoltre, dacché $f$ è un'isometria, vale sicuramente che
		$\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. \\
		
		Sia ora assunto per ipotesi che $\forall$ base $\basis = \{ \vv 1, \ldots, \vv n \}$ di $V$, $\basis' = \{ f(\vv 1), \ldots, f(\vv n) \}$ è una base di $V'$ e $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. Allora, analogamente a prima, detta $\basis = \{ \vv 1, \ldots, \vv n \}$ una base di $V$, $\basis' = f(\basis)$ è una base di $V'$, e in quanto tale,
		per ipotesi, è tale che $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. \\
		
		Sia infine assunto per ipotesi che $\exists$ base $\basis = \{ \vv 1, \ldots, \vv n \}$ di $V$ tale che $\basis' = \{ f(\vv 1), \ldots, f(\vv n) \}$ è una base di $V'$ e $\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. Siano $\v$, $\w \in V$. Allora $\exists a_1$, ..., $a_n$, $b_1$, ..., $b_n \in \KK$
		tali che $\v = a_1 \vv 1 + \ldots + a_n \vv n$ e $\w = b_1 \vv 1 + \ldots + b_n \vv n$. Si ricava pertanto
		che:
		
		\[ \varphi'(f(\v), f(\w)) = \sum_{i=1}^n \sum_{j=1}^n a_i b_j \, \varphi'(f(\vv i), f(\vv j)) =
		\sum_{i=1}^n \sum_{j=1}^n a_i b_j \, \varphi(\vv i, \vv j) = \varphi(\v, \w), \]
		
		da cui la tesi.
	\end{solution}

	\begin{proposition} Sono equivalenti le seguenti affermazioni:
		
		\begin{enumerate}[(i)]
			\item $V$ e $V'$ sono isometrici;
			\item $\forall$ base $\basis$ di $V$, base $\basis'$ di $V'$,
			$M_\basis(\varphi)$ e $M_{\basis'}(\varphi')$ sono congruenti;
			\item $\exists$ base $\basis$ di $V$, base $\basis'$ di $V'$ tale che
			$M_\basis(\varphi)$ e $M_{\basis'}(\varphi')$ sono congruenti.
		\end{enumerate}
	\end{proposition}

	\begin{proof} Se $V$ e $V'$ sono isometrici, sia $f : V \to V'$ un'isometria. Sia $\basisC = \{ \vv 1, \ldots, \vv n \}$ una base di $V$. Allora, poiché $f$ è anche un isomorfismo, $\basisC' = f(\basisC)$ è una base di $V$ tale che
		$\varphi(\vv i, \vv j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$. Pertanto $M_\basisC(\varphi) = M_{\basisC'}(\varphi')$. Si conclude allora che, cambiando base in $V$ (o in $V'$), la matrice associata
		al prodotto scalare varia per congruenza dalla formula di cambiamento di base per il prodotto scalare, da cui si ricava che per ogni scelta di $\basis$ base di $V$ e di $\basis'$ base di $V'$, $M_\basis(\varphi) \cong M_{\basis'}(\varphi')$. Inoltre, se tale risultato è vero per ogni $\basis$ base di $V$ e di $\basis'$ base di $V'$, dal momento che sicuramente esistono due basi $\basis$, $\basis'$ di $V$ e $V'$, vale anche (ii) $\implies$ (iii). \\
		
		Si dimostra ora (iii) $\implies$ (i). Per ipotesi $M_\basis(\varphi) \cong M_{\basis'}(\varphi')$, quindi
		$\exists P \in \GL(n, \KK) \mid M_{\basis'}(\varphi') = P^\top M_\basis(\varphi) P$. Allora $\exists$ $\basis''$
		base di $V'$ tale che $P = M_{\basis''}^{\basis'}(\Idv)$, da cui $P\inv = M_{\basis'}^{\basis''}(\varphi)$. Per la formula di cambiamento di base del prodotto
		scalare, $M_{\basis''}(\varphi) = (P\inv)^\top M_{\basis'} P\inv = M_\basis(\varphi)$. Detta
		$\basis'' = \{ \ww 1, \ldots, \ww n \}$, si costruisce allora l'isomorfismo $f : V \to V'$ tale
		che $f(\vv i) = \ww i$ $\forall 1 \leq i \leq n$.. Dal momento che per costruzione $M_\basis(\varphi) = M_{\basis''}(\varphi')$,
		$\varphi(\vv i, \vv j) = \varphi'(\ww i, \ww j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$.
		Si conclude dunque che $\varphi(\v, \w) = \varphi'(f(\v), f(\w))$ $\forall \v, \w \in V$, e dunque
		che $f$ è un'isometria, come desiderato dalla tesi. 
	\end{proof}

	\begin{proposition} $(V, \varphi)$ e $(V', \varphi')$ spazi vettoriali
		su $\RR$ sono
		isometrici $\iff$ $\varphi$ e $\varphi'$ hanno la stessa segnatura.
	\end{proposition}

	\begin{proof}\nl\nl
		\rightproof Per la precedente proposizione, esistono due basi $\basis$ e $\basis'$, una di $V$ e una di $V'$,
		tali che $M_\basis(\varphi) \cong M_{\basis'}(\varphi)$. Allora queste due matrici condividono la stessa
		segnatura, e così quindi anche $\varphi$ e $\varphi'$. \\

		\leftproof Se $\varphi$ e $\varphi'$ hanno la stessa segnatura, esistono due basi $\basis = \{ \vv 1, \ldots, \vv n \}$ e $\basis' = \{ \ww 1, \ldots, \ww n \}$, una
		di $V$ e una di $V'$, tali che $M = M_\basis(\varphi) = M_{\basis'}(\varphi')$ e che $M$ è una matrice di
		Sylvester. Allora si costruisce $f : V \to V'$ tale che $f(\vv i) = \ww i$. Esso è un isomorfismo, e per
		costruzione $\varphi(\vv i, \vv j) = \varphi'(\ww i, \ww j) = \varphi'(f(\vv i), f(\vv j))$ $\forall 1 \leq i, j \leq n$, da cui
		si conclude che $\varphi(\v, \w) = \varphi'(f(\v), f(\w))$ $\forall \v$, $\w \in V$, e quindi che $V$ e
		$V'$ sono isometrici.
	\end{proof}

	\begin{definition}[sottospazio isotropo]
		Sia $W$ un sottospazio di $V$. Allora $W$ si dice \textbf{sottospazio isotropo} di $V$
		se $\restr{\varphi}{W} = 0$.
	\end{definition}

	\begin{remark}\nl
		\li $V^\perp$ è un sottospazio isotropo di $V$. \\
		\li $\vec{v}$ è un vettore isotropo $\iff$ $W = \Span(\vec v)$ è un sottospazio isotropo di $V$. \\
		\li $W \subseteq V$ è isotropo $\iff$ $W \subseteq W^\perp$.
	\end{remark}

	\begin{proposition}
		Sia $\varphi$ non degenere. Se $W$ è un sottospazio isotropo di $V$, allora
		$\dim W \leq \frac{1}{2} \dim V$.
	\end{proposition}

	\begin{proof}
		Poiché $W$ è un sottospazio isotropo di $V$, $W \subseteq W^\perp \implies \dim W \leq \dim W^\perp$.
		Allora, poiché $\varphi$ è non degenere, $\dim W + \dim W^\perp = \dim V$, $\dim W \leq \dim V - \dim W$,
		da cui $\dim W \leq \frac{1}{2} \dim V$.
	\end{proof}

	\begin{definition}[indice di Witt]
		Si definisce \textbf{indice di Witt} $W(\varphi)$ di $(V, \varphi)$
		come la massima dimensione di un sottospazio isotropo. 
	\end{definition}

	\begin{remark}\nl
		\li Se $\varphi > 0$ o $\varphi < 0$, $W(\varphi) = 0$.
	\end{remark}

	\begin{proposition}
		Sia $\KK = \RR$. Sia $\varphi$ non degenere e sia $\sigma(\varphi) = (\iota_+(\varphi), \iota_-(\varphi), 0)$. Allora
		$W(\varphi) = \min\{\iota_+(\varphi), \iota_-(\varphi)\}$.
	\end{proposition}

	\begin{proof}
		Senza perdità di generalità si assuma $\iota_-(\varphi) \leq \iota_+(\varphi)$ (il caso $\iota_-(\varphi) > \iota_+(\varphi)$ è analogo). Sia $W$ un sottospazio con $\dim W > \iota_-(\varphi)$. Sia $W^+$
		un sottospazio con $\dim W^+ = \iota_+(\varphi)$ e $\restr{\varphi}{W^+} > 0$. Allora, per la formula
		di Grassmann, $\dim W + \dim W^+ > n \implies \dim W + \dim W^+ > \dim W + \dim W^+ - \dim (W \cap W^+) \implies \dim (W \cap W^+) > 0$. Quindi $\exists \w \in W$, $\w \neq \vec 0$ tale che $\varphi(\w, \w) > 0$, da cui
		si ricava che $W$ non è isotropo. Pertanto $W(\varphi) \leq \iota_-(\varphi)$. \\
		
		Sia $a := \iota_+(\varphi)$ e sia $b := \iota_-(\varphi)$.
		Sia ora $\basis = \{ \vv 1, \ldots, \vv a, \ww 1, \ldots, \ww b \}$ una base tale per cui $M_\basis(\varphi)$ è  la matrice di Sylvester per $\varphi$. Siano $\vv 1$, ..., $\vv a$ tali che $\varphi(\vv i, \vv i) = 1$
		con $1 \leq i \leq a$. Analogamente siano $\ww 1$, ..., $\ww b$ tali che $\varphi(\ww i, \ww i) = -1$ con
		$1 \leq i \leq b$. Detta allora $\basis' = \{ \vv 1 ' := \vv 1 + \ww 1, \ldots, \vv b ' := \vv b + \ww b \}$, sia $W = \Span(\basis')$. \\
		
		Si osserva che $\basis'$ è linearmente indipendente, e dunque che $\dim W = \iota_-$. Inoltre
		$\varphi(\vv i ', \vv j ') = \varphi(\vv i + \ww i, \vv j + \ww j)$. Se $i \neq j$, allora
		$\varphi(\vv i ', \vv j ') = 0$, dal momento che i vettori di $\basis$ sono a due a due ortogonali
		tra loro. Se invece $i = j$, allora $\varphi(\vv i ', \vv j ') = \varphi(\vv i, \vv i) + \varphi(\ww i, \ww i) = 1-1=0$. Quindi $M_{\basis'}(\restr{\varphi}{W}) = 0$, da cui si conclude che $\restr{\varphi}{W} = 0$.
		Pertanto $W(\varphi) \geq i_-(\varphi)$, e quindi $W(\varphi) = i_-(\varphi)$, da cui la tesi.
	\end{proof}
\end{document}